{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import pygal\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import urllib.request\n",
    "import requests\n",
    "import curl\n",
    "import pycurl\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from IPython import qt\n",
    "from matplotlib.pyplot import figure\n",
    "from py.xml import raw\n",
    "from requests.api import get\n",
    "from matplotlib import pyplot as plt\n",
    "# from scratch.working_with_data import rescale\n",
    "# from scratch.multiple_regression import least_squares_fit, predict\n",
    "# from scratch.gradient_descent import gradient_step\n",
    "\n",
    "# from stats import mean, median, de_mean, standard_deviation, correlation\n",
    "# from gradient_descent import minimize_stochastic, maximize_stochastic, maximize_batch\n",
    "# from vector import dot, vector_add\n",
    "# from normal import normal_cdf\n",
    "# from matrix import make_matrix, get_column, shape, matrix_multiply\n",
    "# from logistic_regression import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "\n",
    "from scipy.optimize import fmin_tnc\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from typing import*\n",
    "\n",
    "from collections import*\n",
    "# from scipy import*\n",
    "from sklearn.metrics import*\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "import mlxtend\n",
    "\n",
    "# bltin_sum = np.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# def add(a, b): return a + b\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c , v):\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = np.sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = len(data[0])\n",
    "    \n",
    "    # Vector Mean\n",
    "#     n = len(data)\n",
    "#     m = np.sum(data,axis=0)\n",
    "#     means = np.multiply(1/n,m)\n",
    "    means = vector_mean(data)\n",
    "    \n",
    "    # Standard Deviaiton\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "    return means,stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = len(data[0])\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    grad_step = np.add(v,step)\n",
    "    return grad_step\n",
    "\n",
    "# def predict(alpha, beta, x_i):\n",
    "#     pred = beta * x_i + alpha\n",
    "#     return pred\n",
    "\n",
    "# def error(x, y, beta):\n",
    "#     \"\"\"\n",
    "#     The error from predicting beta * x_i + alpha\n",
    "#     when the actual value is y_i\n",
    "#     \"\"\"\n",
    "#     err_fin = predict(alpha, beta, x_i) - y_i\n",
    "#     return err_fin\n",
    "\n",
    "def predict(x, beta):\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x, y, beta):\n",
    "    return predict(x, beta) - y \n",
    "\n",
    "def sqerror_gradient(x, y, beta):\n",
    "    err = error(x, y, beta)\n",
    "    err_fin = [2 * err * x_i for x_i in x]\n",
    "    return err_fin\n",
    "\n",
    "def least_squares_fit(xs, ys, learning_rate = 0.001, num_steps = 1000, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [np.random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def _negative_log_likelihood(x, y, beta):\n",
    "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))\n",
    "    \n",
    "def negative_log_likelihood(xs, ys, beta):\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))\n",
    "\n",
    "def _negative_log_partial_j(x, y, beta, j):\n",
    "    \"\"\"\n",
    "    The jth partial derivative for one data point.\n",
    "    Here i is the index of the data point.\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x, y, beta):\n",
    "    \"\"\"\n",
    "    The gradient for one data point.\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs, ys,beta):\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "                                                                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.dat', names=[\n",
    "  \"sepal length in cm\",\n",
    "  \"sepal width in cm\",\n",
    "  \"petal length in cm\",\n",
    "  \"petal width in cm\",\n",
    "  \"class\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length in cm</th>\n",
       "      <th>sepal width in cm</th>\n",
       "      <th>petal length in cm</th>\n",
       "      <th>petal width in cm</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length in cm  sepal width in cm  petal length in cm  \\\n",
       "0                 5.1                3.5                 1.4   \n",
       "1                 4.9                3.0                 1.4   \n",
       "2                 4.7                3.2                 1.3   \n",
       "3                 4.6                3.1                 1.5   \n",
       "4                 5.0                3.6                 1.4   \n",
       "\n",
       "   petal width in cm        class  \n",
       "0                0.2  Iris-setosa  \n",
       "1                0.2  Iris-setosa  \n",
       "2                0.2  Iris-setosa  \n",
       "3                0.2  Iris-setosa  \n",
       "4                0.2  Iris-setosa  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify classes\n",
    "cls = []\n",
    "for i in range(len(df['class'])):\n",
    "    if df['class'][i] not in cls:\n",
    "        cls.append(df['class'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-139-df7fabb4bc6c>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[i] = 0\n",
      "<ipython-input-139-df7fabb4bc6c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[i] = 1\n",
      "<ipython-input-139-df7fabb4bc6c>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[i] = 2\n"
     ]
    }
   ],
   "source": [
    "# Replace string with word\n",
    "y = df['class']\n",
    "\n",
    "for i in range(len(df['class'])):\n",
    "    if y[i] == cls[0]:\n",
    "        y[i] = 0\n",
    "    elif y[i] == cls[1]:\n",
    "        y[i] = 1\n",
    "    elif y[i] == cls[2]:\n",
    "        y[i] = 2\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break up into groups of petal length, petal width\n",
    "\n",
    "# petal_length\n",
    "# pl = df[\"petal length in cm\"]\n",
    "pl = df.iloc[:,2]\n",
    "pw = df.iloc[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class 1\n",
    "# p1_1 = df['class'][df['class'] == cls[0]]\n",
    "df_class_1 = df.loc[df['class'] == 0]\n",
    "# Petal Length\n",
    "pl_1 = df_class_1[df_class_1.columns[2]]\n",
    "pw_1 = df_class_1[df_class_1.columns[3]]\n",
    "\n",
    "# Class 2\n",
    "df_class_2 = df.loc[df['class'] == 1]\n",
    "# Petal Length\n",
    "pl_2 = df_class_2[df_class_2.columns[2]]\n",
    "pw_2 = df_class_2[df_class_2.columns[3]]\n",
    "\n",
    "# Class 3\n",
    "df_class_3 = df.loc[df['class'] == 2]\n",
    "# Petal Length\n",
    "pl_3 = df_class_3[df_class_3.columns[2]]\n",
    "pw_3 = df_class_3[df_class_3.columns[3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_org_df = df[df.columns[2:5]]\n",
    "data_org_np = data_org_df.to_numpy()\n",
    "data_org_lst = data_org_np.tolist()\n",
    "# data_org_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decision_boundary(data_df):\n",
    "#     # Convert to list\n",
    "#     data_org_np = data_df.to_numpy()\n",
    "#     data_org_lst = data_org_np.tolist()\n",
    "    \n",
    "#     # Class\n",
    "#     ys = [row[2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "#     # Input\n",
    "#     xs = [[1] +row[:2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "#     # Maximum values\n",
    "#     exp_val = data_df.iloc[:,0]\n",
    "#     sal_val = data_df.iloc[:,1]\n",
    "    \n",
    "#     rescaled_xs = rescale(xs)\n",
    "#     beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "#     predictions = [predict(x_i, beta) for x_i in rescaled_xs]\n",
    "    \n",
    "#     random.seed(0)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(rescale_xs[0], ys, 0.33)\n",
    "#     x_train\n",
    "\n",
    "#     learning_rate = 0.01\n",
    "\n",
    "#     # pick a random starting point\n",
    "#     beta = [random.random() for _ in range(3)]\n",
    "#     y_train\n",
    "\n",
    "#     with tqdm.trange(2500) as t:\n",
    "#         for epoch in t:\n",
    "#             gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "#             beta = gradient_step(beta, gradient, -learning_rate)\n",
    "#             loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "#             t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "# #     print(t)\n",
    "\n",
    "#     means, stdevs = scale(xs)\n",
    "#     beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#     beta_unscaled\n",
    "\n",
    "#     means, stdevs = scale(xs)\n",
    "#     beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#     beta_unscaled\n",
    "    \n",
    "    \n",
    "#     # min_range = (data_marks.min())\n",
    "#     # min_range = math.floor(min_range[0])\n",
    "#     # max_range = (data_marks.max())\n",
    "#     # max_range = math.ceil(max_range[0])\n",
    "\n",
    "#     min_val = math.floor(np.min(exp_val))\n",
    "#     max_val = math.ceil(np.max(exp_val))\n",
    "\n",
    "#     # x_db = [xi for xi in range(min_val,max_val)]\n",
    "#     x_db = np.linspace(min_val,max_val,len(exp_val))\n",
    "#     y_db = [(-beta_unscaled[1]/beta_unscaled[2]*xi - beta_unscaled[0]/beta_unscaled[2])\n",
    "#             for xi in x_db]\n",
    "#     return x_db,y_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_db_1\n",
    "len(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 1.4, 0.2],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 1.3, 0.2],\n",
       "  [1, 1.5, 0.2],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 1.7, 0.4],\n",
       "  [1, 1.4, 0.3],\n",
       "  [1, 1.5, 0.2],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 1.5, 0.1],\n",
       "  [1, 1.5, 0.2],\n",
       "  [1, 1.6, 0.2],\n",
       "  [1, 1.4, 0.1],\n",
       "  [1, 1.1, 0.1],\n",
       "  [1, 1.2, 0.2],\n",
       "  [1, 1.5, 0.4],\n",
       "  [1, 1.3, 0.4],\n",
       "  [1, 1.4, 0.3],\n",
       "  [1, 1.7, 0.3],\n",
       "  [1, 1.5, 0.3],\n",
       "  [1, 1.7, 0.2],\n",
       "  [1, 1.5, 0.4],\n",
       "  [1, 1.0, 0.2],\n",
       "  [1, 1.7, 0.5],\n",
       "  [1, 1.9, 0.2],\n",
       "  [1, 1.6, 0.2],\n",
       "  [1, 1.6, 0.4],\n",
       "  [1, 1.5, 0.2],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 1.6, 0.2],\n",
       "  [1, 1.6, 0.2],\n",
       "  [1, 1.5, 0.4],\n",
       "  [1, 1.5, 0.1],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 1.5, 0.1],\n",
       "  [1, 1.2, 0.2],\n",
       "  [1, 1.3, 0.2],\n",
       "  [1, 1.5, 0.1],\n",
       "  [1, 1.3, 0.2],\n",
       "  [1, 1.5, 0.2],\n",
       "  [1, 1.3, 0.3],\n",
       "  [1, 1.3, 0.3],\n",
       "  [1, 1.3, 0.2],\n",
       "  [1, 1.6, 0.6],\n",
       "  [1, 1.9, 0.4],\n",
       "  [1, 1.4, 0.3],\n",
       "  [1, 1.6, 0.2],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 1.5, 0.2],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 4.7, 1.4],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 4.9, 1.5],\n",
       "  [1, 4.0, 1.3],\n",
       "  [1, 4.6, 1.5],\n",
       "  [1, 4.5, 1.3],\n",
       "  [1, 4.7, 1.6],\n",
       "  [1, 3.3, 1.0],\n",
       "  [1, 4.6, 1.3],\n",
       "  [1, 3.9, 1.4],\n",
       "  [1, 3.5, 1.0],\n",
       "  [1, 4.2, 1.5],\n",
       "  [1, 4.0, 1.0],\n",
       "  [1, 4.7, 1.4],\n",
       "  [1, 3.6, 1.3],\n",
       "  [1, 4.4, 1.4],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 4.1, 1.0],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 3.9, 1.1],\n",
       "  [1, 4.8, 1.8],\n",
       "  [1, 4.0, 1.3],\n",
       "  [1, 4.9, 1.5],\n",
       "  [1, 4.7, 1.2],\n",
       "  [1, 4.3, 1.3],\n",
       "  [1, 4.4, 1.4],\n",
       "  [1, 4.8, 1.4],\n",
       "  [1, 5.0, 1.7],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 3.5, 1.0],\n",
       "  [1, 3.8, 1.1],\n",
       "  [1, 3.7, 1.0],\n",
       "  [1, 3.9, 1.2],\n",
       "  [1, 5.1, 1.6],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 4.5, 1.6],\n",
       "  [1, 4.7, 1.5],\n",
       "  [1, 4.4, 1.3],\n",
       "  [1, 4.1, 1.3],\n",
       "  [1, 4.0, 1.3],\n",
       "  [1, 4.4, 1.2],\n",
       "  [1, 4.6, 1.4],\n",
       "  [1, 4.0, 1.2],\n",
       "  [1, 3.3, 1.0],\n",
       "  [1, 4.2, 1.3],\n",
       "  [1, 4.2, 1.2],\n",
       "  [1, 4.2, 1.3],\n",
       "  [1, 4.3, 1.3],\n",
       "  [1, 3.0, 1.1],\n",
       "  [1, 4.1, 1.3]],\n",
       " [[1, 4.7, 1.4],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 4.9, 1.5],\n",
       "  [1, 4.0, 1.3],\n",
       "  [1, 4.6, 1.5],\n",
       "  [1, 4.5, 1.3],\n",
       "  [1, 4.7, 1.6],\n",
       "  [1, 3.3, 1.0],\n",
       "  [1, 4.6, 1.3],\n",
       "  [1, 3.9, 1.4],\n",
       "  [1, 3.5, 1.0],\n",
       "  [1, 4.2, 1.5],\n",
       "  [1, 4.0, 1.0],\n",
       "  [1, 4.7, 1.4],\n",
       "  [1, 3.6, 1.3],\n",
       "  [1, 4.4, 1.4],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 4.1, 1.0],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 3.9, 1.1],\n",
       "  [1, 4.8, 1.8],\n",
       "  [1, 4.0, 1.3],\n",
       "  [1, 4.9, 1.5],\n",
       "  [1, 4.7, 1.2],\n",
       "  [1, 4.3, 1.3],\n",
       "  [1, 4.4, 1.4],\n",
       "  [1, 4.8, 1.4],\n",
       "  [1, 5.0, 1.7],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 3.5, 1.0],\n",
       "  [1, 3.8, 1.1],\n",
       "  [1, 3.7, 1.0],\n",
       "  [1, 3.9, 1.2],\n",
       "  [1, 5.1, 1.6],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 4.5, 1.6],\n",
       "  [1, 4.7, 1.5],\n",
       "  [1, 4.4, 1.3],\n",
       "  [1, 4.1, 1.3],\n",
       "  [1, 4.0, 1.3],\n",
       "  [1, 4.4, 1.2],\n",
       "  [1, 4.6, 1.4],\n",
       "  [1, 4.0, 1.2],\n",
       "  [1, 3.3, 1.0],\n",
       "  [1, 4.2, 1.3],\n",
       "  [1, 4.2, 1.2],\n",
       "  [1, 4.2, 1.3],\n",
       "  [1, 4.3, 1.3],\n",
       "  [1, 3.0, 1.1],\n",
       "  [1, 4.1, 1.3],\n",
       "  [1, 6.0, 2.5],\n",
       "  [1, 5.1, 1.9],\n",
       "  [1, 5.9, 2.1],\n",
       "  [1, 5.6, 1.8],\n",
       "  [1, 5.8, 2.2],\n",
       "  [1, 6.6, 2.1],\n",
       "  [1, 4.5, 1.7],\n",
       "  [1, 6.3, 1.8],\n",
       "  [1, 5.8, 1.8],\n",
       "  [1, 6.1, 2.5],\n",
       "  [1, 5.1, 2.0],\n",
       "  [1, 5.3, 1.9],\n",
       "  [1, 5.5, 2.1],\n",
       "  [1, 5.0, 2.0],\n",
       "  [1, 5.1, 2.4],\n",
       "  [1, 5.3, 2.3],\n",
       "  [1, 5.5, 1.8],\n",
       "  [1, 6.7, 2.2],\n",
       "  [1, 6.9, 2.3],\n",
       "  [1, 5.0, 1.5],\n",
       "  [1, 5.7, 2.3],\n",
       "  [1, 4.9, 2.0],\n",
       "  [1, 6.7, 2.0],\n",
       "  [1, 4.9, 1.8],\n",
       "  [1, 5.7, 2.1],\n",
       "  [1, 6.0, 1.8],\n",
       "  [1, 4.8, 1.8],\n",
       "  [1, 4.9, 1.8],\n",
       "  [1, 5.6, 2.1],\n",
       "  [1, 5.8, 1.6],\n",
       "  [1, 6.1, 1.9],\n",
       "  [1, 6.4, 2.0],\n",
       "  [1, 5.6, 2.2],\n",
       "  [1, 5.1, 1.5],\n",
       "  [1, 5.6, 1.4],\n",
       "  [1, 6.1, 2.3],\n",
       "  [1, 5.6, 2.4],\n",
       "  [1, 5.5, 1.8],\n",
       "  [1, 4.8, 1.8],\n",
       "  [1, 5.4, 2.1],\n",
       "  [1, 5.6, 2.4],\n",
       "  [1, 5.1, 2.3],\n",
       "  [1, 5.1, 1.9],\n",
       "  [1, 5.9, 2.3],\n",
       "  [1, 5.7, 2.5],\n",
       "  [1, 5.2, 2.3],\n",
       "  [1, 5.0, 1.9],\n",
       "  [1, 5.2, 2.0],\n",
       "  [1, 5.4, 2.3],\n",
       "  [1, 5.1, 1.8]]]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xs = [[1] +row[:2] for row in data_org_lst if row[2] < 2]\n",
    "# xs\n",
    "ys = [row[2] for row in data_org_lst if row[2] < 2]\n",
    "\n",
    "xs_1 = []\n",
    "# ys = df['class'].to_numpy().tolist()\n",
    "# ys = []\n",
    "for i in range(len(cls)-1):\n",
    "    xs_0 = [[1] +row[:2] for row in data_org_lst if row[2] >= 0+i  and row[2] < len(cls) - 1+i]\n",
    "#     ys_0 = [row[2] for row in data_org_lst if row[2] > 0+i and row[2] < 2+i]\n",
    "    xs_1.append(xs_0)\n",
    "#     ys.append(ys_0)\n",
    "  \n",
    "xs = xs_1[1]\n",
    "\n",
    "# Maximum values\n",
    "exp_val = data_org_df.iloc[:,0]\n",
    "sal_val = data_org_df.iloc[:,1]\n",
    "\n",
    "# onesx = np.ones((len(data_org_lst),1))\n",
    "# xs = np.vstack(onesx,\n",
    "\n",
    "\n",
    "# for row in data_org_lst:\n",
    "#     print(row[2])\n",
    "# xs_1[2]\n",
    "len(xs_1[0])\n",
    "sal_val\n",
    "xs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decision_boundary(data_df,xs,ys):\n",
    "#     # Convert to list\n",
    "# #     data_org_np = data_df.to_numpy()\n",
    "# #     data_org_lst = data_org_np.tolist()\n",
    "    \n",
    "#     # Class\n",
    "# #     ys = [row[2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "#     # Input\n",
    "# #     xs = [[1] +row[:2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "#     # Maximum values\n",
    "#     exp_val = data_df.iloc[:,0]\n",
    "#     sal_val = data_df.iloc[:,1]\n",
    "    \n",
    "#     rescaled_xs = rescale(xs)\n",
    "#     beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "#     predictions = [predict(x_i, beta) for x_i in rescaled_xs]\n",
    "    \n",
    "#     random.seed(0)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(rescale_xs[0], ys, 0.33)\n",
    "#     x_train\n",
    "\n",
    "#     learning_rate = 0.01\n",
    "\n",
    "#     # pick a random starting point\n",
    "#     beta = [random.random() for _ in range(3)]\n",
    "#     y_train\n",
    "\n",
    "#     with tqdm.trange(2500) as t:\n",
    "#         for epoch in t:\n",
    "#             gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "#             beta = gradient_step(beta, gradient, -learning_rate)\n",
    "#             loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "#             t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "# #     print(t)\n",
    "\n",
    "#     means, stdevs = scale(xs)\n",
    "#     beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#     beta_unscaled\n",
    "\n",
    "#     means, stdevs = scale(xs)\n",
    "#     beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#     beta_unscaled\n",
    "    \n",
    "    \n",
    "#     # min_range = (data_marks.min())\n",
    "#     # min_range = math.floor(min_range[0])\n",
    "#     # max_range = (data_marks.max())\n",
    "#     # max_range = math.ceil(max_range[0])\n",
    "\n",
    "#     min_val = math.floor(np.min(exp_val))\n",
    "#     max_val = math.ceil(np.max(exp_val))\n",
    "\n",
    "#     # x_db = [xi for xi in range(min_val,max_val)]\n",
    "#     x_db = np.linspace(min_val,max_val,len(exp_val))\n",
    "#     y_db = [(-beta_unscaled[1]/beta_unscaled[2]*xi - beta_unscaled[0]/beta_unscaled[2])\n",
    "#             for xi in x_db]\n",
    "#     return x_db,y_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decision_boundary(data_df,xs,ys):\n",
    "#     # Convert to list\n",
    "# #     data_org_np = data_df.to_numpy()\n",
    "# #     data_org_lst = data_org_np.tolist()\n",
    "    \n",
    "#     # Class\n",
    "# #     ys = [row[2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "#     # Input\n",
    "# #     xs = [[1] +row[:2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "# #     # Maximum values\n",
    "# #     exp_val = data_df.iloc[:,0]\n",
    "# #     sal_val = data_df.iloc[:,1]\n",
    "    \n",
    "    \n",
    "#     rescaled_xs = rescale(xs)\n",
    "#     beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "#     predictions = [predict(x_i, beta) for x_i in rescaled_xs]\n",
    "    \n",
    "#     random.seed(0)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "#     x_train\n",
    "\n",
    "#     learning_rate = 0.01\n",
    "\n",
    "#     # pick a random starting point\n",
    "#     beta = [random.random() for _ in range(3)]\n",
    "#     y_train\n",
    "\n",
    "#     with tqdm.trange(2500) as t:\n",
    "#         for epoch in t:\n",
    "#             gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "#             beta = gradient_step(beta, gradient, -learning_rate)\n",
    "#             loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "#             t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "# #     print(t)\n",
    "\n",
    "#     means, stdevs = scale(xs)\n",
    "#     beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#     beta_unscaled\n",
    "\n",
    "#     means, stdevs = scale(xs)\n",
    "#     beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#     beta_unscaled\n",
    "    \n",
    "    \n",
    "#     # min_range = (data_marks.min())\n",
    "#     # min_range = math.floor(min_range[0])\n",
    "#     # max_range = (data_marks.max())\n",
    "#     # max_range = math.ceil(max_range[0])\n",
    "\n",
    "#     min_val = math.floor(np.min(exp_val))\n",
    "#     max_val = math.ceil(np.max(exp_val))\n",
    "\n",
    "#     # x_db = [xi for xi in range(min_val,max_val)]\n",
    "#     x_db = np.linspace(min_val,max_val,len(exp_val))\n",
    "#     y_db = [(-beta_unscaled[1]/beta_unscaled[2]*xi - beta_unscaled[0]/beta_unscaled[2])\n",
    "#             for xi in x_db]\n",
    "#     return x_db,y_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "# data_org_np = data__org_df.to_numpy()\n",
    "# data_org_lst = data_org_np.tolist()\n",
    "    \n",
    "    # Class\n",
    "#     ys = [row[2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "    # Input\n",
    "#     xs = [[1] +row[:2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "#     # Maximum values\n",
    "#     exp_val = data_df.iloc[:,0]\n",
    "#     sal_val = data_df.iloc[:,1]\n",
    "def decision_boundary_2(data_df,xs,ys,label):\n",
    "    cls = label\n",
    "\n",
    "# xs_1 = []\n",
    "# # ys = df['class'].to_numpy().tolist()\n",
    "# # ys = []\n",
    "# for i in range(len(cls)-1):\n",
    "#     xs_0 = [[1] +row[:2] for row in data_org_lst if row[2] >= 0+i  and row[2] < len(cls) - 1+i]\n",
    "# #     ys_0 = [row[2] for row in data_org_lst if row[2] > 0+i and row[2] < 2+i]\n",
    "#     xs_1.append(xs_0)\n",
    "# #     ys.append(ys_0)\n",
    "\n",
    "    x_db_lst = []\n",
    "    y_db_lst = []\n",
    "    if len(xs_1) > 1:\n",
    "        for i in range(len(cls)-1):             \n",
    "\n",
    "            xs = xs_1[i]\n",
    "            rescaled_xs = rescale(xs)\n",
    "            beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "            predictions = [predict(x_i, beta) for x_i in rescaled_xs]\n",
    "\n",
    "            random.seed(0)\n",
    "            x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "            x_train\n",
    "\n",
    "            learning_rate = 0.01\n",
    "\n",
    "            # pick a random starting point\n",
    "            beta = [random.random() for _ in range(3)]\n",
    "            y_train\n",
    "\n",
    "            with tqdm.trange(2500) as t:\n",
    "                for epoch in t:\n",
    "                    gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "                    beta = gradient_step(beta, gradient, -learning_rate)\n",
    "                    loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "                    t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "        #     print(t)\n",
    "\n",
    "            means, stdevs = scale(xs)\n",
    "            beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "                             beta[1] / stdevs[1],\n",
    "                             beta[2] / stdevs[2]]\n",
    "            beta_unscaled\n",
    "\n",
    "            means, stdevs = scale(xs)\n",
    "            beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "                             beta[1] / stdevs[1],\n",
    "                             beta[2] / stdevs[2]]\n",
    "            beta_unscaled\n",
    "\n",
    "\n",
    "            # min_range = (data_marks.min())\n",
    "            # min_range = math.floor(min_range[0])\n",
    "            # max_range = (data_marks.max())\n",
    "            # max_range = math.ceil(max_range[0])\n",
    "\n",
    "            min_val = math.floor(np.min(exp_val))\n",
    "            max_val = math.ceil(np.max(exp_val))\n",
    "\n",
    "            # x_db = [xi for xi in range(min_val,max_val)]\n",
    "            x_db = np.linspace(min_val,max_val,len(exp_val))\n",
    "            x_db_lst.append(x_db)\n",
    "            y_db = [(-beta_unscaled[1]/beta_unscaled[2]*xi - beta_unscaled[0]/beta_unscaled[2])\n",
    "                    for xi in x_db]\n",
    "            y_db_lst.append(y_db)\n",
    "    return x_db_lst,y_db_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-135-d83c934c112b>:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
      "least squares fit: 100%|██████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 279.22it/s]\n",
      "  0%|                                                                                         | 0/2500 [00:00<?, ?it/s]<ipython-input-135-d83c934c112b>:15: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return [sum(vector[i] for vector in vectors)\n",
      "<ipython-input-135-d83c934c112b>:164: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(_negative_log_likelihood(x, y, beta)\n",
      "loss: 0.060 beta: [1.15290674 5.36580482 4.4417838 ]: 100%|███████████████████████| 2500/2500 [00:11<00:00, 216.17it/s]\n",
      "least squares fit: 100%|██████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 253.33it/s]\n",
      "loss: 3.269 beta: [0.01815226 2.26003596 9.86876876]: 100%|███████████████████████| 2500/2500 [00:12<00:00, 203.50it/s]\n"
     ]
    }
   ],
   "source": [
    "x_db_lst,y_db_lst = decision_boundary_2(data_org_df,xs_1,ys,cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_db_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_db_1,y_db_1 = decision_boundary(data_org_df,xs_1[0],ys)\n",
    "# x_db_2,y_db_2 = decision_boundary(data_org_df,xs_1[1],ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter Plots\n",
    "\n",
    "plt.scatter(pl_1,pw_1, color = 'red', marker = '+')\n",
    "plt.scatter(pl_2,pw_2, color = 'blue', marker = 'o')\n",
    "# plt.scatter(pl_3,pw_3, color = 'green', marker = '*')\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs = [(1,) + row[:2] for row in data_org]\n",
    "# ys = [row[2] for row in data_org]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm, nn = scale(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale_xs = []\n",
    "# beta = []\n",
    "# pred = []\n",
    "\n",
    "# for i in range(len(cls)-2):\n",
    "#     rescaled_xs = rescale(xs[i])\n",
    "#     rescale_xs.append(rescaled_xs)\n",
    "#     beta_0 = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "#     beta.append(beta_0)\n",
    "#     predictions = [predict(x_i, beta_0) for x_i in rescaled_xs]\n",
    "#     pred.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaled_xs = rescale(xs_1[0])\n",
    "# beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "# predictions = [predict(x_i, beta) for x_i in rescaled_xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/2500 [00:00<?, ?it/s]<ipython-input-135-d83c934c112b>:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
      "<ipython-input-135-d83c934c112b>:15: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return [sum(vector[i] for vector in vectors)\n",
      "<ipython-input-135-d83c934c112b>:164: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(_negative_log_likelihood(x, y, beta)\n",
      "loss: 3.269 beta: [0.01815226 2.26003596 9.86876876]: 100%|███████████████████████| 2500/2500 [00:12<00:00, 207.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.269 beta: [0.01815226 2.26003596 9.86876876]: 100%|███████████████████████| 2500/2500 [00:12<00:00, 207.39it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "x_train\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# pick a random starting point\n",
    "beta = [random.random() for _ in range(3)]\n",
    "y_train\n",
    "\n",
    "with tqdm.trange(2500) as t:\n",
    "    for epoch in t:\n",
    "        gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "        beta = gradient_step(beta, gradient, -learning_rate)\n",
    "        loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "        t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01815226, 2.26003596, 9.86876876])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means, stdevs = scale(xs)\n",
    "# beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                  beta[1] / stdevs[1],\n",
    "#                  beta[2] / stdevs[2]]\n",
    "# beta_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-135-d83c934c112b>:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(v_i * w_i for v_i, w_i in zip(v, w))\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for i in range(len(beta)-1):\n",
    "\n",
    "    rescaled_xs = rescale(xs_1[i])\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "\n",
    "\n",
    "    true_positives = false_positives = true_negatives = false_negatives = 0\n",
    "\n",
    "    for x_i, y_i in zip(x_test, y_test):\n",
    "        prediction = logistic(dot(beta, x_i))\n",
    "\n",
    "        if y_i == 1 and prediction >= 0.5:  # TP: paid and we predict paid\n",
    "            true_positives += 1\n",
    "        elif y_i == 1:                      # FN: paid and we predict unpaid\n",
    "            false_negatives += 1\n",
    "        elif prediction >= 0.5:             # FP: unpaid and we predict paid\n",
    "            false_positives += 1\n",
    "        else:                               # TN: unpaid and we predict unpaid\n",
    "            true_negatives += 1\n",
    "\n",
    "    precision_0 = true_positives / (true_positives + false_positives)\n",
    "    precision.append(precision_0)\n",
    "    recall_0 = true_positives / (true_positives + false_negatives)\n",
    "    recall.append(recall_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu_1 = np.mean([xi[1] for xi in xs])\n",
    "# sig_1 = standard_deviation([xi[1] for xi in xs])\n",
    "\n",
    "# mu_2 = np.mean([xi[2] for xi in xs])\n",
    "# sig_2 = standard_deviation([xi[2] for xi in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # min_range = (data_marks.min())\n",
    "# # min_range = math.floor(min_range[0])\n",
    "# # max_range = (data_marks.max())\n",
    "# # max_range = math.ceil(max_range[0])\n",
    "\n",
    "# min_val = math.floor(np.min(exp_val))\n",
    "# max_val = math.ceil(np.max(exp_val))\n",
    "\n",
    "# # x_db = [xi for xi in range(min_val,max_val)]\n",
    "# x_db = np.linspace(min_val,max_val,len(exp_val))\n",
    "# y_db = [(-beta_unscaled[1]/beta_unscaled[2]*xi - beta_unscaled[0]/beta_unscaled[2])\n",
    "#         for xi in x_db]\n",
    "# y_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(pl_1,pw_1, color = 'red', marker = '+')\n",
    "plt.scatter(pl_2,pw_2, color = 'blue', marker = 'o')\n",
    "plt.scatter(pl_3,pw_3, color = 'green', marker = '*')\n",
    "# plt.plot(x_db_1, y_db_1, 'purple')\n",
    "# plt.plot(x_db_2, y_db_2, 'darkorange')\n",
    "# plt.plot(x_db, y_db, 'purple')\n",
    "plt.plot(x_db_lst[0], y_db_lst[0], 'purple')\n",
    "plt.plot(x_db_lst[1], y_db_lst[1], 'darkorange')\n",
    "plt.xlabel('experience')\n",
    "plt.ylabel('salary')\n",
    "# plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Paid and Unpaid Users')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
