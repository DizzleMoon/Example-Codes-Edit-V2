{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "# import pygal\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import urllib.request\n",
    "import requests\n",
    "import curl\n",
    "import pycurl\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from IPython import qt\n",
    "from matplotlib.pyplot import figure\n",
    "from py.xml import raw\n",
    "from requests.api import get\n",
    "from matplotlib import pyplot as plt\n",
    "# from scratch.working_with_data import rescale\n",
    "# from scratch.multiple_regression import least_squares_fit, predict\n",
    "# from scratch.gradient_descent import gradient_step\n",
    "\n",
    "# from stats import mean, median, de_mean, standard_deviation, correlation\n",
    "# from gradient_descent import minimize_stochastic, maximize_stochastic, maximize_batch\n",
    "# from vector import dot, vector_add\n",
    "# from normal import normal_cdf\n",
    "# from matrix import make_matrix, get_column, shape, matrix_multiply\n",
    "# from logistic_regression import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "\n",
    "from scipy.optimize import fmin_tnc\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from typing import*\n",
    "\n",
    "from collections import*\n",
    "# from scipy import*\n",
    "from sklearn.metrics import*\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "# import mnist\n",
    "# bltin_sum = np.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# def add(a, b): return a + b\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "Tensor = list\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c , v):\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "#     return np.sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "#     gen = \n",
    "    return np.sum(np.fromiter((v_i * w_i for v_i, w_i in zip(v, w)),float))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = data.shape[0]\n",
    "    \n",
    "    # Vector Mean\n",
    "#     n = len(data)\n",
    "#     m = np.sum(data,axis=0)\n",
    "#     means = np.multiply(1/n,m)\n",
    "    means = vector_mean(data)\n",
    "    \n",
    "    # Standard Deviaiton\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "    return means,stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = data.shape[0]\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    grad_step = np.add(v,step)\n",
    "    return grad_step\n",
    "\n",
    "# def predict(alpha, beta, x_i):\n",
    "#     pred = beta * x_i + alpha\n",
    "#     return pred\n",
    "\n",
    "# def error(x, y, beta):\n",
    "#     \"\"\"\n",
    "#     The error from predicting beta * x_i + alpha\n",
    "#     when the actual value is y_i\n",
    "#     \"\"\"\n",
    "#     err_fin = predict(alpha, beta, x_i) - y_i\n",
    "#     return err_fin\n",
    "\n",
    "def predict(x, beta):\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x, y, beta):\n",
    "    return predict(x, beta) - y \n",
    "\n",
    "def sqerror_gradient(x, y, beta):\n",
    "    err = error(x, y, beta)\n",
    "    err_fin = [2 * err * x_i for x_i in x]\n",
    "    return err_fin\n",
    "\n",
    "def least_squares_fit(xs, ys, learning_rate = 0.001, num_steps = 1000, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [np.random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def _negative_log_likelihood(x, y, beta):\n",
    "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))\n",
    "    \n",
    "def negative_log_likelihood(xs, ys, beta):\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))\n",
    "\n",
    "def _negative_log_partial_j(x, y, beta, j):\n",
    "    \"\"\"\n",
    "    The jth partial derivative for one data point.\n",
    "    Here i is the index of the data point.\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x, y, beta):\n",
    "    \"\"\"\n",
    "    The gradient for one data point.\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs, ys,beta):\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "                                                                \n",
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def sigmoid(t: float) -> float: \n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "# Gradient Descent - step\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "def squared_distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\"\n",
    "    return sum_of_squares(subtract(v, w))\n",
    "\n",
    "def distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes the distance between v and w\"\"\"\n",
    "    return math.sqrt(squared_distance(v, w))\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "\n",
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor.\n",
    "    Otherwise, tensor is 1-dimensional (that is, a vector).\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)  # just a list of floats, use Python sum\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
    "                   for tensor_i in tensor)   # and sum up those results.\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f elementwise\"\"\" \n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor] \n",
    "    \n",
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "\n",
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                   t1: Tensor,\n",
    "                   t2: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]\n",
    "    \n",
    "def inverse_normal_cdf(p: float,\n",
    "                       mu: float = 0,\n",
    "                       sigma: float = 1,\n",
    "                       tolerance: float = 0.00001) -> float:\n",
    "    \"\"\"Find approximate inverse using binary search\"\"\"\n",
    "\n",
    "    # if not standard, compute standard and rescale\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "\n",
    "    low_z = -10.0                      # normal_cdf(-10) is (very close to) 0\n",
    "    hi_z  =  10.0                      # normal_cdf(10)  is (very close to) 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2     # Consider the midpoint\n",
    "        mid_p = normal_cdf(mid_z)      # and the CDF's value there\n",
    "        if mid_p < p:\n",
    "            low_z = mid_z              # Midpoint too low, search above it\n",
    "        else:\n",
    "            hi_z = mid_z               # Midpoint too high, search below it\n",
    "\n",
    "    return mid_z\n",
    "\n",
    "def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -> float:\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "def softmax(tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Softmax along the last dimension\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        # Subtract largest value for numerical stability.\n",
    "        largest = max(tensor)\n",
    "        exps = [math.exp(x - largest) for x in tensor]\n",
    "\n",
    "        sum_of_exps = sum(exps)                 # This is the total \"weight.\"\n",
    "        return [exp_i / sum_of_exps             # Probability is the fraction\n",
    "                for exp_i in exps]              # of the total weight.\n",
    "    else:\n",
    "        return [softmax(tensor_i) for tensor_i in tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    #row represents num classes but they may be real numbers\n",
    "    #So the shape of input is important\n",
    "    #([[1, 3, 5, 7],\n",
    "    #  [1,-9, 4, 8]]\n",
    "\n",
    "    #softmax will be for each of the 2 rows\n",
    "    #[[2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01]\n",
    "    #[8.94679461e-04 4.06183847e-08 1.79701173e-02 9.81135163e-01]]\n",
    "    #respectively But if the input is Tranposed clearly the answer\n",
    "    #will be wrong.\n",
    "\n",
    "    #That needs to be converted to probability\n",
    "    #column represents the vocabulary size.\n",
    "\n",
    "    r, c = logits.shape\n",
    "    predsl = []\n",
    "    for row in logits:\n",
    "        inputs = np.asarray(row)\n",
    "        #print(\"inputs:\",inputs)\n",
    "        predsl.append(np.exp(inputs) / float(sum(np.exp(inputs))))\n",
    "    return np.array(predsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[ 1  3  5  7]\n",
      " [ 1 -9  4  8]]\n",
      "softmax: [[2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01]\n",
      " [8.94679461e-04 4.06183847e-08 1.79701173e-02 9.81135163e-01]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 3, 5, 7],\n",
    "      [1,-9, 4, 8]])\n",
    "print(\"x:\",x)\n",
    "sm=softmax(x)\n",
    "print(\"softmax:\",sm)\n",
    "#prints out\n",
    "#[[2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01],\n",
    "#[8.94679461e-04 4.06183847e-08 1.79701173e-02 9.81135163e-01]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softmax_grad(sm):\n",
    "    # Below is the softmax value for [1, 3, 5, 7]\n",
    "    # [2.14400878e-03 1.58422012e-02 1.17058913e-01 8.64954877e-01]\n",
    "    # initialize the 2-D jacobian matrix.\n",
    "    jacobian_m = np.diag(sm)\n",
    "    for i in range(len(jacobian_m)):\n",
    "        for j in range(len(jacobian_m)):\n",
    "            if i == j:\n",
    "                print(\"equal:\",i, sm[i],(1-sm[i]))\n",
    "                #equal: 0 0.002144008783584634 0.9978559912164153\n",
    "                #equal: 1 0.015842201178506925 0.9841577988214931\n",
    "                #equal: 2 0.11705891323853292 0.8829410867614671\n",
    "                #equal: 3 0.8649548767993754 0.13504512320062456\n",
    "                jacobian_m[i][j] = sm[i] * (1-sm[i])\n",
    "            else:\n",
    "                print(\"not equal:\",i,j,sm[i],sm[j])\n",
    "                #not equal: 0 1 0.002144008783584634 0.015842201178506925\n",
    "                #not equal: 0 2 0.002144008783584634 0.11705891323853292\n",
    "                #not equal: 0 3 0.002144008783584634 0.8649548767993754\n",
    "\n",
    "                #not equal: 1 0 0.015842201178506925 0.002144008783584634\n",
    "                #not equal: 1 2 0.015842201178506925 0.11705891323853292\n",
    "                #not equal: 1 3 0.015842201178506925 0.8649548767993754\n",
    "\n",
    "                #not equal: 2 0 0.11705891323853292 0.002144008783584634\n",
    "                #not equal: 2 1 0.11705891323853292 0.015842201178506925\n",
    "                #not equal: 2 3 0.11705891323853292 0.8649548767993754\n",
    "\n",
    "                #not equal: 3 0 0.8649548767993754 0.002144008783584634\n",
    "                #not equal: 3 1 0.8649548767993754 0.015842201178506925\n",
    "                #not equal: 3 2 0.8649548767993754 0.11705891323853292\n",
    "                jacobian_m[i][j] = -sm[i]*sm[j]\n",
    "\n",
    "    #finally resulting in\n",
    "    #[[ 2.13941201e-03 -3.39658185e-05 -2.50975338e-04 -1.85447085e-03]\n",
    "    #[-3.39658185e-05  1.55912258e-02 -1.85447085e-03 -1.37027892e-02]\n",
    "    #[-2.50975338e-04 -1.85447085e-03  1.03356124e-01 -1.01250678e-01]\n",
    "    #[-1.85447085e-03 -1.37027892e-02 -1.01250678e-01  1.16807938e-01]]\n",
    "\n",
    "    return jacobian_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred,labels):\n",
    "  \"\"\"\n",
    "  One at a time.\n",
    "  args:\n",
    "      pred-(seq=1),input_size\n",
    "      labels-(seq=1),input_size\n",
    "  \"\"\"\n",
    "  return np.multiply(labels, -np.log(pred)).sum(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_one_hot(num,vocab_size):\n",
    "    #print(num)\n",
    "    x = np.zeros(vocab_size)\n",
    "    x[int(num)] = 1\n",
    "    x=np.reshape(x,[1,-1])\n",
    "    #print(\":\",x,x.shape)\n",
    "    return x;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14507794]\n",
      "[17.01904505]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 3, 5, 7],\n",
    "            [1,-9, 4, 8]])\n",
    "y = np.array([3,1])\n",
    "sm=softmax(x)\n",
    "\n",
    "#prints out 0.145\n",
    "print(loss(sm[0],input_one_hot(y[0],4)))\n",
    "#prints out 17.01\n",
    "print(loss(sm[1],input_one_hot(y[1],4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkdatadim(data , degree, msg=None):\n",
    "    if(len(data.shape)!=degree):\n",
    "        if msg is None:\n",
    "            raise ValueError('Dimension must be', degree,\" but is \", len(data.shape),\".\")\n",
    "        else:\n",
    "            raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(pred,labels):\n",
    "    \"\"\"\n",
    "    Does an internal softmax before loss calculation.\n",
    "    args:\n",
    "        pred- batch,seq,input_size\n",
    "        labels-batch,seq(has to be transformed before comparision with preds(line-133).)\n",
    "    \"\"\"\n",
    "    checkdatadim(pred,3)\n",
    "    checkdatadim(labels,2)\n",
    "    batch,seq,size=pred.shape\n",
    "    yhat=np.zeros((batch,seq,size))\n",
    "\n",
    "    for batnum in range(batch):\n",
    "        for seqnum in range(seq):\n",
    "            yhat[batnum][seqnum]=softmax(np.reshape(pred[batnum][seqnum],[-1,size]))\n",
    "    lossesa=np.zeros((batch,seq))\n",
    "    for batnum in range(batch):\n",
    "        for seqnum in range(seq):\n",
    "            lossesa[batnum][seqnum]=loss(np.reshape(yhat[batnum][seqnum],[1,-1]),input_one_hot(labels[batnum][seqnum],size))\n",
    "    return yhat,lossesa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 3, 5, 7],\n",
    "            [1,-9, 4, 8]]])\n",
    "y = np.array([[3,1]])\n",
    "\n",
    "#prints array([[ 0.14507794, 17.01904505]]))\n",
    "# softmaxed,loss=cross_entropy_loss(x,y)\n",
    "# print(\"loss:\",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [[ 0.14507794 17.01904505]]\n",
      "gradient: [[[ 2.14400878e-03  1.58422012e-02  1.17058913e-01 -1.35045123e-01]\n",
      "  [ 8.94679461e-04 -9.99999959e-01  1.79701173e-02  9.81135163e-01]]]\n"
     ]
    }
   ],
   "source": [
    "softmaxed,loss=cross_entropy_loss(x,y)\n",
    "print(\"loss:\",loss)\n",
    "\n",
    "batch,seq,size=x.shape\n",
    "target_one_hot=np.zeros((batch,seq,size))\n",
    "# Adapting the shape of Y\n",
    "for batnum in range(batch):\n",
    "    for i in range(seq):\n",
    "        target_one_hot[batnum][i]=input_one_hot(y[batnum][i],size)\n",
    "dy = softmaxed.copy()\n",
    "dy = dy - target_one_hot\n",
    "# prints out gradient: [[[ 2.14400878e-03  1.58422012e-02  1.17058913e-01 -1.35045123e-01]\n",
    "#                        [ 8.94679461e-04 -9.99999959e-01  1.79701173e-02  9.81135163e-01]]]\n",
    "print(\"gradient:\",dy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array([[1, 3, 5, 7],\n",
    "#       [1,-9, 4, 8]])\n",
    "\n",
    "# y = np.array([3,1])\n",
    "# print(\"x:\",x.shape)\n",
    "# sm=softmax(x)\n",
    "# print(\"softmax:\",sm)\n",
    "# jacobian=_softmax_grad(sm[0])\n",
    "# print(\"jacobian:\",jacobian)\n",
    "# jacobian=_softmax_grad(sm[1])\n",
    "# print(jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
