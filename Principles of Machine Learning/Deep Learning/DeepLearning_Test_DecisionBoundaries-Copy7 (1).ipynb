{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "# import pygal\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import urllib.request\n",
    "import requests\n",
    "import curl\n",
    "import pycurl\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from IPython import qt\n",
    "from matplotlib.pyplot import figure\n",
    "from py.xml import raw\n",
    "from requests.api import get\n",
    "from matplotlib import pyplot as plt\n",
    "# from scratch.working_with_data import rescale\n",
    "# from scratch.multiple_regression import least_squares_fit, predict\n",
    "# from scratch.gradient_descent import gradient_step\n",
    "\n",
    "# from stats import mean, median, de_mean, standard_deviation, correlation\n",
    "# from gradient_descent import minimize_stochastic, maximize_stochastic, maximize_batch\n",
    "# from vector import dot, vector_add\n",
    "# from normal import normal_cdf\n",
    "# from matrix import make_matrix, get_column, shape, matrix_multiply\n",
    "# from logistic_regression import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "\n",
    "from scipy.optimize import fmin_tnc\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from typing import*\n",
    "\n",
    "from collections import*\n",
    "# from scipy import*\n",
    "from sklearn.metrics import*\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "# import mlxtend\n",
    "\n",
    "# bltin_sum = np.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# def add(a, b): return a + b\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c , v):\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = np.sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = len(data[0])\n",
    "    \n",
    "    # Vector Mean\n",
    "#     n = len(data)\n",
    "#     m = np.sum(data,axis=0)\n",
    "#     means = np.multiply(1/n,m)\n",
    "    means = vector_mean(data)\n",
    "    \n",
    "    # Standard Deviaiton\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "    return means,stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = len(data[0])\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    grad_step = np.add(v,step)\n",
    "    return grad_step\n",
    "\n",
    "# def predict(alpha, beta, x_i):\n",
    "#     pred = beta * x_i + alpha\n",
    "#     return pred\n",
    "\n",
    "# def error(x, y, beta):\n",
    "#     \"\"\"\n",
    "#     The error from predicting beta * x_i + alpha\n",
    "#     when the actual value is y_i\n",
    "#     \"\"\"\n",
    "#     err_fin = predict(alpha, beta, x_i) - y_i\n",
    "#     return err_fin\n",
    "\n",
    "def predict(x, beta):\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x, y, beta):\n",
    "    return predict(x, beta) - y \n",
    "\n",
    "def sqerror_gradient(x, y, beta):\n",
    "    err = error(x, y, beta)\n",
    "    err_fin = [2 * err * x_i for x_i in x]\n",
    "    return err_fin\n",
    "\n",
    "def least_squares_fit(xs, ys, learning_rate = 0.001, num_steps = 1000, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [np.random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def _negative_log_likelihood(x, y, beta):\n",
    "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))\n",
    "    \n",
    "def negative_log_likelihood(xs, ys, beta):\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))\n",
    "\n",
    "def _negative_log_partial_j(x, y, beta, j):\n",
    "    \"\"\"\n",
    "    The jth partial derivative for one data point.\n",
    "    Here i is the index of the data point.\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x, y, beta):\n",
    "    \"\"\"\n",
    "    The gradient for one data point.\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs, ys,beta):\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "                                                                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.dat', names=[\n",
    "  \"sepal length in cm\",\n",
    "  \"sepal width in cm\",\n",
    "  \"petal length in cm\",\n",
    "  \"petal width in cm\",\n",
    "  \"class\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length in cm</th>\n",
       "      <th>sepal width in cm</th>\n",
       "      <th>petal length in cm</th>\n",
       "      <th>petal width in cm</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length in cm  sepal width in cm  petal length in cm  \\\n",
       "0                 5.1                3.5                 1.4   \n",
       "1                 4.9                3.0                 1.4   \n",
       "2                 4.7                3.2                 1.3   \n",
       "3                 4.6                3.1                 1.5   \n",
       "4                 5.0                3.6                 1.4   \n",
       "\n",
       "   petal width in cm        class  \n",
       "0                0.2  Iris-setosa  \n",
       "1                0.2  Iris-setosa  \n",
       "2                0.2  Iris-setosa  \n",
       "3                0.2  Iris-setosa  \n",
       "4                0.2  Iris-setosa  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify classes\n",
    "cls = []\n",
    "for i in range(len(df['class'])):\n",
    "    if df['class'][i] not in cls:\n",
    "        cls.append(df['class'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17656/2216541371.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[i] = 0\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17656/2216541371.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[i] = 1\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17656/2216541371.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[i] = 2\n"
     ]
    }
   ],
   "source": [
    "# Replace string with word\n",
    "y = df['class']\n",
    "\n",
    "for i in range(len(df['class'])):\n",
    "    if y[i] == cls[0]:\n",
    "        y[i] = 0\n",
    "    elif y[i] == cls[1]:\n",
    "        y[i] = 1\n",
    "    elif y[i] == cls[2]:\n",
    "        y[i] = 2\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break up into groups of petal length, petal width\n",
    "\n",
    "# petal_length\n",
    "# pl = df[\"petal length in cm\"]\n",
    "pl = df.iloc[:,2]\n",
    "pw = df.iloc[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class 1\n",
    "# p1_1 = df['class'][df['class'] == cls[0]]\n",
    "df_class_1 = df.loc[df['class'] == 0]\n",
    "# Petal Length\n",
    "pl_1 = df_class_1[df_class_1.columns[2]]\n",
    "pw_1 = df_class_1[df_class_1.columns[3]]\n",
    "\n",
    "# Class 2\n",
    "df_class_2 = df.loc[df['class'] == 1]\n",
    "# Petal Length\n",
    "pl_2 = df_class_2[df_class_2.columns[2]]\n",
    "pw_2 = df_class_2[df_class_2.columns[3]]\n",
    "\n",
    "# Class 3\n",
    "df_class_3 = df.loc[df['class'] == 2]\n",
    "# Petal Length\n",
    "pl_3 = df_class_3[df_class_3.columns[2]]\n",
    "pw_3 = df_class_3[df_class_3.columns[3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_org_df = df[df.columns[2:5]]\n",
    "data_org_np = data_org_df.to_numpy()\n",
    "data_org_lst = data_org_np.tolist()\n",
    "# data_org_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decision_boundary(data_df):\n",
    "#     # Convert to list\n",
    "#     data_org_np = data_df.to_numpy()\n",
    "#     data_org_lst = data_org_np.tolist()\n",
    "    \n",
    "#     # Class\n",
    "#     ys = [row[2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "#     # Input\n",
    "#     xs = [[1] +row[:2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "#     # Maximum values\n",
    "#     exp_val = data_df.iloc[:,0]\n",
    "#     sal_val = data_df.iloc[:,1]\n",
    "    \n",
    "#     rescaled_xs = rescale(xs)\n",
    "#     beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "#     predictions = [predict(x_i, beta) for x_i in rescaled_xs]\n",
    "    \n",
    "#     random.seed(0)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(rescale_xs[0], ys, 0.33)\n",
    "#     x_train\n",
    "\n",
    "#     learning_rate = 0.01\n",
    "\n",
    "#     # pick a random starting point\n",
    "#     beta = [random.random() for _ in range(3)]\n",
    "#     y_train\n",
    "\n",
    "#     with tqdm.trange(2500) as t:\n",
    "#         for epoch in t:\n",
    "#             gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "#             beta = gradient_step(beta, gradient, -learning_rate)\n",
    "#             loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "#             t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "# #     print(t)\n",
    "\n",
    "#     means, stdevs = scale(xs)\n",
    "#     beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#     beta_unscaled\n",
    "\n",
    "#     means, stdevs = scale(xs)\n",
    "#     beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#     beta_unscaled\n",
    "    \n",
    "    \n",
    "#     # min_range = (data_marks.min())\n",
    "#     # min_range = math.floor(min_range[0])\n",
    "#     # max_range = (data_marks.max())\n",
    "#     # max_range = math.ceil(max_range[0])\n",
    "\n",
    "#     min_val = math.floor(np.min(exp_val))\n",
    "#     max_val = math.ceil(np.max(exp_val))\n",
    "\n",
    "#     # x_db = [xi for xi in range(min_val,max_val)]\n",
    "#     x_db = np.linspace(min_val,max_val,len(exp_val))\n",
    "#     y_db = [(-beta_unscaled[1]/beta_unscaled[2]*xi - beta_unscaled[0]/beta_unscaled[2])\n",
    "#             for xi in x_db]\n",
    "#     return x_db,y_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_db_1\n",
    "len(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 1.4, 0.2],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 1.3, 0.2],\n",
       "  [1, 1.5, 0.2],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 1.7, 0.4],\n",
       "  [1, 1.4, 0.3],\n",
       "  [1, 1.5, 0.2],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 1.5, 0.1],\n",
       "  [1, 1.5, 0.2],\n",
       "  [1, 1.6, 0.2],\n",
       "  [1, 1.4, 0.1],\n",
       "  [1, 1.1, 0.1],\n",
       "  [1, 1.2, 0.2],\n",
       "  [1, 1.5, 0.4],\n",
       "  [1, 1.3, 0.4],\n",
       "  [1, 1.4, 0.3],\n",
       "  [1, 1.7, 0.3],\n",
       "  [1, 1.5, 0.3],\n",
       "  [1, 1.7, 0.2],\n",
       "  [1, 1.5, 0.4],\n",
       "  [1, 1.0, 0.2],\n",
       "  [1, 1.7, 0.5],\n",
       "  [1, 1.9, 0.2],\n",
       "  [1, 1.6, 0.2],\n",
       "  [1, 1.6, 0.4],\n",
       "  [1, 1.5, 0.2],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 1.6, 0.2],\n",
       "  [1, 1.6, 0.2],\n",
       "  [1, 1.5, 0.4],\n",
       "  [1, 1.5, 0.1],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 1.5, 0.1],\n",
       "  [1, 1.2, 0.2],\n",
       "  [1, 1.3, 0.2],\n",
       "  [1, 1.5, 0.1],\n",
       "  [1, 1.3, 0.2],\n",
       "  [1, 1.5, 0.2],\n",
       "  [1, 1.3, 0.3],\n",
       "  [1, 1.3, 0.3],\n",
       "  [1, 1.3, 0.2],\n",
       "  [1, 1.6, 0.6],\n",
       "  [1, 1.9, 0.4],\n",
       "  [1, 1.4, 0.3],\n",
       "  [1, 1.6, 0.2],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 1.5, 0.2],\n",
       "  [1, 1.4, 0.2],\n",
       "  [1, 4.7, 1.4],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 4.9, 1.5],\n",
       "  [1, 4.0, 1.3],\n",
       "  [1, 4.6, 1.5],\n",
       "  [1, 4.5, 1.3],\n",
       "  [1, 4.7, 1.6],\n",
       "  [1, 3.3, 1.0],\n",
       "  [1, 4.6, 1.3],\n",
       "  [1, 3.9, 1.4],\n",
       "  [1, 3.5, 1.0],\n",
       "  [1, 4.2, 1.5],\n",
       "  [1, 4.0, 1.0],\n",
       "  [1, 4.7, 1.4],\n",
       "  [1, 3.6, 1.3],\n",
       "  [1, 4.4, 1.4],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 4.1, 1.0],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 3.9, 1.1],\n",
       "  [1, 4.8, 1.8],\n",
       "  [1, 4.0, 1.3],\n",
       "  [1, 4.9, 1.5],\n",
       "  [1, 4.7, 1.2],\n",
       "  [1, 4.3, 1.3],\n",
       "  [1, 4.4, 1.4],\n",
       "  [1, 4.8, 1.4],\n",
       "  [1, 5.0, 1.7],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 3.5, 1.0],\n",
       "  [1, 3.8, 1.1],\n",
       "  [1, 3.7, 1.0],\n",
       "  [1, 3.9, 1.2],\n",
       "  [1, 5.1, 1.6],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 4.5, 1.6],\n",
       "  [1, 4.7, 1.5],\n",
       "  [1, 4.4, 1.3],\n",
       "  [1, 4.1, 1.3],\n",
       "  [1, 4.0, 1.3],\n",
       "  [1, 4.4, 1.2],\n",
       "  [1, 4.6, 1.4],\n",
       "  [1, 4.0, 1.2],\n",
       "  [1, 3.3, 1.0],\n",
       "  [1, 4.2, 1.3],\n",
       "  [1, 4.2, 1.2],\n",
       "  [1, 4.2, 1.3],\n",
       "  [1, 4.3, 1.3],\n",
       "  [1, 3.0, 1.1],\n",
       "  [1, 4.1, 1.3]],\n",
       " [[1, 4.7, 1.4],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 4.9, 1.5],\n",
       "  [1, 4.0, 1.3],\n",
       "  [1, 4.6, 1.5],\n",
       "  [1, 4.5, 1.3],\n",
       "  [1, 4.7, 1.6],\n",
       "  [1, 3.3, 1.0],\n",
       "  [1, 4.6, 1.3],\n",
       "  [1, 3.9, 1.4],\n",
       "  [1, 3.5, 1.0],\n",
       "  [1, 4.2, 1.5],\n",
       "  [1, 4.0, 1.0],\n",
       "  [1, 4.7, 1.4],\n",
       "  [1, 3.6, 1.3],\n",
       "  [1, 4.4, 1.4],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 4.1, 1.0],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 3.9, 1.1],\n",
       "  [1, 4.8, 1.8],\n",
       "  [1, 4.0, 1.3],\n",
       "  [1, 4.9, 1.5],\n",
       "  [1, 4.7, 1.2],\n",
       "  [1, 4.3, 1.3],\n",
       "  [1, 4.4, 1.4],\n",
       "  [1, 4.8, 1.4],\n",
       "  [1, 5.0, 1.7],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 3.5, 1.0],\n",
       "  [1, 3.8, 1.1],\n",
       "  [1, 3.7, 1.0],\n",
       "  [1, 3.9, 1.2],\n",
       "  [1, 5.1, 1.6],\n",
       "  [1, 4.5, 1.5],\n",
       "  [1, 4.5, 1.6],\n",
       "  [1, 4.7, 1.5],\n",
       "  [1, 4.4, 1.3],\n",
       "  [1, 4.1, 1.3],\n",
       "  [1, 4.0, 1.3],\n",
       "  [1, 4.4, 1.2],\n",
       "  [1, 4.6, 1.4],\n",
       "  [1, 4.0, 1.2],\n",
       "  [1, 3.3, 1.0],\n",
       "  [1, 4.2, 1.3],\n",
       "  [1, 4.2, 1.2],\n",
       "  [1, 4.2, 1.3],\n",
       "  [1, 4.3, 1.3],\n",
       "  [1, 3.0, 1.1],\n",
       "  [1, 4.1, 1.3],\n",
       "  [1, 6.0, 2.5],\n",
       "  [1, 5.1, 1.9],\n",
       "  [1, 5.9, 2.1],\n",
       "  [1, 5.6, 1.8],\n",
       "  [1, 5.8, 2.2],\n",
       "  [1, 6.6, 2.1],\n",
       "  [1, 4.5, 1.7],\n",
       "  [1, 6.3, 1.8],\n",
       "  [1, 5.8, 1.8],\n",
       "  [1, 6.1, 2.5],\n",
       "  [1, 5.1, 2.0],\n",
       "  [1, 5.3, 1.9],\n",
       "  [1, 5.5, 2.1],\n",
       "  [1, 5.0, 2.0],\n",
       "  [1, 5.1, 2.4],\n",
       "  [1, 5.3, 2.3],\n",
       "  [1, 5.5, 1.8],\n",
       "  [1, 6.7, 2.2],\n",
       "  [1, 6.9, 2.3],\n",
       "  [1, 5.0, 1.5],\n",
       "  [1, 5.7, 2.3],\n",
       "  [1, 4.9, 2.0],\n",
       "  [1, 6.7, 2.0],\n",
       "  [1, 4.9, 1.8],\n",
       "  [1, 5.7, 2.1],\n",
       "  [1, 6.0, 1.8],\n",
       "  [1, 4.8, 1.8],\n",
       "  [1, 4.9, 1.8],\n",
       "  [1, 5.6, 2.1],\n",
       "  [1, 5.8, 1.6],\n",
       "  [1, 6.1, 1.9],\n",
       "  [1, 6.4, 2.0],\n",
       "  [1, 5.6, 2.2],\n",
       "  [1, 5.1, 1.5],\n",
       "  [1, 5.6, 1.4],\n",
       "  [1, 6.1, 2.3],\n",
       "  [1, 5.6, 2.4],\n",
       "  [1, 5.5, 1.8],\n",
       "  [1, 4.8, 1.8],\n",
       "  [1, 5.4, 2.1],\n",
       "  [1, 5.6, 2.4],\n",
       "  [1, 5.1, 2.3],\n",
       "  [1, 5.1, 1.9],\n",
       "  [1, 5.9, 2.3],\n",
       "  [1, 5.7, 2.5],\n",
       "  [1, 5.2, 2.3],\n",
       "  [1, 5.0, 1.9],\n",
       "  [1, 5.2, 2.0],\n",
       "  [1, 5.4, 2.3],\n",
       "  [1, 5.1, 1.8]]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xs = [[1] +row[:2] for row in data_org_lst if row[2] < 2]\n",
    "# xs\n",
    "ys = [row[2] for row in data_org_lst if row[2] < 2]\n",
    "\n",
    "xs_1 = []\n",
    "# ys = df['class'].to_numpy().tolist()\n",
    "# ys = []\n",
    "for i in range(len(cls)-1):\n",
    "    xs_0 = [[1] +row[:2] for row in data_org_lst if row[2] >= 0+i  and row[2] < len(cls) - 1+i]\n",
    "#     ys_0 = [row[2] for row in data_org_lst if row[2] > 0+i and row[2] < 2+i]\n",
    "    xs_1.append(xs_0)\n",
    "#     ys.append(ys_0)\n",
    "  \n",
    "xs = xs_1[1]\n",
    "\n",
    "# Maximum values\n",
    "exp_val = data_org_df.iloc[:,0]\n",
    "sal_val = data_org_df.iloc[:,1]\n",
    "\n",
    "# onesx = np.ones((len(data_org_lst),1))\n",
    "# xs = np.vstack(onesx,\n",
    "\n",
    "\n",
    "# for row in data_org_lst:\n",
    "#     print(row[2])\n",
    "# xs_1[2]\n",
    "len(xs_1[0])\n",
    "sal_val\n",
    "xs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decision_boundary(data_df,xs,ys):\n",
    "#     # Convert to list\n",
    "# #     data_org_np = data_df.to_numpy()\n",
    "# #     data_org_lst = data_org_np.tolist()\n",
    "    \n",
    "#     # Class\n",
    "# #     ys = [row[2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "#     # Input\n",
    "# #     xs = [[1] +row[:2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "#     # Maximum values\n",
    "#     exp_val = data_df.iloc[:,0]\n",
    "#     sal_val = data_df.iloc[:,1]\n",
    "    \n",
    "#     rescaled_xs = rescale(xs)\n",
    "#     beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "#     predictions = [predict(x_i, beta) for x_i in rescaled_xs]\n",
    "    \n",
    "#     random.seed(0)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(rescale_xs[0], ys, 0.33)\n",
    "#     x_train\n",
    "\n",
    "#     learning_rate = 0.01\n",
    "\n",
    "#     # pick a random starting point\n",
    "#     beta = [random.random() for _ in range(3)]\n",
    "#     y_train\n",
    "\n",
    "#     with tqdm.trange(2500) as t:\n",
    "#         for epoch in t:\n",
    "#             gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "#             beta = gradient_step(beta, gradient, -learning_rate)\n",
    "#             loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "#             t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "# #     print(t)\n",
    "\n",
    "#     means, stdevs = scale(xs)\n",
    "#     beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#     beta_unscaled\n",
    "\n",
    "#     means, stdevs = scale(xs)\n",
    "#     beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#     beta_unscaled\n",
    "    \n",
    "    \n",
    "#     # min_range = (data_marks.min())\n",
    "#     # min_range = math.floor(min_range[0])\n",
    "#     # max_range = (data_marks.max())\n",
    "#     # max_range = math.ceil(max_range[0])\n",
    "\n",
    "#     min_val = math.floor(np.min(exp_val))\n",
    "#     max_val = math.ceil(np.max(exp_val))\n",
    "\n",
    "#     # x_db = [xi for xi in range(min_val,max_val)]\n",
    "#     x_db = np.linspace(min_val,max_val,len(exp_val))\n",
    "#     y_db = [(-beta_unscaled[1]/beta_unscaled[2]*xi - beta_unscaled[0]/beta_unscaled[2])\n",
    "#             for xi in x_db]\n",
    "#     return x_db,y_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decision_boundary(data_df,xs,ys):\n",
    "#     # Convert to list\n",
    "# #     data_org_np = data_df.to_numpy()\n",
    "# #     data_org_lst = data_org_np.tolist()\n",
    "    \n",
    "#     # Class\n",
    "# #     ys = [row[2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "#     # Input\n",
    "# #     xs = [[1] +row[:2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "# #     # Maximum values\n",
    "# #     exp_val = data_df.iloc[:,0]\n",
    "# #     sal_val = data_df.iloc[:,1]\n",
    "    \n",
    "    \n",
    "#     rescaled_xs = rescale(xs)\n",
    "#     beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "#     predictions = [predict(x_i, beta) for x_i in rescaled_xs]\n",
    "    \n",
    "#     random.seed(0)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "#     x_train\n",
    "\n",
    "#     learning_rate = 0.01\n",
    "\n",
    "#     # pick a random starting point\n",
    "#     beta = [random.random() for _ in range(3)]\n",
    "#     y_train\n",
    "\n",
    "#     with tqdm.trange(2500) as t:\n",
    "#         for epoch in t:\n",
    "#             gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "#             beta = gradient_step(beta, gradient, -learning_rate)\n",
    "#             loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "#             t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "# #     print(t)\n",
    "\n",
    "#     means, stdevs = scale(xs)\n",
    "#     beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#     beta_unscaled\n",
    "\n",
    "#     means, stdevs = scale(xs)\n",
    "#     beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#     beta_unscaled\n",
    "    \n",
    "    \n",
    "#     # min_range = (data_marks.min())\n",
    "#     # min_range = math.floor(min_range[0])\n",
    "#     # max_range = (data_marks.max())\n",
    "#     # max_range = math.ceil(max_range[0])\n",
    "\n",
    "#     min_val = math.floor(np.min(exp_val))\n",
    "#     max_val = math.ceil(np.max(exp_val))\n",
    "\n",
    "#     # x_db = [xi for xi in range(min_val,max_val)]\n",
    "#     x_db = np.linspace(min_val,max_val,len(exp_val))\n",
    "#     y_db = [(-beta_unscaled[1]/beta_unscaled[2]*xi - beta_unscaled[0]/beta_unscaled[2])\n",
    "#             for xi in x_db]\n",
    "#     return x_db,y_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "# data_org_np = data__org_df.to_numpy()\n",
    "# data_org_lst = data_org_np.tolist()\n",
    "    \n",
    "    # Class\n",
    "#     ys = [row[2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "    # Input\n",
    "#     xs = [[1] +row[:2] for row in data_org_lst if row[2] < 2]\n",
    "    \n",
    "#     # Maximum values\n",
    "#     exp_val = data_df.iloc[:,0]\n",
    "#     sal_val = data_df.iloc[:,1]\n",
    "def decision_boundary_2(data_df,xs,ys,label):\n",
    "    cls = label\n",
    "\n",
    "# xs_1 = []\n",
    "# # ys = df['class'].to_numpy().tolist()\n",
    "# # ys = []\n",
    "# for i in range(len(cls)-1):\n",
    "#     xs_0 = [[1] +row[:2] for row in data_org_lst if row[2] >= 0+i  and row[2] < len(cls) - 1+i]\n",
    "# #     ys_0 = [row[2] for row in data_org_lst if row[2] > 0+i and row[2] < 2+i]\n",
    "#     xs_1.append(xs_0)\n",
    "# #     ys.append(ys_0)\n",
    "\n",
    "    x_db_lst = []\n",
    "    y_db_lst = []\n",
    "    if len(xs_1) > 1:\n",
    "        for i in range(len(cls)-1):             \n",
    "\n",
    "            xs = xs_1[i]\n",
    "            rescaled_xs = rescale(xs)\n",
    "            beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "            predictions = [predict(x_i, beta) for x_i in rescaled_xs]\n",
    "\n",
    "            random.seed(0)\n",
    "            x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "            x_train\n",
    "\n",
    "            learning_rate = 0.01\n",
    "\n",
    "            # pick a random starting point\n",
    "            beta = [random.random() for _ in range(3)]\n",
    "            y_train\n",
    "\n",
    "            with tqdm.trange(2500) as t:\n",
    "                for epoch in t:\n",
    "                    gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "                    beta = gradient_step(beta, gradient, -learning_rate)\n",
    "                    loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "                    t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "        #     print(t)\n",
    "\n",
    "            means, stdevs = scale(xs)\n",
    "            beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "                             beta[1] / stdevs[1],\n",
    "                             beta[2] / stdevs[2]]\n",
    "            beta_unscaled\n",
    "\n",
    "            means, stdevs = scale(xs)\n",
    "            beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "                             beta[1] / stdevs[1],\n",
    "                             beta[2] / stdevs[2]]\n",
    "            beta_unscaled\n",
    "\n",
    "\n",
    "            # min_range = (data_marks.min())\n",
    "            # min_range = math.floor(min_range[0])\n",
    "            # max_range = (data_marks.max())\n",
    "            # max_range = math.ceil(max_range[0])\n",
    "\n",
    "            min_val = math.floor(np.min(exp_val))\n",
    "            max_val = math.ceil(np.max(exp_val))\n",
    "\n",
    "            # x_db = [xi for xi in range(min_val,max_val)]\n",
    "            x_db = np.linspace(min_val,max_val,len(exp_val))\n",
    "            x_db_lst.append(x_db)\n",
    "            y_db = [(-beta_unscaled[1]/beta_unscaled[2]*xi - beta_unscaled[0]/beta_unscaled[2])\n",
    "                    for xi in x_db]\n",
    "            y_db_lst.append(y_db)\n",
    "    return x_db_lst,y_db_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17656/2746858823.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
      "least squares fit: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 284.25it/s]\n",
      "  0%|                                                                                                                                             | 0/2500 [00:00<?, ?it/s]C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17656/2746858823.py:15: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return [sum(vector[i] for vector in vectors)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17656/2746858823.py:164: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(_negative_log_likelihood(x, y, beta)\n",
      "loss: 0.060 beta: [1.15290674 5.36580482 4.4417838 ]: 100%|███████████████████████████████████████████████████████████████████████████| 2500/2500 [00:10<00:00, 229.66it/s]\n",
      "least squares fit: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 256.15it/s]\n",
      "loss: 3.269 beta: [0.01815226 2.26003596 9.86876876]: 100%|███████████████████████████████████████████████████████████████████████████| 2500/2500 [00:10<00:00, 236.28it/s]\n"
     ]
    }
   ],
   "source": [
    "x_db_lst,y_db_lst = decision_boundary_2(data_org_df,xs_1,ys,cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_db_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_db_1,y_db_1 = decision_boundary(data_org_df,xs_1[0],ys)\n",
    "# x_db_2,y_db_2 = decision_boundary(data_org_df,xs_1[1],ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter Plots\n",
    "\n",
    "plt.scatter(pl_1,pw_1, color = 'red', marker = '+')\n",
    "plt.scatter(pl_2,pw_2, color = 'blue', marker = 'o')\n",
    "# plt.scatter(pl_3,pw_3, color = 'green', marker = '*')\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs = [(1,) + row[:2] for row in data_org]\n",
    "# ys = [row[2] for row in data_org]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm, nn = scale(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale_xs = []\n",
    "# beta = []\n",
    "# pred = []\n",
    "\n",
    "# for i in range(len(cls)-2):\n",
    "#     rescaled_xs = rescale(xs[i])\n",
    "#     rescale_xs.append(rescaled_xs)\n",
    "#     beta_0 = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "#     beta.append(beta_0)\n",
    "#     predictions = [predict(x_i, beta_0) for x_i in rescaled_xs]\n",
    "#     pred.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaled_xs = rescale(xs_1[0])\n",
    "# beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "# predictions = [predict(x_i, beta) for x_i in rescaled_xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(0)\n",
    "# # x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "# # x_train\n",
    "\n",
    "# learning_rate = 0.01\n",
    "\n",
    "# # pick a random starting point\n",
    "# beta = [random.random() for _ in range(3)]\n",
    "# # y_train\n",
    "\n",
    "# with tqdm.trange(2500) as t:\n",
    "#     for epoch in t:\n",
    "#         gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "#         beta = gradient_step(beta, gradient, -learning_rate)\n",
    "#         loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "#         t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "# print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5488135039273248, 0.7151893663724195, 0.6027633760716439]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means, stdevs = scale(xs)\n",
    "# beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                  beta[1] / stdevs[1],\n",
    "#                  beta[2] / stdevs[2]]\n",
    "# beta_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17656/2746858823.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(v_i * w_i for v_i, w_i in zip(v, w))\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for i in range(len(beta)-1):\n",
    "\n",
    "    rescaled_xs = rescale(xs_1[i])\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "\n",
    "\n",
    "    true_positives = false_positives = true_negatives = false_negatives = 0\n",
    "\n",
    "    for x_i, y_i in zip(x_test, y_test):\n",
    "        prediction = logistic(dot(beta, x_i))\n",
    "\n",
    "        if y_i == 1 and prediction >= 0.5:  # TP: paid and we predict paid\n",
    "            true_positives += 1\n",
    "        elif y_i == 1:                      # FN: paid and we predict unpaid\n",
    "            false_negatives += 1\n",
    "        elif prediction >= 0.5:             # FP: unpaid and we predict paid\n",
    "            false_positives += 1\n",
    "        else:                               # TN: unpaid and we predict unpaid\n",
    "            true_negatives += 1\n",
    "\n",
    "    precision_0 = true_positives / (true_positives + false_positives)\n",
    "    precision.append(precision_0)\n",
    "    recall_0 = true_positives / (true_positives + false_negatives)\n",
    "    recall.append(recall_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu_1 = np.mean([xi[1] for xi in xs])\n",
    "# sig_1 = standard_deviation([xi[1] for xi in xs])\n",
    "\n",
    "# mu_2 = np.mean([xi[2] for xi in xs])\n",
    "# sig_2 = standard_deviation([xi[2] for xi in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # min_range = (data_marks.min())\n",
    "# # min_range = math.floor(min_range[0])\n",
    "# # max_range = (data_marks.max())\n",
    "# # max_range = math.ceil(max_range[0])\n",
    "\n",
    "# min_val = math.floor(np.min(exp_val))\n",
    "# max_val = math.ceil(np.max(exp_val))\n",
    "\n",
    "# # x_db = [xi for xi in range(min_val,max_val)]\n",
    "# x_db = np.linspace(min_val,max_val,len(exp_val))\n",
    "# y_db = [(-beta_unscaled[1]/beta_unscaled[2]*xi - beta_unscaled[0]/beta_unscaled[2])\n",
    "#         for xi in x_db]\n",
    "# y_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(pl_1,pw_1, color = 'red', marker = '+')\n",
    "plt.scatter(pl_2,pw_2, color = 'blue', marker = 'o')\n",
    "plt.scatter(pl_3,pw_3, color = 'green', marker = '*')\n",
    "# plt.plot(x_db_1, y_db_1, 'purple')\n",
    "# plt.plot(x_db_2, y_db_2, 'darkorange')\n",
    "# plt.plot(x_db, y_db, 'purple')\n",
    "plt.plot(x_db_lst[0], y_db_lst[0], 'purple')\n",
    "plt.plot(x_db_lst[1], y_db_lst[1], 'darkorange')\n",
    "plt.xlabel('experience')\n",
    "plt.ylabel('salary')\n",
    "# plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Paid and Unpaid Users')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
