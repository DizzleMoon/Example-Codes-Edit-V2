{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "# import pygal\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import urllib.request\n",
    "import requests\n",
    "import curl\n",
    "import pycurl\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from IPython import qt\n",
    "from matplotlib.pyplot import figure\n",
    "from py.xml import raw\n",
    "from requests.api import get\n",
    "from matplotlib import pyplot as plt\n",
    "# from scratch.working_with_data import rescale\n",
    "# from scratch.multiple_regression import least_squares_fit, predict\n",
    "# from scratch.gradient_descent import gradient_step\n",
    "\n",
    "# from stats import mean, median, de_mean, standard_deviation, correlation\n",
    "# from gradient_descent import minimize_stochastic, maximize_stochastic, maximize_batch\n",
    "# from vector import dot, vector_add\n",
    "# from normal import normal_cdf\n",
    "# from matrix import make_matrix, get_column, shape, matrix_multiply\n",
    "# from logistic_regression import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "\n",
    "from scipy.optimize import fmin_tnc\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from typing import*\n",
    "\n",
    "from collections import*\n",
    "# from scipy import*\n",
    "from sklearn.metrics import*\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "# import mlxtend\n",
    "\n",
    "# bltin_sum = np.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# def add(a, b): return a + b\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c , v):\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = np.sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = len(data[0])\n",
    "    \n",
    "    # Vector Mean\n",
    "#     n = len(data)\n",
    "#     m = np.sum(data,axis=0)\n",
    "#     means = np.multiply(1/n,m)\n",
    "    means = vector_mean(data)\n",
    "    \n",
    "    # Standard Deviaiton\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "    return means,stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = len(data[0])\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    grad_step = np.add(v,step)\n",
    "    return grad_step\n",
    "\n",
    "# def predict(alpha, beta, x_i):\n",
    "#     pred = beta * x_i + alpha\n",
    "#     return pred\n",
    "\n",
    "# def error(x, y, beta):\n",
    "#     \"\"\"\n",
    "#     The error from predicting beta * x_i + alpha\n",
    "#     when the actual value is y_i\n",
    "#     \"\"\"\n",
    "#     err_fin = predict(alpha, beta, x_i) - y_i\n",
    "#     return err_fin\n",
    "\n",
    "def predict(x, beta):\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x, y, beta):\n",
    "    return predict(x, beta) - y \n",
    "\n",
    "def sqerror_gradient(x, y, beta):\n",
    "    err = error(x, y, beta)\n",
    "    err_fin = [2 * err * x_i for x_i in x]\n",
    "    return err_fin\n",
    "\n",
    "def least_squares_fit(xs, ys, learning_rate = 0.001, num_steps = 1000, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [np.random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def _negative_log_likelihood(x, y, beta):\n",
    "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))\n",
    "    \n",
    "def negative_log_likelihood(xs, ys, beta):\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))\n",
    "\n",
    "def _negative_log_partial_j(x, y, beta, j):\n",
    "    \"\"\"\n",
    "    The jth partial derivative for one data point.\n",
    "    Here i is the index of the data point.\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x, y, beta):\n",
    "    \"\"\"\n",
    "    The gradient for one data point.\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs, ys,beta):\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "                                                                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoding:\n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y_enc = (np.arange(np.max(y) + 1) == y[:, None]).astype(float)\n",
    "\n",
    "print('one-hot encoding:\\n', y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs X:\n",
      " [[ 0.1  0.5]\n",
      " [ 1.1  2.3]\n",
      " [-1.1 -2.3]\n",
      " [-1.5 -2.5]]\n",
      "\n",
      "Weights W:\n",
      " [[0.1 0.2 0.3]\n",
      " [0.1 0.2 0.3]]\n",
      "\n",
      "bias:\n",
      " [0.01 0.1  0.1 ]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.1, 0.5],\n",
    "              [1.1, 2.3],\n",
    "              [-1.1, -2.3],\n",
    "              [-1.5, -2.5]])\n",
    "\n",
    "W = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.1, 0.2, 0.3]])\n",
    "\n",
    "bias = np.array([0.01, 0.1, 0.1])\n",
    "\n",
    "print('Inputs X:\\n', X)\n",
    "print('\\nWeights W:\\n', W)\n",
    "print('\\nbias:\\n', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs X:\n",
      " [[ 0.1  0.5]\n",
      " [ 1.1  2.3]\n",
      " [-1.1 -2.3]\n",
      " [-1.5 -2.5]]\n",
      "\n",
      "Weights W:\n",
      " [[0.1 0.2 0.3]\n",
      " [0.1 0.2 0.3]]\n",
      "\n",
      "bias:\n",
      " [0.01 0.1  0.1 ]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.1, 0.5],\n",
    "              [1.1, 2.3],\n",
    "              [-1.1, -2.3],\n",
    "              [-1.5, -2.5]])\n",
    "\n",
    "W = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.1, 0.2, 0.3]])\n",
    "\n",
    "bias = np.array([0.01, 0.1, 0.1])\n",
    "\n",
    "print('Inputs X:\\n', X)\n",
    "print('\\nWeights W:\\n', W)\n",
    "print('\\nbias:\\n', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net input:\n",
      " [[ 0.07  0.22  0.28]\n",
      " [ 0.35  0.78  1.12]\n",
      " [-0.33 -0.58 -0.92]\n",
      " [-0.39 -0.7  -1.1 ]]\n"
     ]
    }
   ],
   "source": [
    "def net_input(X, W, b):\n",
    "    return (X.dot(W) + b)\n",
    "\n",
    "net_in = net_input(X, W, bias)\n",
    "print('net input:\\n', net_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax:\n",
      " [[0.29450637 0.34216758 0.36332605]\n",
      " [0.21290077 0.32728332 0.45981591]\n",
      " [0.42860913 0.33380113 0.23758974]\n",
      " [0.44941979 0.32962558 0.22095463]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "smax = softmax(net_in)\n",
    "print('softmax:\\n', smax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class labels:  [2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "def to_classlabel(z):\n",
    "    return z.argmax(axis=1)\n",
    "\n",
    "print('predicted class labels: ', to_classlabel(smax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: [1.22245465 1.11692907 1.43720989 1.50979788]\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(output, y_target):\n",
    "    return - np.sum(np.log(output) * (y_target), axis=1)\n",
    "\n",
    "xent = cross_entropy(smax, y_enc)\n",
    "print('Cross Entropy:', xent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  1.3215978715930938\n"
     ]
    }
   ],
   "source": [
    "def cost(output, y_target):\n",
    "    return np.mean(cross_entropy(output, y_target))\n",
    "\n",
    "J_cost = cost(smax, y_enc)\n",
    "print('Cost: ', J_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastian Raschka 2014-2020\n",
    "# mlxtend Machine Learning Library Extensions\n",
    "#\n",
    "# Base Clusteer (Clutering Parent Class)\n",
    "# Author: Sebastian Raschka <sebastianraschka.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    from inspect import signature\n",
    "except ImportError:\n",
    "    from ..externals.signature_py27 import signature\n",
    "\n",
    "\n",
    "class _BaseModel(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._init_time = time()\n",
    "\n",
    "    def _check_arrays(self, X, y=None):\n",
    "        if isinstance(X, list):\n",
    "            raise ValueError('X must be a numpy array')\n",
    "        if not len(X.shape) == 2:\n",
    "            raise ValueError('X must be a 2D array. Try X[:, numpy.newaxis]')\n",
    "        try:\n",
    "            if y is None:\n",
    "                return\n",
    "        except(AttributeError):\n",
    "            if not len(y.shape) == 1:\n",
    "                raise ValueError('y must be a 1D array.')\n",
    "\n",
    "        if not len(y) == X.shape[0]:\n",
    "            raise ValueError('X and y must contain the same number of samples')\n",
    "\n",
    "    @classmethod\n",
    "    def _get_param_names(cls):\n",
    "        \"\"\"Get parameter names for the estimator\n",
    "        adapted from\n",
    "        https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py\n",
    "        Author: Gael Varoquaux <gael.varoquaux@normalesup.org>\n",
    "        License: BSD 4 clause\n",
    "        \"\"\"\n",
    "        # fetch the constructor or the original constructor before\n",
    "        # deprecation wrapping if any\n",
    "        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n",
    "        if init is object.__init__:\n",
    "            # No explicit constructor to introspect\n",
    "            return []\n",
    "\n",
    "        # introspect the constructor arguments to find the model parameters\n",
    "        # to represent\n",
    "        init_signature = signature(init)\n",
    "        # Consider the constructor parameters excluding 'self'\n",
    "        parameters = [p for p in init_signature.parameters.values()\n",
    "                      if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n",
    "        for p in parameters:\n",
    "            if p.kind == p.VAR_POSITIONAL:\n",
    "                raise RuntimeError(\"scikit-learn estimators should always \"\n",
    "                                   \"specify their parameters in the signature\"\n",
    "                                   \" of their __init__ (no varargs).\"\n",
    "                                   \" %s with constructor %s doesn't \"\n",
    "                                   \" follow this convention.\"\n",
    "                                   % (cls, init_signature))\n",
    "        # Extract and sort argument names excluding 'self'\n",
    "        return sorted([p.name for p in parameters])\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator.\n",
    "        Parameters\n",
    "        ----------\n",
    "        deep : boolean, optional\n",
    "            If True, will return the parameters for this estimator and\n",
    "            contained subobjects that are estimators.\n",
    "        Returns\n",
    "        -------\n",
    "        params : mapping of string to any\n",
    "            Parameter names mapped to their values.'\n",
    "        adapted from\n",
    "        https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py\n",
    "        Author: Gael Varoquaux <gael.varoquaux@normalesup.org>\n",
    "        License: BSD 3 clause\n",
    "        \"\"\"\n",
    "        out = dict()\n",
    "        for key in self._get_param_names():\n",
    "            value = getattr(self, key, None)\n",
    "            if deep and hasattr(value, 'get_params'):\n",
    "                deep_items = value.get_params().items()\n",
    "                out.update((key + '__' + k, val) for k, val in deep_items)\n",
    "            out[key] = value\n",
    "        return out\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set the parameters of this estimator.\n",
    "        The method works on simple estimators as well as on nested objects\n",
    "        (such as pipelines). The latter have parameters of the form\n",
    "        ``<component>__<parameter>`` so that it's possible to update each\n",
    "        component of a nested object.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        adapted from\n",
    "        https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py\n",
    "        Author: Gael Varoquaux <gael.varoquaux@normalesup.org>\n",
    "        License: BSD 3 clause\n",
    "        \"\"\"\n",
    "        if not params:\n",
    "            # Simple optimization to gain speed (inspect is slow)\n",
    "            return self\n",
    "        valid_params = self.get_params(deep=True)\n",
    "\n",
    "        nested_params = defaultdict(dict)  # grouped by prefix\n",
    "        for key, value in params.items():\n",
    "            key, delim, sub_key = key.partition('__')\n",
    "            if key not in valid_params:\n",
    "                raise ValueError('Invalid parameter %s for estimator %s. '\n",
    "                                 'Check the list of available parameters '\n",
    "                                 'with `estimator.get_params().keys()`.' %\n",
    "                                 (key, self))\n",
    "\n",
    "            if delim:\n",
    "                nested_params[key][sub_key] = value\n",
    "            else:\n",
    "                setattr(self, key, value)\n",
    "                valid_params[key] = value\n",
    "\n",
    "        for key, sub_params in nested_params.items():\n",
    "            valid_params[key].set_params(**sub_params)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastian Raschka 2014-2020\n",
    "# mlxtend Machine Learning Library Extensions\n",
    "#\n",
    "# Base Clusteer (Clutering Parent Class)\n",
    "# Author: Sebastian Raschka <sebastianraschka.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "from sys import stderr\n",
    "from time import time\n",
    "\n",
    "\n",
    "class _IterativeModel(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _shuffle_arrays(self, arrays):\n",
    "        \"\"\"Shuffle arrays in unison.\"\"\"\n",
    "        r = np.random.permutation(len(arrays[0]))\n",
    "        return [ary[r] for ary in arrays]\n",
    "\n",
    "    def _print_progress(self, iteration, n_iter,\n",
    "                        cost=None, time_interval=10):\n",
    "        if self.print_progress > 0:\n",
    "            s = '\\rIteration: %d/%d' % (iteration, n_iter)\n",
    "            if cost:\n",
    "                s += ' | Cost %.2f' % cost\n",
    "            if self.print_progress > 1:\n",
    "                if not hasattr(self, 'ela_str_'):\n",
    "                    self.ela_str_ = '00:00:00'\n",
    "                if not iteration % time_interval:\n",
    "                    ela_sec = time() - self._init_time\n",
    "                    self.ela_str_ = self._to_hhmmss(ela_sec)\n",
    "                s += ' | Elapsed: %s' % self.ela_str_\n",
    "                if self.print_progress > 2:\n",
    "                    if not hasattr(self, 'eta_str_'):\n",
    "                        self.eta_str_ = '00:00:00'\n",
    "                    if not iteration % time_interval:\n",
    "                        eta_sec = ((ela_sec / float(iteration)) *\n",
    "                                   n_iter - ela_sec)\n",
    "                        if eta_sec < 0.0:\n",
    "                            eta_sec = 0.0\n",
    "                        self.eta_str_ = self._to_hhmmss(eta_sec)\n",
    "                    s += ' | ETA: %s' % self.eta_str_\n",
    "            stderr.write(s)\n",
    "            stderr.flush()\n",
    "\n",
    "    def _to_hhmmss(self, sec):\n",
    "        m, s = divmod(sec, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        return \"%d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "    def _yield_minibatches_idx(self, rgen, n_batches, data_ary, shuffle=True):\n",
    "            indices = np.arange(data_ary.shape[0])\n",
    "\n",
    "            if shuffle:\n",
    "                indices = rgen.permutation(indices)\n",
    "            if n_batches > 1:\n",
    "                remainder = data_ary.shape[0] % n_batches\n",
    "\n",
    "                if remainder:\n",
    "                    minis = np.array_split(indices[:-remainder], n_batches)\n",
    "                    minis[-1] = np.concatenate((minis[-1],\n",
    "                                                indices[-remainder:]),\n",
    "                                               axis=0)\n",
    "                else:\n",
    "                    minis = np.array_split(indices, n_batches)\n",
    "\n",
    "            else:\n",
    "                minis = (indices,)\n",
    "\n",
    "            for idx_batch in minis:\n",
    "                yield idx_batch\n",
    "\n",
    "    def _init_params(self, weights_shape, bias_shape=(1,),\n",
    "                     random_seed=None, dtype='float64', scale=0.01,\n",
    "                     bias_const=0.0):\n",
    "        \"\"\"Initialize weight coefficients.\"\"\"\n",
    "        rgen = np.random.RandomState(random_seed)\n",
    "        w = rgen.normal(loc=0.0, scale=scale, size=weights_shape)\n",
    "        b = np.zeros(shape=bias_shape)\n",
    "        if bias_const != 0.0:\n",
    "            b += bias_const\n",
    "        return b.astype(dtype), w.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastian Raschka 2014-2020\n",
    "# mlxtend Machine Learning Library Extensions\n",
    "#\n",
    "# Base Clusteer (Clutering Parent Class)\n",
    "# Author: Sebastian Raschka <sebastianraschka.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class _MultiClass(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _one_hot(self, y, n_labels, dtype):\n",
    "        \"\"\"Returns a matrix where each sample in y is represented\n",
    "           as a row, and each column represents the class label in\n",
    "           the one-hot encoding scheme.\n",
    "        Example:\n",
    "            y = np.array([0, 1, 2, 3, 4, 2])\n",
    "            mc = _BaseMultiClass()\n",
    "            mc._one_hot(y=y, n_labels=5, dtype='float')\n",
    "            np.array([[1., 0., 0., 0., 0.],\n",
    "                      [0., 1., 0., 0., 0.],\n",
    "                      [0., 0., 1., 0., 0.],\n",
    "                      [0., 0., 0., 1., 0.],\n",
    "                      [0., 0., 0., 0., 1.],\n",
    "                      [0., 0., 1., 0., 0.]])\n",
    "        \"\"\"\n",
    "        mat = np.zeros((len(y), n_labels))\n",
    "        for i, val in enumerate(y):\n",
    "            mat[i, val] = 1\n",
    "        return mat.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "\n",
    "class _Classifier(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _check_target_array(self, y, allowed=None):\n",
    "        if not isinstance(y[0], (int, np.integer)):\n",
    "            raise AttributeError('y must be an integer array.\\nFound %s'\n",
    "                                 % y.dtype)\n",
    "        found_labels = np.unique(y)\n",
    "        if (found_labels < 0).any():\n",
    "            raise AttributeError('y array must not contain negative labels.'\n",
    "                                 '\\nFound %s' % found_labels)\n",
    "        if allowed is not None:\n",
    "            found_labels = tuple(found_labels)\n",
    "            if found_labels not in allowed:\n",
    "                raise AttributeError('Labels not in %s.\\nFound %s'\n",
    "                                     % (allowed, found_labels))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute the prediction accuracy\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values (true class labels).\n",
    "        Returns\n",
    "        ---------\n",
    "        acc : float\n",
    "            The prediction accuracy as a float\n",
    "            between 0.0 and 1.0 (perfect score).\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        acc = np.sum(y == y_pred, axis=0) / float(X.shape[0])\n",
    "        return acc\n",
    "\n",
    "    def fit(self, X, y, init_params=True):\n",
    "        \"\"\"Learn model from training data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "        init_params : bool (default: True)\n",
    "            Re-initializes model parameters prior to fitting.\n",
    "            Set False to continue training with weights from\n",
    "            a previous model fitting.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        self._is_fitted = False\n",
    "        self._check_arrays(X=X, y=y)\n",
    "        self._check_target_array(y)\n",
    "        if hasattr(self, 'self.random_seed') and self.random_seed:\n",
    "            self._rgen = np.random.RandomState(self.random_seed)\n",
    "        self._init_time = time()\n",
    "        self._fit(X=X, y=y, init_params=init_params)\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict targets from X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        Returns\n",
    "        ----------\n",
    "        target_values : array-like, shape = [n_samples]\n",
    "          Predicted target values.\n",
    "        \"\"\"\n",
    "        self._check_arrays(X=X)\n",
    "        if not self._is_fitted:\n",
    "            raise AttributeError('Model is not fitted, yet.')\n",
    "        return self._predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Sebastian Raschka 2014-2020\n",
    "# mlxtend Machine Learning Library Extensions\n",
    "#\n",
    "# Implementation of the mulitnomial logistic regression algorithm for\n",
    "# classification.\n",
    "\n",
    "# Author: Sebastian Raschka <sebastianraschka.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "# from .._base import _BaseModel\n",
    "# from .._base import _IterativeModel\n",
    "# from .._base import _MultiClass\n",
    "# from .._base import _Classifier\n",
    "\n",
    "\n",
    "class SoftmaxRegression(_BaseModel, _IterativeModel,\n",
    "                        _Classifier,  _MultiClass):\n",
    "\n",
    "    \"\"\"Softmax regression classifier.\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float (default: 0.01)\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    epochs : int (default: 50)\n",
    "        Passes over the training dataset.\n",
    "        Prior to each epoch, the dataset is shuffled\n",
    "        if `minibatches > 1` to prevent cycles in stochastic gradient descent.\n",
    "    l2 : float\n",
    "        Regularization parameter for L2 regularization.\n",
    "        No regularization if l2=0.0.\n",
    "    minibatches : int (default: 1)\n",
    "        The number of minibatches for gradient-based optimization.\n",
    "        If 1: Gradient Descent learning\n",
    "        If len(y): Stochastic Gradient Descent (SGD) online learning\n",
    "        If 1 < minibatches < len(y): SGD Minibatch learning\n",
    "    n_classes : int (default: None)\n",
    "        A positive integer to declare the number of class labels\n",
    "        if not all class labels are present in a partial training set.\n",
    "        Gets the number of class labels automatically if None.\n",
    "    random_seed : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "    print_progress : int (default: 0)\n",
    "        Prints progress in fitting to stderr.\n",
    "        0: No output\n",
    "        1: Epochs elapsed and cost\n",
    "        2: 1 plus time elapsed\n",
    "        3: 2 plus estimated time until completion\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 2d-array, shape={n_features, 1}\n",
    "      Model weights after fitting.\n",
    "    b_ : 1d-array, shape={1,}\n",
    "      Bias unit after fitting.\n",
    "    cost_ : list\n",
    "        List of floats, the average cross_entropy for each epoch.\n",
    "    Examples\n",
    "    -----------\n",
    "    For usage examples, please see\n",
    "    http://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, epochs=50,\n",
    "                 l2=0.0,\n",
    "                 minibatches=1,\n",
    "                 n_classes=None,\n",
    "                 random_seed=None,\n",
    "                 print_progress=0):\n",
    "\n",
    "        _BaseModel.__init__(self)\n",
    "        _IterativeModel.__init__(self)\n",
    "        _Classifier.__init__(self)\n",
    "        _MultiClass.__init__(self)\n",
    "\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.l2 = l2\n",
    "        self.minibatches = minibatches\n",
    "        self.n_classes = n_classes\n",
    "        self.random_seed = random_seed\n",
    "        self.print_progress = print_progress\n",
    "        self._is_fitted = False\n",
    "\n",
    "    def _net_input(self, X, W, b):\n",
    "        return (X.dot(W) + b)\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        e_x = np.exp(z - z.max(axis=1, keepdims=True))\n",
    "        out = e_x / e_x.sum(axis=1, keepdims=True)\n",
    "        return out\n",
    "        # return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "    def _cross_entropy(self, output, y_target):\n",
    "        return - np.sum(np.log(output) * (y_target), axis=1)\n",
    "\n",
    "    def _cost(self, cross_entropy):\n",
    "        L2_term = self.l2 * np.sum(self.w_ ** 2)\n",
    "        cross_entropy = cross_entropy + L2_term\n",
    "        return 0.5 * np.mean(cross_entropy)\n",
    "\n",
    "    def _to_classlabels(self, z):\n",
    "        return z.argmax(axis=1)\n",
    "\n",
    "    def _fit(self, X, y, init_params=True):\n",
    "        self._check_target_array(y)\n",
    "        if init_params:\n",
    "            if self.n_classes is None:\n",
    "                self.n_classes = np.max(y) + 1\n",
    "            self._n_features = X.shape[1]\n",
    "\n",
    "            self.b_, self.w_ = self._init_params(\n",
    "                weights_shape=(self._n_features, self.n_classes),\n",
    "                bias_shape=(self.n_classes,),\n",
    "                random_seed=self.random_seed)\n",
    "            self.cost_ = []\n",
    "\n",
    "        y_enc = self._one_hot(y=y, n_labels=self.n_classes, dtype=np.float)\n",
    "\n",
    "        self.init_time_ = time()\n",
    "        rgen = np.random.RandomState(self.random_seed)\n",
    "        for i in range(self.epochs):\n",
    "            for idx in self._yield_minibatches_idx(\n",
    "                    rgen=rgen,\n",
    "                    n_batches=self.minibatches,\n",
    "                    data_ary=y,\n",
    "                    shuffle=True):\n",
    "\n",
    "                # givens:\n",
    "                # w_ -> n_feat x n_classes\n",
    "                # b_  -> n_classes\n",
    "\n",
    "                # net_input, softmax and diff -> n_samples x n_classes:\n",
    "                net = self._net_input(X[idx], self.w_, self.b_)\n",
    "                softm = self._softmax(net)\n",
    "                diff = softm - y_enc[idx]\n",
    "\n",
    "                # gradient -> n_features x n_classes\n",
    "                grad = np.dot(X[idx].T, diff)\n",
    "\n",
    "                # update in opp. direction of the cost gradient\n",
    "                self.w_ -= (self.eta * grad +\n",
    "                            self.eta * self.l2 * self.w_)\n",
    "                self.b_ -= (self.eta * np.sum(diff, axis=0))\n",
    "\n",
    "            # compute cost of the whole epoch\n",
    "            net = self._net_input(X, self.w_, self.b_)\n",
    "            softm = self._softmax(net)\n",
    "            cross_ent = self._cross_entropy(output=softm, y_target=y_enc)\n",
    "            cost = self._cost(cross_ent)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            if self.print_progress:\n",
    "                self._print_progress(iteration=i + 1,\n",
    "                                     n_iter=self.epochs,\n",
    "                                     cost=cost)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities of X from the net input.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        Returns\n",
    "        ----------\n",
    "        Class probabilties : array-like, shape= [n_samples, n_classes]\n",
    "        \"\"\"\n",
    "        net = self._net_input(X, self.w_, self.b_)\n",
    "        softm = self._softmax(net)\n",
    "        return softm\n",
    "\n",
    "    def _predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return self._to_classlabels(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('iris.dat', names=[ \"sepal length in cm\",  \"sepal width in cm\", \"petal length in cm\", \"petal width in cm\",\n",
    "  \"class\"])\n",
    "df.head()\n",
    "\n",
    "clr = []\n",
    "for i in range(len(df['class'])):\n",
    "    if df['class'][i] not in clr:\n",
    "        clr.append(df['class'][i])\n",
    "clr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_37972/1538601023.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[i] = 0\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_37972/1538601023.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[i] = 1\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_37972/1538601023.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[i] = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-8.97673879e-01, -1.30859282e+00],\n",
       "       [-1.13920048e+00, -1.30859282e+00],\n",
       "       [-1.38072709e+00, -1.30859282e+00],\n",
       "       [-1.50149039e+00, -1.30859282e+00],\n",
       "       [-1.01843718e+00, -1.30859282e+00],\n",
       "       [-5.35383973e-01, -1.04652483e+00],\n",
       "       [-1.50149039e+00, -1.17755883e+00],\n",
       "       [-1.01843718e+00, -1.30859282e+00],\n",
       "       [-1.74301699e+00, -1.30859282e+00],\n",
       "       [-1.13920048e+00, -1.43962681e+00],\n",
       "       [-5.35383973e-01, -1.30859282e+00],\n",
       "       [-1.25996379e+00, -1.30859282e+00],\n",
       "       [-1.25996379e+00, -1.43962681e+00],\n",
       "       [-1.86378030e+00, -1.43962681e+00],\n",
       "       [-5.23307643e-02, -1.30859282e+00],\n",
       "       [-1.73094066e-01, -1.04652483e+00],\n",
       "       [-5.35383973e-01, -1.04652483e+00],\n",
       "       [-8.97673879e-01, -1.17755883e+00],\n",
       "       [-1.73094066e-01, -1.17755883e+00],\n",
       "       [-8.97673879e-01, -1.17755883e+00],\n",
       "       [-5.35383973e-01, -1.30859282e+00],\n",
       "       [-8.97673879e-01, -1.04652483e+00],\n",
       "       [-1.50149039e+00, -1.30859282e+00],\n",
       "       [-8.97673879e-01, -9.15490838e-01],\n",
       "       [-1.25996379e+00, -1.30859282e+00],\n",
       "       [-1.01843718e+00, -1.30859282e+00],\n",
       "       [-1.01843718e+00, -1.04652483e+00],\n",
       "       [-7.76910577e-01, -1.30859282e+00],\n",
       "       [-7.76910577e-01, -1.30859282e+00],\n",
       "       [-1.38072709e+00, -1.30859282e+00],\n",
       "       [-1.25996379e+00, -1.30859282e+00],\n",
       "       [-5.35383973e-01, -1.04652483e+00],\n",
       "       [-7.76910577e-01, -1.43962681e+00],\n",
       "       [-4.14620671e-01, -1.30859282e+00],\n",
       "       [-1.13920048e+00, -1.43962681e+00],\n",
       "       [-1.01843718e+00, -1.30859282e+00],\n",
       "       [-4.14620671e-01, -1.30859282e+00],\n",
       "       [-1.13920048e+00, -1.43962681e+00],\n",
       "       [-1.74301699e+00, -1.30859282e+00],\n",
       "       [-8.97673879e-01, -1.30859282e+00],\n",
       "       [-1.01843718e+00, -1.17755883e+00],\n",
       "       [-1.62225369e+00, -1.17755883e+00],\n",
       "       [-1.74301699e+00, -1.30859282e+00],\n",
       "       [-1.01843718e+00, -7.84456844e-01],\n",
       "       [-8.97673879e-01, -1.04652483e+00],\n",
       "       [-1.25996379e+00, -1.17755883e+00],\n",
       "       [-8.97673879e-01, -1.30859282e+00],\n",
       "       [-1.50149039e+00, -1.30859282e+00],\n",
       "       [-6.56147275e-01, -1.30859282e+00],\n",
       "       [-1.01843718e+00, -1.30859282e+00],\n",
       "       [ 1.39682886e+00,  2.63815108e-01],\n",
       "       [ 6.72249049e-01,  3.94849102e-01],\n",
       "       [ 1.27606556e+00,  3.94849102e-01],\n",
       "       [-4.14620671e-01,  1.32781114e-01],\n",
       "       [ 7.93012351e-01,  3.94849102e-01],\n",
       "       [-1.73094066e-01,  1.32781114e-01],\n",
       "       [ 5.51485746e-01,  5.25883096e-01],\n",
       "       [-1.13920048e+00, -2.60320868e-01],\n",
       "       [ 9.13775653e-01,  1.32781114e-01],\n",
       "       [-7.76910577e-01,  2.63815108e-01],\n",
       "       [-1.01843718e+00, -2.60320868e-01],\n",
       "       [ 6.84325379e-02,  3.94849102e-01],\n",
       "       [ 1.89195840e-01, -2.60320868e-01],\n",
       "       [ 3.09959142e-01,  2.63815108e-01],\n",
       "       [-2.93857369e-01,  1.32781114e-01],\n",
       "       [ 1.03453895e+00,  2.63815108e-01],\n",
       "       [-2.93857369e-01,  3.94849102e-01],\n",
       "       [-5.23307643e-02, -2.60320868e-01],\n",
       "       [ 4.30722444e-01,  3.94849102e-01],\n",
       "       [-2.93857369e-01, -1.29286874e-01],\n",
       "       [ 6.84325379e-02,  7.87951084e-01],\n",
       "       [ 3.09959142e-01,  1.32781114e-01],\n",
       "       [ 5.51485746e-01,  3.94849102e-01],\n",
       "       [ 3.09959142e-01,  1.74711992e-03],\n",
       "       [ 6.72249049e-01,  1.32781114e-01],\n",
       "       [ 9.13775653e-01,  2.63815108e-01],\n",
       "       [ 1.15530226e+00,  2.63815108e-01],\n",
       "       [ 1.03453895e+00,  6.56917090e-01],\n",
       "       [ 1.89195840e-01,  3.94849102e-01],\n",
       "       [-1.73094066e-01, -2.60320868e-01],\n",
       "       [-4.14620671e-01, -1.29286874e-01],\n",
       "       [-4.14620671e-01, -2.60320868e-01],\n",
       "       [-5.23307643e-02,  1.74711992e-03],\n",
       "       [ 1.89195840e-01,  5.25883096e-01],\n",
       "       [-5.35383973e-01,  3.94849102e-01],\n",
       "       [ 1.89195840e-01,  5.25883096e-01],\n",
       "       [ 1.03453895e+00,  3.94849102e-01],\n",
       "       [ 5.51485746e-01,  1.32781114e-01],\n",
       "       [-2.93857369e-01,  1.32781114e-01],\n",
       "       [-4.14620671e-01,  1.32781114e-01],\n",
       "       [-4.14620671e-01,  1.74711992e-03],\n",
       "       [ 3.09959142e-01,  2.63815108e-01],\n",
       "       [-5.23307643e-02,  1.74711992e-03],\n",
       "       [-1.01843718e+00, -2.60320868e-01],\n",
       "       [-2.93857369e-01,  1.32781114e-01],\n",
       "       [-1.73094066e-01,  1.74711992e-03],\n",
       "       [-1.73094066e-01,  1.32781114e-01],\n",
       "       [ 4.30722444e-01,  1.32781114e-01],\n",
       "       [-8.97673879e-01, -1.29286874e-01],\n",
       "       [-1.73094066e-01,  1.32781114e-01],\n",
       "       [ 5.51485746e-01,  1.70518904e+00],\n",
       "       [-5.23307643e-02,  9.18985077e-01],\n",
       "       [ 1.51759216e+00,  1.18105307e+00],\n",
       "       [ 5.51485746e-01,  7.87951084e-01],\n",
       "       [ 7.93012351e-01,  1.31208706e+00],\n",
       "       [ 2.12140867e+00,  1.18105307e+00],\n",
       "       [-1.13920048e+00,  6.56917090e-01],\n",
       "       [ 1.75911877e+00,  7.87951084e-01],\n",
       "       [ 1.03453895e+00,  7.87951084e-01],\n",
       "       [ 1.63835547e+00,  1.70518904e+00],\n",
       "       [ 7.93012351e-01,  1.05001907e+00],\n",
       "       [ 6.72249049e-01,  9.18985077e-01],\n",
       "       [ 1.15530226e+00,  1.18105307e+00],\n",
       "       [-1.73094066e-01,  1.05001907e+00],\n",
       "       [-5.23307643e-02,  1.57415505e+00],\n",
       "       [ 6.72249049e-01,  1.44312105e+00],\n",
       "       [ 7.93012351e-01,  7.87951084e-01],\n",
       "       [ 2.24217198e+00,  1.31208706e+00],\n",
       "       [ 2.24217198e+00,  1.44312105e+00],\n",
       "       [ 1.89195840e-01,  3.94849102e-01],\n",
       "       [ 1.27606556e+00,  1.44312105e+00],\n",
       "       [-2.93857369e-01,  1.05001907e+00],\n",
       "       [ 2.24217198e+00,  1.05001907e+00],\n",
       "       [ 5.51485746e-01,  7.87951084e-01],\n",
       "       [ 1.03453895e+00,  1.18105307e+00],\n",
       "       [ 1.63835547e+00,  7.87951084e-01],\n",
       "       [ 4.30722444e-01,  7.87951084e-01],\n",
       "       [ 3.09959142e-01,  7.87951084e-01],\n",
       "       [ 6.72249049e-01,  1.18105307e+00],\n",
       "       [ 1.63835547e+00,  5.25883096e-01],\n",
       "       [ 1.87988207e+00,  9.18985077e-01],\n",
       "       [ 2.48369858e+00,  1.05001907e+00],\n",
       "       [ 6.72249049e-01,  1.31208706e+00],\n",
       "       [ 5.51485746e-01,  3.94849102e-01],\n",
       "       [ 3.09959142e-01,  2.63815108e-01],\n",
       "       [ 2.24217198e+00,  1.44312105e+00],\n",
       "       [ 5.51485746e-01,  1.57415505e+00],\n",
       "       [ 6.72249049e-01,  7.87951084e-01],\n",
       "       [ 1.89195840e-01,  7.87951084e-01],\n",
       "       [ 1.27606556e+00,  1.18105307e+00],\n",
       "       [ 1.03453895e+00,  1.57415505e+00],\n",
       "       [ 1.27606556e+00,  1.44312105e+00],\n",
       "       [-5.23307643e-02,  9.18985077e-01],\n",
       "       [ 1.15530226e+00,  1.44312105e+00],\n",
       "       [ 1.03453895e+00,  1.70518904e+00],\n",
       "       [ 1.03453895e+00,  1.44312105e+00],\n",
       "       [ 5.51485746e-01,  9.18985077e-01],\n",
       "       [ 7.93012351e-01,  1.05001907e+00],\n",
       "       [ 4.30722444e-01,  1.44312105e+00],\n",
       "       [ 6.84325379e-02,  7.87951084e-01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, [0, 3]] # sepal length and petal width\n",
    "X = (X - X.mean())/X.std()\n",
    "X.head()\n",
    "\n",
    "y0 = df['class']\n",
    "y = y0\n",
    "for i in range(len(y0)):\n",
    "#     print(i)\n",
    "    if y0[i] == 'Iris-setosa':\n",
    "        y[i] = 0\n",
    "    elif y0[i] == 'Iris-versicolor':\n",
    "        y[i] = 1\n",
    "    elif y0[i] == 'Iris-virginica':\n",
    "        y[i] = 2\n",
    "y.to_numpy()\n",
    "X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# this_dir, this_filename = os.path.split(\"I:\\Back Up\\Deep\\Principles of Machine Learning\\Code\\iris.csv.gz\")\n",
    "\n",
    "# # this_dir, this_filename = os.path.split(\"http://localhost:8888/edit/Deep/Principles%20of%20Machine%20Learning/Deep%20Learning/iris.csv.gz\")\n",
    "# DATA_PATH = os.path.join(this_dir, \"iris.csv.gz\")\n",
    "# # DATA_PATH = df = pd.read_csv('iris.dat', names=[ \"sepal length in cm\",  \"sepal width in cm\", \"petal length in cm\", \"petal width in cm\",\n",
    "# #   \"class\"])\n",
    "# # DATA_PATH = DATA_PATH.to_numpy()\n",
    "# # DATA_PATH = \"I:\\Back Up\\Deep\\Principles of Machine Learning\\Code\\iris.csv.gz\"\n",
    "\n",
    "\n",
    "# def iris_data(version='uci'):\n",
    "#     \"\"\"Iris flower dataset.\n",
    "#     Source : https://archive.ics.uci.edu/ml/datasets/Iris\n",
    "#     Number of samples : 150\n",
    "#     Class labels : {0, 1, 2}, distribution: [50, 50, 50]\n",
    "#         0 = setosa, 1 = versicolor, 2 = virginica.\n",
    "#     Dataset Attributes:\n",
    "#         - 1) sepal length [cm]\n",
    "#         - 2) sepal width [cm]\n",
    "#         - 3) petal length [cm]\n",
    "#         - 4) petal width [cm]\n",
    "#     Parameters\n",
    "#     --------\n",
    "#     version : string, optional (default: 'uci').\n",
    "#       Version to use {'uci', 'corrected'}. 'uci' loads the dataset\n",
    "#       as deposited on the UCI machine learning repository, and \n",
    "#       'corrected' provides the version that is consistent with\n",
    "#       Fisher's original paper. See Note for details.\n",
    "#     Returns\n",
    "#     --------\n",
    "#     X, y : [n_samples, n_features], [n_class_labels]\n",
    "#         X is the feature matrix with 150 flower samples as rows,\n",
    "#         and 4 feature columns sepal length, sepal width,\n",
    "#         petal length, and petal width.\n",
    "#         y is a 1-dimensional array of the class labels {0, 1, 2}\n",
    "#     Note\n",
    "#     --------\n",
    "#     The Iris dataset (originally collected by Edgar Anderson) and\n",
    "#     available in UCI's machine learning repository is different from\n",
    "#     the Iris dataset described in the original paper by  R.A. Fisher [1]). \n",
    "#     Precisely, there are two data points (row number\n",
    "#     34 and 37) in UCI's Machine Learning repository are different from the\n",
    "#     origianlly published Iris dataset. Also, the original version of the Iris\n",
    "#     Dataset, which can be loaded via `version='corrected'` is the same\n",
    "#     as the one in R.\n",
    "#     [1] . A. Fisher (1936). \"The use of multiple measurements in taxonomic\n",
    "#     problems\". Annals of Eugenics. 7 (2): 179188\n",
    "#     Examples\n",
    "#     -----------\n",
    "#     For usage examples, please see\n",
    "#     http://rasbt.github.io/mlxtend/user_guide/data/iris_data/\n",
    "#     \"\"\"\n",
    "#     if version == \"uci\":\n",
    "#         tmp = np.genfromtxt(fname=DATA_PATH, delimiter=',')\n",
    "#         X, y = tmp[:, :-1], tmp[:, -1]\n",
    "#         y = y.astype(int)\n",
    "#     elif version == \"corrected\":\n",
    "#         tmp = np.genfromtxt(fname=DATA_PATH, delimiter=',')\n",
    "#         X, y = tmp[:, :-1], tmp[:, -1]\n",
    "#         X[34] = [4.9, 3.1, 1.5, 0.2]\n",
    "#         X[37] = [4.9, 3.6, 1.4, 0.1]\n",
    "#         y = y.astype(int)\n",
    "#     else:\n",
    "#         raise ValueError(\"version must be 'uci' or 'corrected'.\")\n",
    "#     return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_37972/1270259641.py:118: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_enc = self._one_hot(y=y, n_labels=self.n_classes, dtype=np.float)\n",
      "Iteration: 500/500 | Cost 0.06 | Elapsed: 0:00:00 | ETA: 0:00:00"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SoftmaxRegression at 0x1a5adb1b310>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = SoftmaxRegression(eta=0.01, \n",
    "                       epochs=500, \n",
    "                       minibatches=1, \n",
    "                       random_seed=1,\n",
    "                       print_progress=3)\n",
    "lr.fit(X.to_numpy(),y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastian Raschka 2014-2020\n",
    "# mlxtend Machine Learning Library Extensions\n",
    "#\n",
    "# A counter class for printing the progress of an iterator.\n",
    "# Author: Sebastian Raschka <sebastianraschka.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def check_Xy(X, y, y_int=True):\n",
    "\n",
    "    # check types\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        raise ValueError('X must be a NumPy array. Found %s' % type(X))\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        raise ValueError('y must be a NumPy array. Found %s' % type(y))\n",
    "\n",
    "    if 'int' not in str(y.dtype):\n",
    "        raise ValueError('y must be an integer array. Found %s. '\n",
    "                         'Try passing the array as y.astype(np.integer)'\n",
    "                         % y.dtype)\n",
    "\n",
    "    if not ('float' in str(X.dtype) or 'int' in str(X.dtype)):\n",
    "        raise ValueError('X must be an integer or float array. Found %s.'\n",
    "                         % X.dtype)\n",
    "\n",
    "    # check dim\n",
    "    if len(X.shape) != 2:\n",
    "        raise ValueError('X must be a 2D array. Found %s' % str(X.shape))\n",
    "    if len(y.shape) > 1:\n",
    "        raise ValueError('y must be a 1D array. Found %s' % str(y.shape))\n",
    "\n",
    "    # check other\n",
    "    if y.shape[0] != X.shape[0]:\n",
    "        raise ValueError('y and X must contain the same number of samples. '\n",
    "                         'Got y: %d, X: %d' % (y.shape[0], X.shape[0]))\n",
    "\n",
    "\n",
    "def format_kwarg_dictionaries(default_kwargs=None, user_kwargs=None,\n",
    "                              protected_keys=None):\n",
    "    \"\"\"Function to combine default and user specified kwargs dictionaries\n",
    "    Parameters\n",
    "    ----------\n",
    "    default_kwargs : dict, optional\n",
    "        Default kwargs (default is None).\n",
    "    user_kwargs : dict, optional\n",
    "        User specified kwargs (default is None).\n",
    "    protected_keys : array_like, optional\n",
    "        Sequence of keys to be removed from the returned dictionary\n",
    "        (default is None).\n",
    "    Returns\n",
    "    -------\n",
    "    formatted_kwargs : dict\n",
    "        Formatted kwargs dictionary.\n",
    "    \"\"\"\n",
    "    formatted_kwargs = {}\n",
    "    for d in [default_kwargs, user_kwargs]:\n",
    "        if not isinstance(d, (dict, type(None))):\n",
    "            raise TypeError('d must be of type dict or None, but '\n",
    "                            'got {} instead'.format(type(d)))\n",
    "        if d is not None:\n",
    "            formatted_kwargs.update(d)\n",
    "    if protected_keys is not None:\n",
    "        for key in protected_keys:\n",
    "            formatted_kwargs.pop(key, None)\n",
    "\n",
    "    return formatted_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, clf,\n",
    "                          feature_index=None,\n",
    "                          filler_feature_values=None,\n",
    "                          filler_feature_ranges=None,\n",
    "                          ax=None,\n",
    "                          X_highlight=None,\n",
    "                          res=None,\n",
    "                          zoom_factor=1.,\n",
    "                          legend=1,\n",
    "                          hide_spines=True,\n",
    "                          markers='s^oxv<>',\n",
    "                          colors=('#1f77b4,#ff7f0e,#3ca02c,#d62728,'\n",
    "                                  '#9467bd,#8c564b,#e377c2,'\n",
    "                                  '#7f7f7f,#bcbd22,#17becf'),\n",
    "                          scatter_kwargs=None,\n",
    "                          contourf_kwargs=None,\n",
    "                          scatter_highlight_kwargs=None):\n",
    "    \"\"\"Plot decision regions of a classifier.\n",
    "    Please note that this functions assumes that class labels are\n",
    "    labeled consecutively, e.g,. 0, 1, 2, 3, 4, and 5. If you have class\n",
    "    labels with integer labels > 4, you may want to provide additional colors\n",
    "    and/or markers as `colors` and `markers` arguments.\n",
    "    See http://matplotlib.org/examples/color/named_colors.html for more\n",
    "    information.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape = [n_samples, n_features]\n",
    "        Feature Matrix.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        True class labels.\n",
    "    clf : Classifier object.\n",
    "        Must have a .predict method.\n",
    "    feature_index : array-like (default: (0,) for 1D, (0, 1) otherwise)\n",
    "        Feature indices to use for plotting. The first index in\n",
    "        `feature_index` will be on the x-axis, the second index will be\n",
    "        on the y-axis.\n",
    "    filler_feature_values : dict (default: None)\n",
    "        Only needed for number features > 2. Dictionary of feature\n",
    "        index-value pairs for the features not being plotted.\n",
    "    filler_feature_ranges : dict (default: None)\n",
    "        Only needed for number features > 2. Dictionary of feature\n",
    "        index-value pairs for the features not being plotted. Will use the\n",
    "        ranges provided to select training samples for plotting.\n",
    "    ax : matplotlib.axes.Axes (default: None)\n",
    "        An existing matplotlib Axes. Creates\n",
    "        one if ax=None.\n",
    "    X_highlight : array-like, shape = [n_samples, n_features] (default: None)\n",
    "        An array with data points that are used to highlight samples in `X`.\n",
    "    res : float or array-like, shape = (2,) (default: None)\n",
    "        This parameter was used to define the grid width,\n",
    "        but it has been deprecated in favor of\n",
    "        determining the number of points given the figure DPI and size\n",
    "        automatically for optimal results and computational efficiency.\n",
    "        To increase the resolution, it's is recommended to use to provide\n",
    "        a `dpi argument via matplotlib, e.g., `plt.figure(dpi=600)`.\n",
    "    zoom_factor : float (default: 1.0)\n",
    "        Controls the scale of the x- and y-axis of the decision plot.\n",
    "    hide_spines : bool (default: True)\n",
    "        Hide axis spines if True.\n",
    "    legend : int (default: 1)\n",
    "        Integer to specify the legend location.\n",
    "        No legend if legend is 0.\n",
    "    markers : str (default: 's^oxv<>')\n",
    "        Scatterplot markers.\n",
    "    colors : str (default: 'red,blue,limegreen,gray,cyan')\n",
    "        Comma separated list of colors.\n",
    "    scatter_kwargs : dict (default: None)\n",
    "        Keyword arguments for underlying matplotlib scatter function.\n",
    "    contourf_kwargs : dict (default: None)\n",
    "        Keyword arguments for underlying matplotlib contourf function.\n",
    "    scatter_highlight_kwargs : dict (default: None)\n",
    "        Keyword arguments for underlying matplotlib scatter function.\n",
    "    Returns\n",
    "    ---------\n",
    "    ax : matplotlib.axes.Axes object\n",
    "    Examples\n",
    "    -----------\n",
    "    For usage examples, please see\n",
    "    http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/\n",
    "    \"\"\"\n",
    "\n",
    "    check_Xy(X, y, y_int=True)  # Validate X and y arrays\n",
    "    dim = X.shape[1]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if res is not None:\n",
    "        warnings.warn(\"The 'res' parameter has been deprecated.\"\n",
    "                      \"To increase the resolution, it's is recommended\"\n",
    "                      \"to use to provide a `dpi argument via matplotlib,\"\n",
    "                      \"e.g., `plt.figure(dpi=600)`.\",\n",
    "                      DeprecationWarning)\n",
    "\n",
    "    plot_testdata = True\n",
    "    if not isinstance(X_highlight, np.ndarray):\n",
    "        if X_highlight is not None:\n",
    "            raise ValueError('X_highlight must be a NumPy array or None')\n",
    "        else:\n",
    "            plot_testdata = False\n",
    "    elif len(X_highlight.shape) < 2:\n",
    "        raise ValueError('X_highlight must be a 2D array')\n",
    "\n",
    "    if feature_index is not None:\n",
    "        # Unpack and validate the feature_index values\n",
    "        if dim == 1:\n",
    "            raise ValueError(\n",
    "                'feature_index requires more than one training feature')\n",
    "        try:\n",
    "            x_index, y_index = feature_index\n",
    "        except ValueError:\n",
    "            raise ValueError(\n",
    "                'Unable to unpack feature_index. Make sure feature_index '\n",
    "                'only has two dimensions.')\n",
    "        try:\n",
    "            X[:, x_index], X[:, y_index]\n",
    "        except IndexError:\n",
    "            raise IndexError(\n",
    "                'feature_index values out of range. X.shape is {}, but '\n",
    "                'feature_index is {}'.format(X.shape, feature_index))\n",
    "    else:\n",
    "        feature_index = (0, 1)\n",
    "        x_index, y_index = feature_index\n",
    "\n",
    "    # Extra input validation for higher number of training features\n",
    "    if dim > 2:\n",
    "        if filler_feature_values is None:\n",
    "            raise ValueError('Filler values must be provided when '\n",
    "                             'X has more than 2 training features.')\n",
    "\n",
    "        if filler_feature_ranges is not None:\n",
    "            if not set(filler_feature_values) == set(filler_feature_ranges):\n",
    "                raise ValueError(\n",
    "                    'filler_feature_values and filler_feature_ranges must '\n",
    "                    'have the same keys')\n",
    "\n",
    "        # Check that all columns in X are accounted for\n",
    "        column_check = np.zeros(dim, dtype=bool)\n",
    "        for idx in filler_feature_values:\n",
    "            column_check[idx] = True\n",
    "        for idx in feature_index:\n",
    "            column_check[idx] = True\n",
    "        if not all(column_check):\n",
    "            missing_cols = np.argwhere(~column_check).flatten()\n",
    "            raise ValueError(\n",
    "                'Column(s) {} need to be accounted for in either '\n",
    "                'feature_index or filler_feature_values'.format(missing_cols))\n",
    "\n",
    "    marker_gen = cycle(list(markers))\n",
    "\n",
    "    n_classes = np.unique(y).shape[0]\n",
    "    colors = colors.split(',')\n",
    "    colors_gen = cycle(colors)\n",
    "    colors = [next(colors_gen) for c in range(n_classes)]\n",
    "\n",
    "    # Get minimum and maximum\n",
    "    x_min, x_max = (X[:, x_index].min() - 1./zoom_factor,\n",
    "                    X[:, x_index].max() + 1./zoom_factor)\n",
    "    if dim == 1:\n",
    "        y_min, y_max = -1, 1\n",
    "    else:\n",
    "        y_min, y_max = (X[:, y_index].min() - 1./zoom_factor,\n",
    "                        X[:, y_index].max() + 1./zoom_factor)\n",
    "\n",
    "    xnum, ynum = plt.gcf().dpi * plt.gcf().get_size_inches()\n",
    "    xnum, ynum = floor(xnum), ceil(ynum)\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, num=int(xnum)),\n",
    "                         np.linspace(y_min, y_max, num=int(ynum)))\n",
    "\n",
    "    if dim == 1:\n",
    "        X_predict = np.array([xx.ravel()]).T\n",
    "    else:\n",
    "        X_grid = np.array([xx.ravel(), yy.ravel()]).T\n",
    "        X_predict = np.zeros((X_grid.shape[0], dim))\n",
    "        X_predict[:, x_index] = X_grid[:, 0]\n",
    "        X_predict[:, y_index] = X_grid[:, 1]\n",
    "        if dim > 2:\n",
    "            for feature_idx in filler_feature_values:\n",
    "                X_predict[:, feature_idx] = filler_feature_values[feature_idx]\n",
    "    Z = clf.predict(X_predict.astype(X.dtype))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot decisoin region\n",
    "    # Make sure contourf_kwargs has backwards compatible defaults\n",
    "    contourf_kwargs_default = {'alpha': 0.45, 'antialiased': True}\n",
    "    contourf_kwargs = format_kwarg_dictionaries(\n",
    "                        default_kwargs=contourf_kwargs_default,\n",
    "                        user_kwargs=contourf_kwargs,\n",
    "                        protected_keys=['colors', 'levels'])\n",
    "    cset = ax.contourf(xx, yy, Z,\n",
    "                       colors=colors,\n",
    "                       levels=np.arange(Z.max() + 2) - 0.5,\n",
    "                       **contourf_kwargs)\n",
    "\n",
    "    ax.contour(xx, yy, Z, cset.levels,\n",
    "               colors='k',\n",
    "               linewidths=0.5,\n",
    "               antialiased=True)\n",
    "\n",
    "    ax.axis([xx.min(), xx.max(), yy.min(), yy.max()])\n",
    "\n",
    "    # Scatter training data samples\n",
    "    # Make sure scatter_kwargs has backwards compatible defaults\n",
    "    scatter_kwargs_default = {'alpha': 0.8, 'edgecolor': 'black'}\n",
    "    scatter_kwargs = format_kwarg_dictionaries(\n",
    "                        default_kwargs=scatter_kwargs_default,\n",
    "                        user_kwargs=scatter_kwargs,\n",
    "                        protected_keys=['c', 'marker', 'label'])\n",
    "    for idx, c in enumerate(np.unique(y)):\n",
    "        if dim == 1:\n",
    "            y_data = [0 for i in X[y == c]]\n",
    "            x_data = X[y == c]\n",
    "        elif dim == 2:\n",
    "            y_data = X[y == c, y_index]\n",
    "            x_data = X[y == c, x_index]\n",
    "        elif dim > 2 and filler_feature_ranges is not None:\n",
    "            class_mask = y == c\n",
    "            feature_range_mask = get_feature_range_mask(\n",
    "                            X, filler_feature_values=filler_feature_values,\n",
    "                            filler_feature_ranges=filler_feature_ranges)\n",
    "            y_data = X[class_mask & feature_range_mask, y_index]\n",
    "            x_data = X[class_mask & feature_range_mask, x_index]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        ax.scatter(x=x_data,\n",
    "                   y=y_data,\n",
    "                   c=colors[idx],\n",
    "                   marker=next(marker_gen),\n",
    "                   label=c,\n",
    "                   **scatter_kwargs)\n",
    "\n",
    "    if hide_spines:\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    if dim == 1:\n",
    "        ax.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "    if plot_testdata:\n",
    "        if dim == 1:\n",
    "            x_data = X_highlight\n",
    "            y_data = [0 for i in X_highlight]\n",
    "        elif dim == 2:\n",
    "            x_data = X_highlight[:, x_index]\n",
    "            y_data = X_highlight[:, y_index]\n",
    "        else:\n",
    "            feature_range_mask = get_feature_range_mask(\n",
    "                    X_highlight, filler_feature_values=filler_feature_values,\n",
    "                    filler_feature_ranges=filler_feature_ranges)\n",
    "            y_data = X_highlight[feature_range_mask, y_index]\n",
    "            x_data = X_highlight[feature_range_mask, x_index]\n",
    "\n",
    "        # Make sure scatter_highlight_kwargs backwards compatible defaults\n",
    "        scatter_highlight_defaults = {'c': '',\n",
    "                                      'edgecolor': 'black',\n",
    "                                      'alpha': 1.0,\n",
    "                                      'linewidths': 1,\n",
    "                                      'marker': 'o',\n",
    "                                      's': 80}\n",
    "        scatter_highlight_kwargs = format_kwarg_dictionaries(\n",
    "                                    default_kwargs=scatter_highlight_defaults,\n",
    "                                    user_kwargs=scatter_highlight_kwargs)\n",
    "        ax.scatter(x_data,\n",
    "                   y_data,\n",
    "                   **scatter_highlight_kwargs)\n",
    "\n",
    "    if legend:\n",
    "        if dim > 2 and filler_feature_ranges is None:\n",
    "            pass\n",
    "        else:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            ax.legend(handles, labels,\n",
    "                      framealpha=0.3, scatterpoints=1, loc=legend)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.to_numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_decision_regions(X.to_numpy(),y.to_numpy().astype(int),clf=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ys = y.to_numpy().astype(int)\n",
    "x_new = X.to_numpy()\n",
    "x_new_lst = x_new.tolist()\n",
    "plt.scatter(x_new[:,0],x_new[:,1])\n",
    "# x_new_lst\n",
    "x_new.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = []\n",
    "for i in ys:\n",
    "    if i not in label:\n",
    "        label.append(i)\n",
    "        \n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decision_bounds(x_new,ys,label):\n",
    "    \n",
    "#     x_db_lst = []\n",
    "#     y_db_lst = []\n",
    "    \n",
    "#     x_new_lst = x_new.tolist()\n",
    "    \n",
    "#     rescaled_xs = rescale(x_new)\n",
    "#     beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "#     predictions = [predict(x_i, beta) for x_i in rescaled_xs]\n",
    "    \n",
    "#     random.seed(0)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "\n",
    "#     print(y_train)\n",
    "\n",
    "#     learning_rate = 0.01\n",
    "\n",
    "#     # pick a random starting point\n",
    "#     beta = [random.random() for _ in range(3)]\n",
    "#     beta\n",
    "    \n",
    "#     xs_1 = []\n",
    "#     # ys = df['class'].to_numpy().tolist()\n",
    "#     # ys = []\n",
    "#     for i in range(len(label)-1):\n",
    "#     #     xs_0 = [[1] +row[:2] for row in data_org_lst if row[2] >= 0+i  and row[2] < len(cls) - 1+i]\n",
    "#         xs_0 = [[1]+x_new_lst[n] for n in range(len(x_new_lst)) if ys[n] >= 0+i and ys[n] < 2+i]\n",
    "#     #     ys_0 = [row[2] for row in data_org_lst if row[2] > 0+i and row[2] < 2+i]\n",
    "#         xs_1.append(xs_0)\n",
    "#     #     ys.append(ys_0)\n",
    "    \n",
    "#     beta_unscaled = []\n",
    "#     for k in range(len(label)-1):\n",
    "#         means, stdevs = scale(xs_1[k])\n",
    "#         beta_1= [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                      beta[1] / stdevs[1],\n",
    "#                      beta[2] / stdevs[2]]\n",
    "#         beta_unscaled.append(beta_1)\n",
    "        \n",
    "#     min_val = -2\n",
    "#     max_val = 2.5\n",
    "\n",
    "#     # x_db = [xi for xi in range(min_val,max_val)]\n",
    "#     for k in range(len(label)-1):\n",
    "#         x_db = np.linspace(min_val,max_val,len(ys))\n",
    "#         x_db_lst.append(x_db)\n",
    "#         y_db = [(-beta_unscaled[k][1]/beta_unscaled[k][2]*xi - beta_unscaled[k][0]/beta_unscaled[k][2])\n",
    "#                 for xi in x_db]\n",
    "#         y_db_lst.append(y_db)\n",
    "    \n",
    "#     return x_db_lst,y_db_lst\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_db_lst2,y_db_lst2 = decision_bounds(x_new,ys,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_db_lst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs_1 = []\n",
    "# # ys = df['class'].to_numpy().tolist()\n",
    "# # ys = []\n",
    "# for i in range(len(label)-1):\n",
    "# #     xs_0 = [[1] +row[:2] for row in data_org_lst if row[2] >= 0+i  and row[2] < len(cls) - 1+i]\n",
    "#     xs_0 = [[1]+x_new_lst[n] for n in range(len(x_new_lst)) if ys[n] >= 0+i and ys[n] < 2+i]\n",
    "# #     ys_0 = [row[2] for row in data_org_lst if row[2] > 0+i and row[2] < 2+i]\n",
    "#     xs_1.append(xs_0)\n",
    "# #     ys.append(ys_0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaled_xs = rescale(xs_1[1])\n",
    "# beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "# predictions = [predict(x_i, beta) for x_i in rescaled_xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaled_xs = rescale(xs_1[1])\n",
    "# beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "# predictions = [predict(x_i, beta) for x_i in rescaled_xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(0)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "\n",
    "# print(y_train)\n",
    "\n",
    "# learning_rate = 0.01\n",
    "\n",
    "# # pick a random starting point\n",
    "# beta = [random.random() for _ in range(3)]\n",
    "# beta\n",
    "# # with tqdm.trange(5000) as t:\n",
    "# #     for epoch in t:\n",
    "# #         gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "# #         beta = gradient_step(beta, gradient, -learning_rate)\n",
    "# #         loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "# #         t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "# # print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs_0 = [[1]+x_new_lst[n] for n in range(len(x_new_lst)) if ys[n] >= 0 and ys[n] < 2]\n",
    "# xs_1 = [[1]+x_new_lst[n] for n in range(len(x_new_lst)) if ys[n] >= 1 and ys[n] < 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs_1 = []\n",
    "# # ys = df['class'].to_numpy().tolist()\n",
    "# # ys = []\n",
    "# for i in range(len(label)-1):\n",
    "# #     xs_0 = [[1] +row[:2] for row in data_org_lst if row[2] >= 0+i  and row[2] < len(cls) - 1+i]\n",
    "#     xs_0 = [[1]+x_new_lst[n] for n in range(len(x_new_lst)) if ys[n] >= 0+i and ys[n] < 2+i]\n",
    "# #     ys_0 = [row[2] for row in data_org_lst if row[2] > 0+i and row[2] < 2+i]\n",
    "#     xs_1.append(xs_0)\n",
    "# #     ys.append(ys_0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break up by labels\n",
    "# for j in range(len(xs_0)):\n",
    "# Label 1\n",
    "x_0 = [x_new_lst[m] for m in range(len(x_new_lst)) if ys[m] == 0]\n",
    "x_0 = np.array(x_0)\n",
    "# Label 2\n",
    "x_1 = [x_new_lst[m] for m in range(len(x_new_lst)) if ys[m] == 1]\n",
    "x_1 = np.array(x_1)\n",
    "# Label 3\n",
    "x_2 = [x_new_lst[m] for m in range(len(x_new_lst)) if ys[m] == 2]\n",
    "x_2 = np.array(x_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.scatter(x_new[:,0],x_new[:,1], marker = '*', color = 'red')\n",
    "# plt.scatter(x_0[:,0],x_0[:,1], marker = '*', color = 'red')\n",
    "# plt.scatter(x_1[:,0],x_1[:,1], marker = '+', color = 'green')\n",
    "# plt.scatter(x_2[:,0],x_2[:,1], marker = 'o', color = 'blue')\n",
    "# # plt.plot(x_db,y_db)\n",
    "# # plt.plot(x_db_1,y_db_1)\n",
    "# # plt.plot(x_db_lst2[0],y_db_lst2[0])\n",
    "# # plt.plot(x_db_lst2[1],y_db_lst2[1])\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_bounds(x_new,ys,label):\n",
    "    \n",
    "    x_db_lst = []\n",
    "    y_db_lst = []\n",
    "    \n",
    "    x_new_lst = x_new.tolist()\n",
    "    \n",
    "    xs_1 = []\n",
    "    # ys = df['class'].to_numpy().tolist()\n",
    "    # ys = []\n",
    "    for i in range(len(label)-1):\n",
    "    #     xs_0 = [[1] +row[:2] for row in data_org_lst if row[2] >= 0+i  and row[2] < len(cls) - 1+i]\n",
    "        xs_0 = [[1]+x_new_lst[n] for n in range(len(x_new_lst)) if ys[n] >= 0+i and ys[n] < 2+i]\n",
    "    #     ys_0 = [row[2] for row in data_org_lst if row[2] > 0+i and row[2] < 2+i]\n",
    "        xs_1.append(xs_0)\n",
    "    #     ys.append(ys_0)\n",
    "   \n",
    "   \n",
    "    rescaled_xs = rescale(xs_1[1])\n",
    "    beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "    predictions = [predict(x_i, beta) for x_i in rescaled_xs]\n",
    "    \n",
    "    random.seed(0)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "\n",
    "#     print(y_train)\n",
    "\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # pick a random starting point\n",
    "    beta = [random.random() for _ in range(3)]\n",
    "    beta\n",
    "    \n",
    "#     xs_1 = []\n",
    "#     # ys = df['class'].to_numpy().tolist()\n",
    "#     # ys = []\n",
    "#     for i in range(len(label)-1):\n",
    "#     #     xs_0 = [[1] +row[:2] for row in data_org_lst if row[2] >= 0+i  and row[2] < len(cls) - 1+i]\n",
    "#         xs_0 = [[1]+x_new_lst[n] for n in range(len(x_new_lst)) if ys[n] >= 0+i and ys[n] < 2+i]\n",
    "#     #     ys_0 = [row[2] for row in data_org_lst if row[2] > 0+i and row[2] < 2+i]\n",
    "#         xs_1.append(xs_0)\n",
    "#     #     ys.append(ys_0)\n",
    "\n",
    "    random.seed(0)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "    x_train\n",
    "\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # pick a random starting point\n",
    "    beta = [random.random() for _ in range(3)]\n",
    "    y_train\n",
    "\n",
    "    with tqdm.trange(2500) as t:\n",
    "        for epoch in t:\n",
    "            gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "            beta = gradient_step(beta, gradient, -learning_rate)\n",
    "            loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "            t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "    print(t)\n",
    "\n",
    "    beta_unscaled = []\n",
    "    for k in range(len(label)-1):\n",
    "        means, stdevs = scale(xs_1[k])\n",
    "        beta_1= [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "                     beta[1] / stdevs[1],\n",
    "                     beta[2] / stdevs[2]]\n",
    "        beta_unscaled.append(beta_1)\n",
    "        \n",
    "    min_val = -2\n",
    "    max_val = 2.5\n",
    "\n",
    "    # x_db = [xi for xi in range(min_val,max_val)]\n",
    "    for k in range(len(label)-1):\n",
    "        x_db = np.linspace(min_val,max_val,len(ys))\n",
    "        x_db_lst.append(x_db)\n",
    "        y_db = [(-beta_unscaled[k][1]/beta_unscaled[k][2]*xi - beta_unscaled[k][0]/beta_unscaled[k][2])\n",
    "                for xi in x_db]\n",
    "        y_db_lst.append(y_db)\n",
    "    \n",
    "    return x_db_lst,y_db_lst\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_37972/2746858823.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
      "least squares fit: 100%|| 1000/1000 [00:03<00:00, 302.59it/s]\n",
      "  0%|                                                                                                                                             | 0/2500 [00:00<?, ?it/s]C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_37972/2746858823.py:15: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return [sum(vector[i] for vector in vectors)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_37972/2746858823.py:164: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(_negative_log_likelihood(x, y, beta)\n",
      "loss: 4.276 beta: [ 0.31263739  0.2225783  10.58747186]: 100%|| 2500/2500 [00:11<00:00, 211.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.276 beta: [ 0.31263739  0.2225783  10.58747186]: 100%|| 2500/2500 [00:11<00:00, 211.07it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_db_lst3,y_db_lst3 = decision_bounds(x_new,ys,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaled_xs = rescale(xs_1[1])\n",
    "# beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "# predictions = [predict(x_i, beta) for x_i in rescaled_xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(0)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "# x_train\n",
    "\n",
    "# learning_rate = 0.01\n",
    "\n",
    "# # pick a random starting point\n",
    "# beta = [random.random() for _ in range(3)]\n",
    "# y_train\n",
    "\n",
    "# with tqdm.trange(2500) as t:\n",
    "#     for epoch in t:\n",
    "#         gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "#         beta = gradient_step(beta, gradient, -learning_rate)\n",
    "#         loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "#         t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
    "\n",
    "# print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means, stdevs = scale(xs_1[0])\n",
    "# beta_unscaled_2 = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                  beta[1] / stdevs[1],\n",
    "#                  beta[2] / stdevs[2]]\n",
    "# beta_unscaled_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means, stdevs = scale(xs_1[1])\n",
    "# beta_unscaled_1= [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                  beta[1] / stdevs[1],\n",
    "#                  beta[2] / stdevs[2]]\n",
    "# beta_unscaled_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta1 = []\n",
    "# for k in range(len(label)-1):\n",
    "#     means, stdevs = scale(xs_1[k])\n",
    "#     beta_unscaled_1= [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "#                  beta[1] / stdevs[1],\n",
    "#                  beta[2] / stdevs[2]]\n",
    "#     beta1.append(beta_unscaled_1)\n",
    "# beta1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # min_range = (data_marks.min())\n",
    "# # min_range = math.floor(min_range[0])\n",
    "# # max_range = (data_marks.max())\n",
    "# # max_range = math.ceil(max_range[0])\n",
    "\n",
    "# # min_val = math.floor(np.min(exp_val))\n",
    "# # max_val = math.ceil(np.max(exp_val))\n",
    "\n",
    "# min_val = -2\n",
    "# max_val = 2.5\n",
    "\n",
    "# # min_val = math.floor(np.min(x_new_lst))\n",
    "# # max_val = math.ceil(np.max(x_new_lst))\n",
    "\n",
    "# # x_db = [xi for xi in range(min_val,max_val)]\n",
    "# x_db = np.linspace(min_val,max_val,len(ys))\n",
    "# # y_db = [(-beta_unscaled[1]/beta_unscaled[2]*xi - beta_unscaled[0]/beta_unscaled[2])\n",
    "# #         for xi in x_db]\n",
    "\n",
    "# # y_db = [(-beta1[0][1]/beta1[0][2]*xi - beta1[0][0]/beta1[0][2])\n",
    "# #         for xi in x_db]\n",
    "\n",
    "# y_db = [(-beta_unscaled_2[1]/beta_unscaled_2[2]*xi - beta_unscaled_2[0]/beta_unscaled_2[2])\n",
    "#         for xi in x_db]\n",
    "# # \n",
    "# print(min_val)\n",
    "# print(max_val)\n",
    "\n",
    "# xx = []\n",
    "# yy = []\n",
    "# for k in range(len(y_db)):\n",
    "#     xx.append(x_new_lst[k][0])\n",
    "#     yy.append(x_new_lst[k][1])\n",
    "# # x_new_lst\n",
    "# max(xx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_val = -2\n",
    "# max_val = 2.5\n",
    "\n",
    "# # min_val = math.floor(np.min(x_new_lst))\n",
    "# # max_val = math.ceil(np.max(x_new_lst))\n",
    "\n",
    "# # x_db = [xi for xi in range(min_val,max_val)]\n",
    "# x_db_1 = np.linspace(min_val,max_val,len(ys))\n",
    "# y_db_1 = [(-beta_unscaled_1[1]/beta_unscaled_1[2]*xi - beta_unscaled_1[0]/beta_unscaled_1[2])\n",
    "#         for xi in x_db_1]\n",
    "# print(min_val)\n",
    "# print(max_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaled_xs = rescale(x_new)\n",
    "# beta = least_squares_fit(rescaled_xs, ys, 0.001, 1000, 1)\n",
    "# predictions = [predict(x_i, beta) for x_i in rescaled_xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_db_lst,y_db_lst = decision_boundary_2(x_new_lst,ys,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.scatter(x_new[:,0],x_new[:,1], marker = '*', color = 'red')\n",
    "plt.scatter(x_0[:,0],x_0[:,1], marker = '*', color = 'red')\n",
    "plt.scatter(x_1[:,0],x_1[:,1], marker = '+', color = 'green')\n",
    "plt.scatter(x_2[:,0],x_2[:,1], marker = 'o', color = 'blue')\n",
    "# plt.plot(x_db_1,y_db_1)\n",
    "# plt.plot(x_db,y_db)\n",
    "plt.plot(x_db_lst3[0],y_db_lst3[0])\n",
    "plt.plot(x_db_lst3[1],y_db_lst3[1])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
