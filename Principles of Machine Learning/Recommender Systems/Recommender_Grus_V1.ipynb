{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "# import pygal\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import urllib.request\n",
    "import requests\n",
    "import curl\n",
    "import pycurl\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from IPython import qt\n",
    "from matplotlib.pyplot import figure\n",
    "from py.xml import raw\n",
    "from requests.api import get\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# from scratch.working_with_data import rescale\n",
    "# from scratch.multiple_regression import least_squares_fit, predict\n",
    "# from scratch.gradient_descent import gradient_step\n",
    "# from stats import mean, median, de_mean, standard_deviation, correlation\n",
    "# from gradient_descent import minimize_stochastic, maximize_stochastic, maximize_batch\n",
    "# from vector import dot, vector_add\n",
    "# from normal import normal_cdf\n",
    "# from matrix import make_matrix, get_column, shape, matrix_multiply\n",
    "# from logistic_regression import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "from scipy.optimize import fmin_tnc\n",
    "import itertools\n",
    "import random\n",
    "import tqdm\n",
    "from typing import*\n",
    "from collections import*\n",
    "# from scipy import*\n",
    "from sklearn.metrics import*\n",
    "from numpy import *\n",
    "# import mnist\n",
    "# bltin_sum = np.sum\n",
    "import random\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# def add(a, b): return a + b\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "Tensor = list\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c , v):\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "#     return np.sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "#     gen = \n",
    "    return np.sum(np.fromiter((v_i * w_i for v_i, w_i in zip(v, w)),float))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = data.shape[0]\n",
    "    \n",
    "    # Vector Mean\n",
    "#     n = len(data)\n",
    "#     m = np.sum(data,axis=0)\n",
    "#     means = np.multiply(1/n,m)\n",
    "    means = vector_mean(data)\n",
    "    \n",
    "    # Standard Deviaiton\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "    return means,stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = data.shape[0]\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    grad_step = np.add(v,step)\n",
    "    return grad_step\n",
    "\n",
    "# def predict(alpha, beta, x_i):\n",
    "#     pred = beta * x_i + alpha\n",
    "#     return pred\n",
    "\n",
    "# def error(x, y, beta):\n",
    "#     \"\"\"\n",
    "#     The error from predicting beta * x_i + alpha\n",
    "#     when the actual value is y_i\n",
    "#     \"\"\"\n",
    "#     err_fin = predict(alpha, beta, x_i) - y_i\n",
    "#     return err_fin\n",
    "\n",
    "def predict(x, beta):\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x, y, beta):\n",
    "    return predict(x, beta) - y \n",
    "\n",
    "def sqerror_gradient(x, y, beta):\n",
    "    err = error(x, y, beta)\n",
    "    err_fin = [2 * err * x_i for x_i in x]\n",
    "    return err_fin\n",
    "\n",
    "def least_squares_fit(xs, ys, learning_rate = 0.001, num_steps = 1000, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [np.random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def _negative_log_likelihood(x, y, beta):\n",
    "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))\n",
    "    \n",
    "def negative_log_likelihood(xs, ys, beta):\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))\n",
    "\n",
    "def _negative_log_partial_j(x, y, beta, j):\n",
    "    \"\"\"\n",
    "    The jth partial derivative for one data point.\n",
    "    Here i is the index of the data point.\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x, y, beta):\n",
    "    \"\"\"\n",
    "    The gradient for one data point.\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs, ys,beta):\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "                                                                \n",
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def sigmoid(t: float) -> float: \n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "# Gradient Descent - step\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "def squared_distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\"\n",
    "    return sum_of_squares(subtract(v, w))\n",
    "\n",
    "def distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes the distance between v and w\"\"\"\n",
    "    return math.sqrt(squared_distance(v, w))\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "\n",
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor.\n",
    "    Otherwise, tensor is 1-dimensional (that is, a vector).\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)  # just a list of floats, use Python sum\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
    "                   for tensor_i in tensor)   # and sum up those results.\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f elementwise\"\"\" \n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor] \n",
    "    \n",
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "\n",
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                   t1: Tensor,\n",
    "                   t2: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]\n",
    "    \n",
    "def inverse_normal_cdf(p: float,\n",
    "                       mu: float = 0,\n",
    "                       sigma: float = 1,\n",
    "                       tolerance: float = 0.00001) -> float:\n",
    "    \"\"\"Find approximate inverse using binary search\"\"\"\n",
    "\n",
    "    # if not standard, compute standard and rescale\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "\n",
    "    low_z = -10.0                      # normal_cdf(-10) is (very close to) 0\n",
    "    hi_z  =  10.0                      # normal_cdf(10)  is (very close to) 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2     # Consider the midpoint\n",
    "        mid_p = normal_cdf(mid_z)      # and the CDF's value there\n",
    "        if mid_p < p:\n",
    "            low_z = mid_z              # Midpoint too low, search above it\n",
    "        else:\n",
    "            hi_z = mid_z               # Midpoint too high, search below it\n",
    "\n",
    "    return mid_z\n",
    "\n",
    "def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -> float:\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "\n",
    "def num_differences(v1, v2):\n",
    "    assert len(v1) == len(v2)\n",
    "    return len([x1 for x1, x2 in zip(v1, v2) if x1 != x2])\n",
    "\n",
    "\n",
    "def cluster_means(k: int,\n",
    "                  inputs: List[Vector],\n",
    "                  assignments: List[int]) -> List[Vector]:\n",
    "    # clusters[i] contains the inputs whose assignment is i\n",
    "    clusters = [[] for i in range(k)]\n",
    "    for input, assignment in zip(inputs, assignments):\n",
    "        clusters[assignment].append(input)\n",
    "\n",
    "    # if a cluster is empty, just use a random point\n",
    "    return [vector_mean(cluster) if cluster else random.choice(inputs)\n",
    "            for cluster in clusters]\n",
    "\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k: int) -> None:\n",
    "        self.k = k                      # number of clusters\n",
    "        self.means = None\n",
    "\n",
    "    def classify(self, input: Vector) -> int:\n",
    "        \"\"\"return the index of the cluster closest to the input\"\"\"\n",
    "        return min(range(self.k),\n",
    "                   key=lambda i: squared_distance(input, self.means[i]))\n",
    "\n",
    "    def train(self, inputs: List[Vector]) -> None:\n",
    "        # Start with random assignments\n",
    "        assignments = [random.randrange(self.k) for _ in inputs]\n",
    "\n",
    "        with tqdm.tqdm(itertools.count()) as t:\n",
    "            for _ in t:\n",
    "                # Compute means and find new assignments\n",
    "                self.means = cluster_means(self.k, inputs, assignments)\n",
    "                new_assignments = [self.classify(input) for input in inputs]\n",
    "\n",
    "                # Check how many assignments changed and if we're done\n",
    "                num_changed = num_differences(assignments, new_assignments)\n",
    "                if num_changed == 0:\n",
    "                    return\n",
    "\n",
    "                # Otherwise keep the new assignments, and compute new means\n",
    "                assignments = new_assignments\n",
    "                self.means = cluster_means(self.k, inputs, assignments)\n",
    "                t.set_description(f\"changed: {num_changed} / {len(inputs)}\")\n",
    "                \n",
    "def squared_clustering_errors(inputs, k):\n",
    "    \"\"\"finds the total squared error from k-means clustering the\n",
    "inputs\"\"\"\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(inputs)\n",
    "    means = clusterer.means\n",
    "    assignments = [clusterer.classify(input) for input in inputs]\n",
    "\n",
    "    return sum(squared_distance(input, means[cluster])\n",
    "               for input, cluster in zip(inputs, assignments))\n",
    "\n",
    "def recolor(pixel):\n",
    "    cluster = clusterer.classify(pixel)        # index of the closest cluster\n",
    "    return clusterer.means[cluster]            # mean of the closest cluster\n",
    "\n",
    "def get_values(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        return [cluster.value]\n",
    "    else:\n",
    "        return [value\n",
    "                for child in cluster.children\n",
    "                for value in get_values(child)]\n",
    "\n",
    "\n",
    "def cluster_distance(cluster1,\n",
    "                     cluster2,\n",
    "                     distance_agg: Callable = min):\n",
    "    \"\"\"\n",
    "    compute all the pairwise distances between cluster1 and cluster2\n",
    "    and apply the aggregation function _distance_agg_ to the resulting\n",
    "list\n",
    "    \"\"\"\n",
    "    return distance_agg([distance(v1, v2) for v1 in get_values(cluster1) for v2 in get_values(cluster2)])\n",
    "\n",
    "def get_merge_order(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        return float('inf')  # was never merged\n",
    "    else:\n",
    "        return cluster.order\n",
    "    \n",
    "def get_children(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        raise TypeError(\"Leaf has no children\")\n",
    "    else:\n",
    "        return cluster.children\n",
    "    \n",
    "class Merged(NamedTuple):\n",
    "    children: tuple\n",
    "    order: int\n",
    "\n",
    "def bottom_up_cluster(inputs: List[Vector],\n",
    "                      distance_agg: Callable = min):\n",
    "    # Start with all leaves\n",
    "    clusters: List[Cluster] = [Leaf(input) for input in inputs]\n",
    "\n",
    "    def pair_distance(pair):\n",
    "        return cluster_distance(pair[0], pair[1], distance_agg)\n",
    " \n",
    "    # as long as we have more than one cluster left...\n",
    "    while len(clusters) > 1:\n",
    "        # find the two closest clusters\n",
    "        c1, c2 = min(((cluster1, cluster2) \n",
    "                      for i, cluster1 in enumerate(clusters)\n",
    "                      for cluster2 in clusters[:i]),\n",
    "                      key=pair_distance)\n",
    "\n",
    "        # remove them from the list of clusters\n",
    "        clusters = [c for c in clusters if c != c1 and c != c2]\n",
    "\n",
    "        # merge them, using merge_order = # of clusters left\n",
    "        merged_cluster = Merged((c1, c2), order=len(clusters))\n",
    "\n",
    "        # and add their merge\n",
    "        clusters.append(merged_cluster)\n",
    "\n",
    "    # when there's only one cluster left, return it\n",
    "    return clusters[0]\n",
    "\n",
    "def generate_clusters(base_cluster,\n",
    "                      num_clusters):\n",
    "    # start with a list with just the base cluster\n",
    "    clusters = [base_cluster]\n",
    "\n",
    "    # as long as we don't have enough clusters yet...\n",
    "    while len(clusters) < num_clusters:\n",
    "        # choose the last-merged of our clusters\n",
    "        next_cluster = min(clusters, key=get_merge_order)\n",
    "        # remove it from the list\n",
    "        clusters = [c for c in clusters if c != next_cluster]\n",
    "\n",
    "        # and add its children to the list (i.e., unmerge it)\n",
    "        clusters.extend(get_children(next_cluster))\n",
    "\n",
    "    # once we have enough clusters...\n",
    "    return clusters\n",
    "\n",
    "def cosine_similarity(v1: Vector, v2: Vector) -> float:\n",
    "    return dot(v1, v2) / math.sqrt(dot(v1, v1) * dot(v2, v2))\n",
    "\n",
    "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
    "    if init == 'normal':\n",
    "        return random_normal(*dims)\n",
    "    elif init == 'uniform':\n",
    "        return random_uniform(*dims)\n",
    "    elif init == 'xavier':\n",
    "        variance = len(dims) / sum(dims)\n",
    "        return random_normal(*dims, variance=variance)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown init: {init}\")\n",
    "        \n",
    "def random_normal() -> float:\n",
    "    \"\"\"Returns a random draw from a standard normal distribution\"\"\"\n",
    "    return inverse_normal_cdf(random.random())\n",
    "\n",
    "def random_normal(*dims: int,\n",
    "                  mean: float = 0.0,\n",
    "                  variance: float = 1.0) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [mean + variance * inverse_normal_cdf(random.random())\n",
    "                for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_normal(*dims[1:], mean=mean, variance=variance)                for _ in range(dims[0])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_interests = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\",\n",
    "\"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\",\n",
    "\"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"], \n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_interests = Counter(interest\n",
    "                            for user_interests in users_interests\n",
    "                            for interest in user_interests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Hadoop': 2,\n",
       "         'Big Data': 3,\n",
       "         'HBase': 3,\n",
       "         'Java': 3,\n",
       "         'Spark': 1,\n",
       "         'Storm': 1,\n",
       "         'Cassandra': 2,\n",
       "         'NoSQL': 1,\n",
       "         'MongoDB': 2,\n",
       "         'Postgres': 2,\n",
       "         'Python': 4,\n",
       "         'scikit-learn': 2,\n",
       "         'scipy': 1,\n",
       "         'numpy': 1,\n",
       "         'statsmodels': 2,\n",
       "         'pandas': 2,\n",
       "         'R': 4,\n",
       "         'statistics': 3,\n",
       "         'regression': 3,\n",
       "         'probability': 3,\n",
       "         'machine learning': 2,\n",
       "         'decision trees': 1,\n",
       "         'libsvm': 2,\n",
       "         'C++': 2,\n",
       "         'Haskell': 1,\n",
       "         'programming languages': 1,\n",
       "         'mathematics': 1,\n",
       "         'theory': 1,\n",
       "         'Mahout': 1,\n",
       "         'neural networks': 2,\n",
       "         'deep learning': 2,\n",
       "         'artificial intelligence': 2,\n",
       "         'MapReduce': 1,\n",
       "         'databases': 1,\n",
       "         'MySQL': 1,\n",
       "         'support vector machines': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_interests = Counter(interest\n",
    "                            for user_interests in users_interests\n",
    "                            for interest in user_interests)\n",
    "popular_interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def most_popular_new_interests(\n",
    "        user_interests: List[str],\n",
    "        max_results: int = 5) -> List[Tuple[str, int]]:\n",
    "    suggestions = [(interest, frequency)\n",
    "                   for interest, frequency in\n",
    "popular_interests.most_common()\n",
    "                   if interest not in user_interests]\n",
    "    return suggestions[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 4), ('R', 4), ('Big Data', 3), ('Java', 3), ('statistics', 3)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_popular_new_interests([\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big Data', 'C++', 'Cassandra', 'HBase', 'Hadoop', 'Haskell']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_interests = sorted({interest\n",
    "                           for user_interests in users_interests\n",
    "                           for interest in user_interests})\n",
    "unique_interests[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_interest_vector(user_interests: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Given a list of interests, produce a vector whose ith element is 1\n",
    "    if unique_interests[i] is in the list, 0 otherwise\n",
    "    \"\"\"\n",
    "    return [1 if interest in user_interests else 0\n",
    "            for interest in unique_interests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interest_vectors = [make_user_interest_vector(user_interests)\n",
    "                         for user_interests in users_interests]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarities = [[cosine_similarity(interest_vector_i,\n",
    "interest_vector_j) for interest_vector_j in user_interest_vectors] \n",
    "for interest_vector_i in user_interest_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_users_to(user_id: int) -> List[Tuple[int, float]]:\n",
    "    pairs = [(other_user_id, similarity)                      # Find other\n",
    "             for other_user_id, similarity in                 # users with\n",
    "                enumerate(user_similarities[user_id])         # nonzero\n",
    "             if user_id != other_user_id and similarity > 0]  # similarity.\n",
    "\n",
    "    return sorted(pairs,                                      # Sort them\n",
    "                  key=lambda pair: pair[-1],                  # most similar\n",
    "                  reverse=True)                               # first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 0.5669467095138409),\n",
       " (1, 0.3380617018914066),\n",
       " (8, 0.1889822365046136),\n",
       " (13, 0.1690308509457033),\n",
       " (5, 0.1543033499620919)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " most_similar_users_to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def user_based_suggestions(user_id: int,\n",
    "                           include_current_interests: bool = False):\n",
    "    # Sum up the similarities\n",
    "    suggestions: Dict[str, float] = defaultdict(float)\n",
    "    for other_user_id, similarity in most_similar_users_to(user_id): \n",
    "        for interest in users_interests[other_user_id]:\n",
    "            suggestions[interest] += similarity\n",
    "\n",
    "    # Convert them to a sorted list \n",
    "    suggestions = sorted(suggestions.items(),\n",
    "                     key=lambda pair: pair[-1],  # weight\n",
    "                     reverse=True)\n",
    "\n",
    "    # And (maybe) exclude already interests\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion, weight)\n",
    "                for suggestion, weight in suggestions\n",
    "                if suggestion not in users_interests[user_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MapReduce', 0.5669467095138409),\n",
       " ('MongoDB', 0.50709255283711),\n",
       " ('Postgres', 0.50709255283711),\n",
       " ('NoSQL', 0.3380617018914066),\n",
       " ('neural networks', 0.1889822365046136),\n",
       " ('deep learning', 0.1889822365046136),\n",
       " ('artificial intelligence', 0.1889822365046136),\n",
       " ('databases', 0.1690308509457033),\n",
       " ('MySQL', 0.1690308509457033),\n",
       " ('Python', 0.1543033499620919),\n",
       " ('R', 0.1543033499620919),\n",
       " ('C++', 0.1543033499620919),\n",
       " ('Haskell', 0.1543033499620919),\n",
       " ('programming languages', 0.1543033499620919)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_based_suggestions(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_user_matrix = [[user_interest_vector[j]\n",
    "                         for user_interest_vector in user_interest_vectors]\n",
    "                        for j, _ in enumerate(unique_interests)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j)\n",
    "                          for user_vector_j in interest_user_matrix] \n",
    "                         for user_vector_i in interest_user_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_interests_to(interest_id: int):\n",
    "    similarities = interest_similarities[interest_id]\n",
    "    pairs = [(unique_interests[other_interest_id], similarity)\n",
    "             for other_interest_id, similarity in enumerate(similarities)\n",
    "             if interest_id != other_interest_id and similarity > 0]\n",
    "    return sorted(pairs,\n",
    "                  key=lambda pair: pair[-1],\n",
    "                  reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hadoop', 0.8164965809277261),\n",
       " ('Java', 0.6666666666666666),\n",
       " ('MapReduce', 0.5773502691896258),\n",
       " ('Spark', 0.5773502691896258),\n",
       " ('Storm', 0.5773502691896258),\n",
       " ('Cassandra', 0.4082482904638631),\n",
       " ('artificial intelligence', 0.4082482904638631),\n",
       " ('deep learning', 0.4082482904638631),\n",
       " ('neural networks', 0.4082482904638631),\n",
       " ('HBase', 0.3333333333333333)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_interests_to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_based_suggestions(user_id: int,\n",
    "                           include_current_interests: bool = False):\n",
    "    # Add up the similar interests\n",
    "    suggestions = defaultdict(float)\n",
    "    user_interest_vector = user_interest_vectors[user_id]\n",
    "    for interest_id, is_interested in enumerate(user_interest_vector):\n",
    "        if is_interested == 1:\n",
    "            similar_interests = most_similar_interests_to(interest_id)\n",
    "            for interest, similarity in similar_interests:\n",
    "                suggestions[interest] += similarity\n",
    "\n",
    "    # Sort them by weight \n",
    "    suggestions = sorted(suggestions.items(),\n",
    "                         key=lambda pair: pair[-1],\n",
    "                         reverse=True)\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion, weight)\n",
    "                for suggestion, weight in suggestions\n",
    "                if suggestion not in users_interests[user_id]]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MapReduce', 1.861807319565799),\n",
       " ('MongoDB', 1.3164965809277263),\n",
       " ('Postgres', 1.3164965809277263),\n",
       " ('NoSQL', 1.2844570503761732),\n",
       " ('MySQL', 0.5773502691896258),\n",
       " ('databases', 0.5773502691896258),\n",
       " ('Haskell', 0.5773502691896258),\n",
       " ('programming languages', 0.5773502691896258),\n",
       " ('artificial intelligence', 0.4082482904638631),\n",
       " ('deep learning', 0.4082482904638631),\n",
       " ('neural networks', 0.4082482904638631),\n",
       " ('C++', 0.4082482904638631),\n",
       " ('Python', 0.2886751345948129),\n",
       " ('R', 0.2886751345948129)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_based_suggestions(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This points to the current directory, modify if your files are elsewhere.\n",
    "MOVIES = \"u.item\"   # pipe-delimited: movie_id|title|...\n",
    "RATINGS = \"u.data\"  # tab-delimited: user_id, movie_id, rating, timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rating(NamedTuple):\n",
    "    user_id: str \n",
    "    movie_id: str\n",
    "    rating: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify this encoding to avoid a UnicodeDecodeError.\n",
    "# See: https://stackoverflow.com/a/53136168/1076346.\n",
    "with open(MOVIES, encoding=\"iso-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"|\")\n",
    "    movies = {movie_id: title for movie_id, title, *_ in reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of [Rating]\n",
    "with open(RATINGS, encoding=\"iso-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    ratings = [Rating(user_id, movie_id, float(rating))\n",
    "               for user_id, movie_id, rating, _ in reader]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36 Star Wars (1977)\n",
      "4.20 Empire Strikes Back, The (1980)\n",
      "4.01 Return of the Jedi (1983)\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Data structure for accumulating ratings by movie_id\n",
    "star_wars_ratings = {movie_id: []\n",
    "                     for movie_id, title in movies.items()\n",
    "                     if re.search(\"Star Wars|Empire Strikes|Jedi\", title)}\n",
    " \n",
    "# Iterate over ratings, accumulating the Star Wars ones\n",
    "for rating in ratings:\n",
    "    if rating.movie_id in star_wars_ratings:\n",
    "        star_wars_ratings[rating.movie_id].append(rating.rating)\n",
    " \n",
    "# Compute the average rating for each movie\n",
    "avg_ratings = [(sum(title_ratings) / len(title_ratings), movie_id)\n",
    "               for movie_id, title_ratings in star_wars_ratings.items()]\n",
    " \n",
    "# And then print them in order\n",
    "for avg_rating, movie_id in sorted(avg_ratings, reverse=True):\n",
    "    print(f\"{avg_rating:.2f} {movies[movie_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "random.shuffle(ratings)\n",
    "\n",
    "split1 = int(len(ratings) * 0.7)\n",
    "split2 = int(len(ratings) * 0.85)\n",
    "\n",
    "train = ratings[:split1]              # 70% of the data\n",
    "validation = ratings[split1:split2]   # 15% of the data\n",
    "test = ratings[split2:]               # 15% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_36228/3072576367.py:1: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  avg_rating = np.sum(rating.rating for rating in train) / len(train)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_36228/3072576367.py:2: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  baseline_error = np.sum((rating.rating - avg_rating) ** 2\n"
     ]
    }
   ],
   "source": [
    "avg_rating = np.sum(rating.rating for rating in train) / len(train)\n",
    "baseline_error = np.sum((rating.rating - avg_rating) ** 2\n",
    "                     for rating in test) / len(test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_DIM = 2\n",
    " \n",
    "# Find unique ids\n",
    "user_ids = {rating.user_id for rating in ratings}\n",
    "movie_ids = {rating.movie_id for rating in ratings}\n",
    " \n",
    "# Then create a random vector per id\n",
    "user_vectors = {user_id: random_tensor(EMBEDDING_DIM)\n",
    "                for user_id in user_ids}\n",
    "movie_vectors = {movie_id: random_tensor(EMBEDDING_DIM)\n",
    "                 for movie_id in movie_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(dataset: List[Rating],\n",
    "         learning_rate: float = None) -> None:\n",
    "    with tqdm.tqdm(dataset) as t: \n",
    "        loss = 0.0\n",
    "        for i, rating in enumerate(t):\n",
    "            movie_vector = movie_vectors[rating.movie_id]\n",
    "            user_vector = user_vectors[rating.user_id]\n",
    "            predicted = dot(user_vector, movie_vector)\n",
    "            error = predicted - rating.rating\n",
    "            loss += error ** 2\n",
    "\n",
    "            if learning_rate is not None:\n",
    "                #     predicted = m_0 * u_0 + ... + m_k * u_k\n",
    "                # So each u_j enters output with coefficent m_j\n",
    "                # and each m_j enters output with coefficient u_j\n",
    "                user_gradient = [error * m_j for m_j in movie_vector]\n",
    "                movie_gradient = [error * u_j for u_j in user_vector]\n",
    "\n",
    "                # Take gradient steps\n",
    "                for j in range(EMBEDDING_DIM):\n",
    "                    user_vector[j] -= learning_rate * user_gradient[j]\n",
    "                    movie_vector[j] -= learning_rate * movie_gradient[j]\n",
    "\n",
    "            t.set_description(f\"avg loss: {loss / (i + 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.045000000000000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 1.1185114816214152: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 70000/70000 [00:46<00:00, 1507.19it/s]\n",
      "avg loss: 1.0611231221147297: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 15000/15000 [00:11<00:00, 1263.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.03280500000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.983055374676988: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 70000/70000 [00:54<00:00, 1287.74it/s]\n",
      "avg loss: 0.9566223872328043: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 70000/70000 [00:52<00:00, 1325.45it/s]\n",
      "avg loss: 1.0186082845279554: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 15000/15000 [00:11<00:00, 1258.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.02657205000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.9961950115006796: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 15000/15000 [00:11<00:00, 1312.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.021523360500000012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.9060932161473823: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 70000/70000 [00:53<00:00, 1313.29it/s]\n",
      "avg loss: 0.8941972739474312: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 70000/70000 [00:55<00:00, 1256.24it/s]\n",
      "avg loss: 0.9813851685641196: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 15000/15000 [00:11<00:00, 1265.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.01743392200500001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8838501814068278: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 70000/70000 [00:57<00:00, 1220.10it/s]\n",
      "avg loss: 0.8747655165744322: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 70000/70000 [00:55<00:00, 1263.74it/s]\n",
      "avg loss: 0.9708178724458619: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 15000/15000 [00:11<00:00, 1344.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0.014121476824050006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.9630628855735579: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 15000/15000 [00:11<00:00, 1300.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.011438396227480507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.9599699192378287: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 15000/15000 [00:11<00:00, 1332.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.010294556604732457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8423186774829218: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 70000/70000 [00:51<00:00, 1356.19it/s]\n",
      "avg loss: 0.8296219358583735: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 70000/70000 [00:53<00:00, 1297.03it/s]\n",
      "avg loss: 0.9483575111621965: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 15000/15000 [00:11<00:00, 1284.58it/s]\n",
      "avg loss: 0.9482360815178427: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 15000/15000 [00:11<00:00, 1318.42it/s]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "for epoch in range(20):\n",
    "    learning_rate *= 0.9\n",
    "    print(epoch, learning_rate)\n",
    "    loop(train, learning_rate=learning_rate)\n",
    "    loop(validation)\n",
    "loop(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
