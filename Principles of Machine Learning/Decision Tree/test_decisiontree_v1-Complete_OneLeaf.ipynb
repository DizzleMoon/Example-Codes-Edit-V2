{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "# import pygal\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import urllib.request\n",
    "import requests\n",
    "import curl\n",
    "import pycurl\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from IPython import qt\n",
    "from matplotlib.pyplot import figure\n",
    "from py.xml import raw\n",
    "from requests.api import get\n",
    "from matplotlib import pyplot as plt\n",
    "# from scratch.working_with_data import rescale\n",
    "# from scratch.multiple_regression import least_squares_fit, predict\n",
    "# from scratch.gradient_descent import gradient_step\n",
    "\n",
    "# from stats import mean, median, de_mean, standard_deviation, correlation\n",
    "# from gradient_descent import minimize_stochastic, maximize_stochastic, maximize_batch\n",
    "# from vector import dot, vector_add\n",
    "# from normal import normal_cdf\n",
    "# from matrix import make_matrix, get_column, shape, matrix_multiply\n",
    "# from logistic_regression import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "\n",
    "from scipy.optimize import fmin_tnc\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from typing import*\n",
    "\n",
    "from collections import*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# def add(a, b): return a + b\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c , v):\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = np.sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = data.shape[0]\n",
    "    \n",
    "    # Vector Mean\n",
    "#     n = len(data)\n",
    "#     m = np.sum(data,axis=0)\n",
    "#     means = np.multiply(1/n,m)\n",
    "    means = vector_mean(data)\n",
    "    \n",
    "    # Standard Deviaiton\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "    return means,stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = data.shape[0]\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    grad_step = np.add(v,step)\n",
    "    return grad_step\n",
    "\n",
    "# def predict(alpha, beta, x_i):\n",
    "#     pred = beta * x_i + alpha\n",
    "#     return pred\n",
    "\n",
    "# def error(x, y, beta):\n",
    "#     \"\"\"\n",
    "#     The error from predicting beta * x_i + alpha\n",
    "#     when the actual value is y_i\n",
    "#     \"\"\"\n",
    "#     err_fin = predict(alpha, beta, x_i) - y_i\n",
    "#     return err_fin\n",
    "\n",
    "def predict(x, beta):\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x, y, beta):\n",
    "    return predict(x, beta) - y \n",
    "\n",
    "def sqerror_gradient(x, y, beta):\n",
    "    err = error(x, y, beta)\n",
    "    err_fin = [2 * err * x_i for x_i in x]\n",
    "    return err_fin\n",
    "\n",
    "def least_squares_fit(xs, ys, learning_rate = 0.001, num_steps = 1000, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [np.random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def _negative_log_likelihood(x, y, beta):\n",
    "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))\n",
    "    \n",
    "def negative_log_likelihood(xs, ys, beta):\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))\n",
    "\n",
    "def _negative_log_partial_j(x, y, beta, j):\n",
    "    \"\"\"\n",
    "    The jth partial derivative for one data point.\n",
    "    Here i is the index of the data point.\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x, y, beta):\n",
    "    \"\"\"\n",
    "    The gradient for one data point.\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs, ys,beta):\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "                                                                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i/100.0 for i in range(101) ]\n",
    "entropy_x = [0] + [-p*math.log(p,2) for p in x if p > 0]\n",
    "# import matplotlib.pyplot as plt\n",
    "plt.plot(x, entropy_x)\n",
    "plt.xlabel('probability')\n",
    "plt.ylabel('-p*log(p)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy\n",
    "def entropy(class_probabilities):\n",
    "    \"\"\"Given a list of class probabilities, compute the entropy\"\"\"\n",
    "    ent = sum(-p * math.log(p, 2)\n",
    "               for p in class_probabilities \n",
    "               if p > 0)\n",
    "#     ent_0 = []\n",
    "#     for p in class_probabilities :\n",
    "#         if p > 0:\n",
    "#             ent_0.append(-p * math.log(p,2))\n",
    "#     ent = sum(ent_0)\n",
    "            \n",
    "    return ent\n",
    "\n",
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    class_prob = [count / total_count for count in Counter(labels).values()]\n",
    "    return class_prob\n",
    "\n",
    "def data_entropy(labels):\n",
    "    data_ent = entropy(class_probabilities(labels))\n",
    "    return data_ent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Entropy of a Partition\n",
    "\n",
    "def partition_entropy(subsets):\n",
    "    \"\"\"Returns the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    part_ent = sum(data_entropy(subset) * len(subset) / total_count for subset in subsets)\n",
    "\n",
    "    return part_ent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,50)\n",
    "# x = pd.DataFrame({'x':x})\n",
    "\n",
    "y1 = np.random.uniform(10,15,10)\n",
    "y2 = np.random.uniform(20,25,10)\n",
    "y3 = np.random.uniform(0,5,10)\n",
    "y4 = np.random.uniform(30,32,10)\n",
    "y5 = np.random.uniform(13,17,10)\n",
    "\n",
    "y = np.concatenate((y1,y2,y3,y4,y5))\n",
    "y = y[:,None]\n",
    "\n",
    "x = x.tolist()\n",
    "y = y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6935361388961919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Candidate(level='Senior', did_well=False),\n",
       " Candidate(level='Senior', did_well=False),\n",
       " Candidate(level='Mid', did_well=True),\n",
       " Candidate(level='Junior', did_well=True),\n",
       " Candidate(level='Junior', did_well=True),\n",
       " Candidate(level='Junior', did_well=False),\n",
       " Candidate(level='Mid', did_well=True),\n",
       " Candidate(level='Senior', did_well=False),\n",
       " Candidate(level='Senior', did_well=True),\n",
       " Candidate(level='Junior', did_well=True),\n",
       " Candidate(level='Senior', did_well=True),\n",
       " Candidate(level='Mid', did_well=True),\n",
       " Candidate(level='Mid', did_well=True),\n",
       " Candidate(level='Junior', did_well=False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Decision Tree\n",
    "\n",
    "# Test\n",
    "# Create class\n",
    "class Candidate(NamedTuple):\n",
    "    level: int\n",
    "    did_well: Optional[bool] = None  # allow unlabeled data\n",
    "# Data\n",
    "# inputs = [Candidate('Senior', 'Java',   False, False, False),\n",
    "#           Candidate('Senior', 'Java',   False, True,  False),\n",
    "#           Candidate('Mid',    'Python', False, False, True),\n",
    "#           Candidate('Junior', 'Python', False, False, True),\n",
    "#           Candidate('Junior', 'R',      True,  False, True),\n",
    "#           Candidate('Junior', 'R',      True,  True,  False),\n",
    "#           Candidate('Mid',    'R',      True,  True,  True),\n",
    "#           Candidate('Senior', 'Python', False, False, False),\n",
    "#           Candidate('Senior', 'R',      True,  False, True),\n",
    "#           Candidate('Junior', 'Python', True,  False, True),\n",
    "#           Candidate('Senior', 'Python', True,  True,  True),\n",
    "#           Candidate('Mid',    'Python', False, True,  True),\n",
    "#           Candidate('Mid',    'Java',   True,  False, True),\n",
    "#           Candidate('Junior', 'Python', False, True,  False)\n",
    "#          ]\n",
    "\n",
    "\n",
    "inputs = [Candidate('Senior', False),\n",
    "          Candidate('Senior', False),\n",
    "          Candidate('Mid',    True),\n",
    "          Candidate('Junior', True),\n",
    "          Candidate('Junior', True),\n",
    "          Candidate('Junior', False),\n",
    "          Candidate('Mid',  True),\n",
    "          Candidate('Senior', False),\n",
    "          Candidate('Senior', True),\n",
    "          Candidate('Junior', True),\n",
    "          Candidate('Senior', True),\n",
    "          Candidate('Mid',    True),\n",
    "          Candidate('Mid',    True),\n",
    "          Candidate('Junior', False)\n",
    "         ]\n",
    "\n",
    "\n",
    "# Generic type of inputs\n",
    "T = TypeVar('T')\n",
    "\n",
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\"\n",
    "    partitions: Dict[Any, List[T]] = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key = getattr(input, attribute)  # value of the specified attribute\n",
    "        partitions[key].append(input)    # add input to the correct partition\n",
    "    return partitions\n",
    "\n",
    "# Compute Entropy\n",
    "def partition_entropy_by(inputs, attribute, label_attribute):\n",
    "    \"\"\"Compute the entropy corresponding to the given partition\"\"\"\n",
    "    # partitions consist of our inputs \n",
    "    partitions = partition_by(inputs, attribute)\n",
    "\n",
    "    # but partition_entropy needs just the class labels\n",
    "    labels = [[getattr(input, label_attribute) for input in partition]\n",
    "              for partition in partitions.values()]\n",
    "\n",
    "    return partition_entropy(labels)\n",
    "\n",
    "for key in ['level']:\n",
    "    print(key, partition_entropy_by(inputs, key, 'did_well'))\n",
    "\n",
    "# partition_entropy_by(inputs, 'level', 'did_well')\n",
    "\n",
    "# senior_inputs = [input for input in inputs if input.level == 'Senior']\n",
    "\n",
    "# senior_inputs\n",
    "\n",
    "# type(senior_inputs)\n",
    "inputs\n",
    "# type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11.468390843210004],\n",
       " [11.71079528945469],\n",
       " [14.586433538378067],\n",
       " [10.88478765284087],\n",
       " [13.461486353554154],\n",
       " [10.217358978460199],\n",
       " [12.018300749402178],\n",
       " [11.357256717920064],\n",
       " [12.69853486627685],\n",
       " [11.553187054934172],\n",
       " [21.734250940913046],\n",
       " [21.714507227673803],\n",
       " [21.029369048499017],\n",
       " [20.43545957841347],\n",
       " [24.340520929548145],\n",
       " [23.53427141415523],\n",
       " [21.135522428507553],\n",
       " [20.478018075738706],\n",
       " [23.555455567676177],\n",
       " [21.64369002829859],\n",
       " [4.378695581357639],\n",
       " [3.6539625732103795],\n",
       " [1.8505044777890896],\n",
       " [3.7244172900442756],\n",
       " [3.08186299875584],\n",
       " [3.2426463725095056],\n",
       " [0.7068071595447312],\n",
       " [4.047147333706989],\n",
       " [0.8489420768140404],\n",
       " [1.8099654373987613],\n",
       " [30.19627095939926],\n",
       " [31.81819858496529],\n",
       " [30.946702984820135],\n",
       " [31.164583952816955],\n",
       " [31.683571425600704],\n",
       " [30.106185313132322],\n",
       " [30.295600015066515],\n",
       " [30.74530324388674],\n",
       " [30.6497281841069],\n",
       " [31.597872813384278],\n",
       " [16.71418722645306],\n",
       " [15.305829380491303],\n",
       " [13.196698515657122],\n",
       " [14.916037557128607],\n",
       " [16.096335087101554],\n",
       " [14.061200739938778],\n",
       " [15.295068882589279],\n",
       " [16.983509535018875],\n",
       " [15.294145059544572],\n",
       " [14.428174360557565]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# senior_inputs = [input for input in inputs if input.level == x[0]]\n",
    "# senior_inputs\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Attribute:  did_well\n",
      "Label Counts:  Counter({True: 9, False: 5})\n",
      "Most Common Label :  True\n",
      "Split Attributes:  ['level']\n",
      "Length of Label Counts:  2\n",
      "Best Attribute: level\n",
      "Target Attribute:  did_well\n",
      "Label Counts:  Counter({False: 3, True: 2})\n",
      "Most Common Label :  False\n",
      "Split Attributes:  []\n",
      "Length of Label Counts:  2\n",
      "Target Attribute:  did_well\n",
      "Label Counts:  Counter({True: 4})\n",
      "Most Common Label :  True\n",
      "Split Attributes:  []\n",
      "Length of Label Counts:  1\n",
      "Target Attribute:  did_well\n",
      "Label Counts:  Counter({True: 3, False: 2})\n",
      "Most Common Label :  True\n",
      "Split Attributes:  []\n",
      "Length of Label Counts:  2\n",
      "Split(attribute='level', subtrees={'Senior': Leaf(value=False), 'Mid': Leaf(value=True), 'Junior': Leaf(value=True)}, default_value=True)\n",
      "Tree Attribute:  level\n",
      "a_trees True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting It All Together\n",
    "\n",
    "class Leaf(NamedTuple): \n",
    "     value: Any\n",
    "            \n",
    "class Split(NamedTuple):\n",
    "    attribute:Any\n",
    "    subtrees: dict\n",
    "    default_value: Any = None\n",
    "\n",
    "DecisionTree = Union[Leaf, Split]\n",
    "\n",
    "# Representation\n",
    "# hiring_tree = Split('level', {   # first, consider \"level\"\n",
    "#     'Junior': Split('phd', {     # if level is \"Junior\", next look at \"phd\"\n",
    "#         False: Leaf(True),       #   if \"phd\" is False, predict True\n",
    "#         True: Leaf(False)        #   if \"phd\" is True, predict False\n",
    "#     }),\n",
    "#     'Mid': Leaf(True),           # if level is \"Mid\", just predict True\n",
    "#     'Senior': Split('tweets', {  # if level is \"Senior\", look at \"tweets\"\n",
    "#         False: Leaf(False),      #   if \"tweets\" is False, predict False\n",
    "#         True: Leaf(True)         #   if \"tweets\" is True, predict True\n",
    "#     })\n",
    "# })\n",
    "\n",
    "hiring_tree = Split('level',{'Junior' : Split('phd', {False:Leaf(True),True: Leaf(False)}),\n",
    "                             'Mid': Leaf(True), 'Senior': Split('tweets', {False:Leaf(False),True:Leaf(True)})})\n",
    "\n",
    "def classify(tree, input):\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "\n",
    "    # If this is a leaf node, return its value\n",
    "    if isinstance(tree, Leaf):\n",
    "        return tree.value \n",
    "\n",
    "    # Otherwise this tree consists of an attribute to split on\n",
    "    # and a dictionary whose keys are values of that attribute\n",
    "    # and whose values are subtrees to consider next \n",
    "    \n",
    "    subtree_key = getattr(input, tree.attribute)\n",
    "    print(\"Tree Attribute: \", tree.attribute)\n",
    "\n",
    "    if subtree_key not in tree.subtrees:   # If no subtree for key,\n",
    "        a_trees = tree.default_value \n",
    "        print(\"a_trees\",a_trees)\n",
    "        return a_trees         # return the default value.\n",
    "\n",
    "    subtree = tree.subtrees[subtree_key]   # Choose the appropriate subtree\n",
    "    print('Subtree1: ', subtree)\n",
    "    return classify(subtree, input)        # and use it to classify the input.\n",
    "\n",
    "def build_tree_id3(inputs, split_attributes, target_attribute):\n",
    "    print(\"Target Attribute: \", target_attribute)\n",
    "    # Count target labels\n",
    "    label_counts = Counter(getattr(input, target_attribute) for input in inputs)\n",
    "    most_common_label = label_counts.most_common(1)[0][0]\n",
    "    print(\"Label Counts: \", label_counts)\n",
    "    print(\"Most Common Label : \", most_common_label)\n",
    "    print(\"Split Attributes: \", split_attributes)\n",
    "    print(\"Length of Label Counts: \", len(label_counts))\n",
    "    # If there's a unique label, predict it\n",
    "    if len(label_counts) == 1:\n",
    "        return Leaf(most_common_label)\n",
    "\n",
    "    # If no split attributes left, return the majority label\n",
    "    if not split_attributes:\n",
    "        return Leaf(most_common_label)\n",
    "    \n",
    "    # Otherwise split by the best attribute\n",
    "    def split_entropy(attribute):\n",
    "#     \"\"\"Helper function for finding the best attribute\"\"\"\n",
    "        return partition_entropy_by(inputs, attribute, target_attribute)\n",
    "    \n",
    "    best_attribute = min(split_attributes, key=split_entropy)\n",
    "    print(\"Best Attribute:\", best_attribute)\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_attributes = [a for a in split_attributes if a != best_attribute]\n",
    "    # Recursively build the subtrees\n",
    "    subtrees = {attribute_value : build_tree_id3(subset, new_attributes, target_attribute)\n",
    "               for attribute_value, subset in partitions.items()}\n",
    "#     print(\"SubTrees: \", subtrees)\n",
    "    return Split(best_attribute, subtrees, default_value=most_common_label)\n",
    "\n",
    "tree = build_tree_id3(inputs, ['level'], 'did_well')\n",
    "print(tree)\n",
    "\n",
    "# Tests\n",
    "# test_a = classify(tree, Candidate(\"Junior\"))\n",
    "# test_b = classify(tree, Candidate(\"Junior\"))\n",
    "test_c = classify(tree, Candidate(\"Intern\"))\n",
    "test_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "\n",
    "def forest_classify(trees, input):\n",
    "    votes = [classify(tree, input) for tree in trees]\n",
    "    vote_counts = Counter(votes)\n",
    "    return vote_counts.most_common(1)[0][0]\n",
    "\n",
    "def build_forest_id3(inputs, split_candidates=None, num_split_candidates=None):\n",
    "\n",
    "    # if this is our first pass,\n",
    "    # all keys of the first input are split candidates\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "\n",
    "    \n",
    "    # count Trues and Falses in the inputs\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = len(inputs) - num_trues\n",
    "    \n",
    "    if num_trues == 0:\n",
    "        return False\n",
    "    \n",
    "    if num_falses == 0:\n",
    "        return True\n",
    "    \n",
    "    if not split_candidates:\n",
    "        return num_trues >= num_falses\n",
    "    \n",
    "    # if there's already few enough split candidates, look at all of them\n",
    "    if num_split_candidates is None or len(split_candidates) <= num_split_candidates:\n",
    "        sampled_split_candidates = split_candidates    \n",
    "    # otherwise pick a random sample\n",
    "    else:\n",
    "        sampled_split_candidates = random.sample(split_candidates, num_split_candidates)\n",
    "\n",
    "    # otherwise, split on the best attribute\n",
    "    best_attribute = min(sampled_split_candidates, key=split_entropy)\n",
    "    \n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates if a != best_attribute]\n",
    "    \n",
    "    # recursively build the subtrees\n",
    "    subtrees = { attribute : build_tree_id3(subset, new_candidates)\n",
    "                 for attribute, subset in partitions.items() }\n",
    "    \n",
    "    subtrees[None] = num_trues > num_falses # default case\n",
    "    \n",
    "    return (best_attribute, subtrees)\n",
    "\n",
    "def build_forest(inputs, n=3):\n",
    "    return [build_tree_id3(inputs, num_split_candidates=3) for i in range(n)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if there are already few enough split candidates, look at all of them\n",
    "# if len(split_candidates) <= self.num_split_candidates:\n",
    "#     sampled_split_candidates = split_candidates\n",
    "# # otherwise pick a random sample\n",
    "# # else:\n",
    "# #     sampled_split_candidates = random.sample(split_candidates, self.num_split_candidates)                                    self.num_split_candidates)\n",
    "\n",
    "# # now choose the best attribute only from those candidates\n",
    "# # best_attribute = min(sampled_split_candidates, key=split_entropy)\n",
    "\n",
    "# # partitions = partition_by(inputs, best_attribute)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
