{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "\n",
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "# import pygal\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import urllib.request\n",
    "import requests\n",
    "import curl\n",
    "import pycurl\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from IPython import qt\n",
    "from matplotlib.pyplot import figure\n",
    "from py.xml import raw\n",
    "from requests.api import get\n",
    "from matplotlib import pyplot as plt\n",
    "# from scratch.working_with_data import rescale\n",
    "# from scratch.multiple_regression import least_squares_fit, predict\n",
    "# from scratch.gradient_descent import gradient_step\n",
    "\n",
    "# from stats import mean, median, de_mean, standard_deviation, correlation\n",
    "# from gradient_descent import minimize_stochastic, maximize_stochastic, maximize_batch\n",
    "# from vector import dot, vector_add\n",
    "# from normal import normal_cdf\n",
    "# from matrix import make_matrix, get_column, shape, matrix_multiply\n",
    "# from logistic_regression import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "\n",
    "from scipy.optimize import fmin_tnc\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from typing import*\n",
    "\n",
    "from collections import*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# def add(a, b): return a + b\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c , v):\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = np.sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = data.shape[0]\n",
    "    \n",
    "    # Vector Mean\n",
    "#     n = len(data)\n",
    "#     m = np.sum(data,axis=0)\n",
    "#     means = np.multiply(1/n,m)\n",
    "    means = vector_mean(data)\n",
    "    \n",
    "    # Standard Deviaiton\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "    return means,stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = data.shape[0]\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    grad_step = np.add(v,step)\n",
    "    return grad_step\n",
    "\n",
    "# def predict(alpha, beta, x_i):\n",
    "#     pred = beta * x_i + alpha\n",
    "#     return pred\n",
    "\n",
    "# def error(x, y, beta):\n",
    "#     \"\"\"\n",
    "#     The error from predicting beta * x_i + alpha\n",
    "#     when the actual value is y_i\n",
    "#     \"\"\"\n",
    "#     err_fin = predict(alpha, beta, x_i) - y_i\n",
    "#     return err_fin\n",
    "\n",
    "def predict(x, beta):\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x, y, beta):\n",
    "    return predict(x, beta) - y \n",
    "\n",
    "def sqerror_gradient(x, y, beta):\n",
    "    err = error(x, y, beta)\n",
    "    err_fin = [2 * err * x_i for x_i in x]\n",
    "    return err_fin\n",
    "\n",
    "def least_squares_fit(xs, ys, learning_rate = 0.001, num_steps = 1000, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [np.random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def _negative_log_likelihood(x, y, beta):\n",
    "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))\n",
    "    \n",
    "def negative_log_likelihood(xs, ys, beta):\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))\n",
    "\n",
    "def _negative_log_partial_j(x, y, beta, j):\n",
    "    \"\"\"\n",
    "    The jth partial derivative for one data point.\n",
    "    Here i is the index of the data point.\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x, y, beta):\n",
    "    \"\"\"\n",
    "    The gradient for one data point.\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs, ys,beta):\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "                                                                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy\n",
    "def entropy(class_probabilities):\n",
    "    \"\"\"Given a list of class probabilities, compute the entropy\"\"\"\n",
    "    ent = sum(-p * math.log(p, 2)\n",
    "               for p in class_probabilities \n",
    "               if p > 0)\n",
    "#     ent_0 = []\n",
    "#     for p in class_probabilities :\n",
    "#         if p > 0:\n",
    "#             ent_0.append(-p * math.log(p,2))\n",
    "#     ent = sum(ent_0)\n",
    "            \n",
    "    return ent\n",
    "\n",
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    class_prob = [count / total_count for count in Counter(labels).values()]\n",
    "    return class_prob\n",
    "\n",
    "def data_entropy(labels):\n",
    "    data_ent = entropy(class_probabilities(labels))\n",
    "    return data_ent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Entropy of a Partition\n",
    "\n",
    "def partition_entropy(subsets):\n",
    "    \"\"\"Returns the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    part_ent = sum(data_entropy(subset) * len(subset) / total_count for subset in subsets)\n",
    "\n",
    "    return part_ent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlook 2.2299486392052588\n",
      "temp 2.276735927209087\n",
      "humidity 2.6644977792004614\n",
      "wind 2.536412500309067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Decision Tree using Panda\n",
    "# outlook = df['Outlook']\n",
    "# temp = df['Temp']\n",
    "# humidity = df['Humidity']\n",
    "# wind = df['Wind']\n",
    "# decision = df['Decision']\n",
    "\n",
    "class Candidate(NamedTuple):\n",
    "    outlook: str\n",
    "    temp: str\n",
    "    humidity: str\n",
    "    wind: str\n",
    "    decision: float  # allow unlabeled data\n",
    "        \n",
    "# inputs = Candidate(outlook,temp,humidity,wind,decision)\n",
    "\n",
    "inputs = [\n",
    "    Candidate('Sunny', 'Hot','High','Weak',25),\n",
    "    Candidate('Sunny', 'Hot','High','Strong', 30),\n",
    "    Candidate('Overcast','Hot','High','Weak', 46),\n",
    "    Candidate('Rain','Mild','High','Weak', 45),\n",
    "    Candidate('Rain','Cool', 'Normal','Weak', 52),\n",
    "    Candidate('Rain','Hot','Normal','Strong',23),\n",
    "    Candidate('Overcast','Cool','Normal','Strong', 43),\n",
    "    Candidate('Sunny','Mild','High','Weak', 35),\n",
    "    Candidate('Sunny','Cool','Normal','Weak', 38),\n",
    "    Candidate('Rain','Mild','Normal','Weak', 46),\n",
    "    Candidate('Sunny','Mild','Normal','Strong',  48),\n",
    "    Candidate('Overcast','Mild','High','Strong', 52),\n",
    "    Candidate('Overcast','Hot','Normal','Weak',  44),\n",
    "    Candidate('Rain','Mild','High','Strong', 30)\n",
    "]\n",
    "\n",
    "# inputs = []\n",
    "# for i in range(len(inputs)):\n",
    "#     inputs.append(Candidate(outlook[i],temp[i],humidity[i],wind[i],decision[i]))\n",
    "#     abc = inputs[0][0].keys()\n",
    "# Candidate\n",
    "# type(inputs)\n",
    "inputs[0][1]\n",
    "\n",
    "# Generic type of inputs\n",
    "T = TypeVar('T')\n",
    "\n",
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\"\n",
    "    partitions: Dict[Any, List[T]] = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key = getattr(input, attribute)  # value of the specified attribute\n",
    "        partitions[key].append(input)    # add input to the correct partition\n",
    "    return partitions\n",
    "\n",
    "# Compute Entropy\n",
    "def partition_entropy_by(inputs, attribute, label_attribute):\n",
    "    \"\"\"Compute the entropy corresponding to the given partition\"\"\"\n",
    "    # partitions consist of our inputs \n",
    "    partitions = partition_by(inputs, attribute)\n",
    "\n",
    "    # but partition_entropy needs just the class labels\n",
    "    labels = [[getattr(input, label_attribute) for input in partition]\n",
    "              for partition in partitions.values()]\n",
    "\n",
    "    return partition_entropy(labels)\n",
    "\n",
    "for key in ['outlook','temp','humidity','wind']:\n",
    "    print(key, partition_entropy_by(inputs, key, 'decision'))\n",
    "\n",
    "# partition_entropy_by(inputs, 'level', 'did_well')\n",
    "\n",
    "# senior_inputs = [input for input in inputs if input.level == 'Senior']\n",
    "\n",
    "# senior_inputs\n",
    "\n",
    "# type(senior_inputs)\n",
    "inputs\n",
    "type(inputs)\n",
    "\n",
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['outlook', 'temp', 'humidity', 'wind']\n",
      "['temp', 'humidity', 'wind']\n",
      "['humidity', 'wind']\n",
      "['humidity']\n",
      "['humidity']\n",
      "['humidity', 'wind']\n",
      "['wind']\n",
      "['wind']\n",
      "['humidity', 'wind']\n",
      "['temp', 'humidity', 'wind']\n",
      "['humidity', 'wind']\n",
      "['wind']\n",
      "['wind']\n",
      "['humidity', 'wind']\n",
      "['humidity', 'wind']\n",
      "['temp', 'humidity', 'wind']\n",
      "['humidity', 'wind']\n",
      "['wind']\n",
      "[]\n",
      "[]\n",
      "['wind']\n",
      "['humidity', 'wind']\n",
      "['humidity', 'wind']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting It All Together\n",
    "\n",
    "class Leaf(NamedTuple): \n",
    "     value: Any\n",
    "            \n",
    "class Split(NamedTuple):\n",
    "    attribute:Any\n",
    "    subtrees: dict\n",
    "    default_value: Any = None\n",
    "\n",
    "DecisionTree = Union[Leaf, Split]\n",
    "\n",
    "# Representation\n",
    "# hiring_tree = Split('level', {   # first, consider \"level\"\n",
    "#     'Junior': Split('phd', {     # if level is \"Junior\", next look at \"phd\"\n",
    "#         False: Leaf(True),       #   if \"phd\" is False, predict True\n",
    "#         True: Leaf(False)        #   if \"phd\" is True, predict False\n",
    "#     }),\n",
    "#     'Mid': Leaf(True),           # if level is \"Mid\", just predict True\n",
    "#     'Senior': Split('tweets', {  # if level is \"Senior\", look at \"tweets\"\n",
    "#         False: Leaf(False),      #   if \"tweets\" is False, predict False\n",
    "#         True: Leaf(True)         #   if \"tweets\" is True, predict True\n",
    "#     })\n",
    "# })\n",
    "\n",
    "# hiring_tree = Split('level',{'Junior' : Split('phd', {False:Leaf(True),True: Leaf(False)}),\n",
    "#                              'Mid': Leaf(True), 'Senior': Split('tweets', {False:Leaf(False),True:Leaf(True)})})\n",
    "\n",
    "def classify(tree, input):\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "\n",
    "    # If this is a leaf node, return its value\n",
    "    if isinstance(tree, Leaf):\n",
    "        return tree.value \n",
    "\n",
    "    # Otherwise this tree consists of an attribute to split on\n",
    "    # and a dictionary whose keys are values of that attribute\n",
    "    # and whose values are subtrees to consider next \n",
    "    \n",
    "    subtree_key = getattr(input, tree.attribute)\n",
    "\n",
    "    if subtree_key not in tree.subtrees:   # If no subtree for key,\n",
    "        return tree.default_value          # return the default value.\n",
    "\n",
    "    subtree = tree.subtrees[subtree_key]   # Choose the appropriate subtree\n",
    "    return classify(subtree, input)        # and use it to classify the input.\n",
    "\n",
    "def build_tree_id3(inputs, split_attributes, target_attribute):\n",
    "    # Count target labels\n",
    "    label_counts = Counter(getattr(input, target_attribute) for input in inputs)\n",
    "    most_common_label = label_counts.most_common(1)[0][0]\n",
    "    \n",
    "    print(split_attributes)\n",
    "    # If there's a unique label, predict it\n",
    "    if len(label_counts) == 1:\n",
    "        return Leaf(most_common_label)\n",
    "\n",
    "    # If no split attributes left, return the majority label\n",
    "    if not split_attributes:\n",
    "        return Leaf(most_common_label)\n",
    "    \n",
    "    # Otherwise split by the best attribute\n",
    "    def split_entropy(attribute):\n",
    "#     \"\"\"Helper function for finding the best attribute\"\"\"\n",
    "        return partition_entropy_by(inputs, attribute, target_attribute)\n",
    "\n",
    "    best_attribute = min(split_attributes, key=split_entropy)\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_attributes = [a for a in split_attributes if a != best_attribute]\n",
    "    # Recursively build the subtrees\n",
    "    subtrees = {attribute_value : build_tree_id3(subset, new_attributes, target_attribute)\n",
    "               for attribute_value, subset in partitions.items()}\n",
    "    return Split(best_attribute, subtrees, default_value=most_common_label)\n",
    "\n",
    "# tree = build_tree_id3(inputs, ['level', 'lang', 'tweets', 'phd'], 'did_well')\n",
    "tree = build_tree_id3(inputs, ['outlook','temp','humidity','wind'], 'decision')\n",
    "\n",
    "# # Tests\n",
    "test_a = classify(tree, Candidate(\"Rain\", \"Hot\", \"Normal\", \"Strong\",\"decison\"))\n",
    "# test_b = classify(tree, Candidate(\"Junior\", \"Java\", True, True))\n",
    "# test_c = classify(tree, Candidate(\"Intern\", \"Java\", True, True))\n",
    "# test_d = classify(tree, Candidate(\"Intern\", None, None, None))\n",
    "# test_d\n",
    "# tree2\n",
    "test_a\n",
    "# tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
