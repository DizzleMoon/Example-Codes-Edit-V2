{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import urllib.request\n",
    "import requests\n",
    "import curl\n",
    "import pycurl\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from IPython import qt\n",
    "from matplotlib.pyplot import figure\n",
    "from py.xml import raw\n",
    "from requests.api import get\n",
    "from matplotlib import pyplot as plt\n",
    "# from scratch.working_with_data import rescale\n",
    "# from scratch.multiple_regression import least_squares_fit, predict\n",
    "# from scratch.gradient_descent import gradient_step\n",
    "\n",
    "# from stats import mean, median, de_mean, standard_deviation, correlation\n",
    "# from gradient_descent import minimize_stochastic, maximize_stochastic, maximize_batch\n",
    "# from vector import dot, vector_add\n",
    "# from normal import normal_cdf\n",
    "# from matrix import make_matrix, get_column, shape, matrix_multiply\n",
    "# from logistic_regression import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "\n",
    "from scipy.optimize import fmin_tnc\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from typing import*\n",
    "\n",
    "from collections import*\n",
    "# from scipy import*\n",
    "from sklearn.metrics import*\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "# bltin_sum = np.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# def add(a, b): return a + b\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c , v):\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "#     return np.sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "#     gen = \n",
    "    return np.sum(np.fromiter((v_i * w_i for v_i, w_i in zip(v, w)),float))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = data.shape[0]\n",
    "    \n",
    "    # Vector Mean\n",
    "#     n = len(data)\n",
    "#     m = np.sum(data,axis=0)\n",
    "#     means = np.multiply(1/n,m)\n",
    "    means = vector_mean(data)\n",
    "    \n",
    "    # Standard Deviaiton\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "    return means,stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = data.shape[0]\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    grad_step = np.add(v,step)\n",
    "    return grad_step\n",
    "\n",
    "# def predict(alpha, beta, x_i):\n",
    "#     pred = beta * x_i + alpha\n",
    "#     return pred\n",
    "\n",
    "# def error(x, y, beta):\n",
    "#     \"\"\"\n",
    "#     The error from predicting beta * x_i + alpha\n",
    "#     when the actual value is y_i\n",
    "#     \"\"\"\n",
    "#     err_fin = predict(alpha, beta, x_i) - y_i\n",
    "#     return err_fin\n",
    "\n",
    "def predict(x, beta):\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x, y, beta):\n",
    "    return predict(x, beta) - y \n",
    "\n",
    "def sqerror_gradient(x, y, beta):\n",
    "    err = error(x, y, beta)\n",
    "    err_fin = [2 * err * x_i for x_i in x]\n",
    "    return err_fin\n",
    "\n",
    "def least_squares_fit(xs, ys, learning_rate = 0.001, num_steps = 1000, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [np.random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def _negative_log_likelihood(x, y, beta):\n",
    "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))\n",
    "    \n",
    "def negative_log_likelihood(xs, ys, beta):\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))\n",
    "\n",
    "def _negative_log_partial_j(x, y, beta, j):\n",
    "    \"\"\"\n",
    "    The jth partial derivative for one data point.\n",
    "    Here i is the index of the data point.\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x, y, beta):\n",
    "    \"\"\"\n",
    "    The gradient for one data point.\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs, ys,beta):\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "                                                                \n",
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def sigmoid(t: float) -> float: \n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "# Gradient Descent - step\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "def squared_distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\"\n",
    "    return sum_of_squares(subtract(v, w))\n",
    "\n",
    "def distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes the distance between v and w\"\"\"\n",
    "    return math.sqrt(squared_distance(v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = range(-10, 11)\n",
    "actuals = [derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h=0.001) for x in xs]\n",
    " \n",
    "# plot to show they're basically the same\n",
    "# import matplotlib.pyplot as plt\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "plt.plot(xs, actuals, 'rx', label='Actual')       # red  x\n",
    "plt.plot(xs, estimates, 'b+', label='Estimate')   # blue +\n",
    "plt.legend(loc=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[Vector],float], v, i, h: float):\n",
    "    \"\"\"Returns the i-th partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j == i else 0)    # add h to just the ith element of  v\n",
    "         for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[Vector], float],\n",
    "                      v: Vector,\n",
    "                      h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.965300468877181e-09"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# pick a random starting point\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v)    # compute the gradient at v\n",
    "    v = gradient_step(v, grad, -0.01)    # take a negative gradient step\n",
    "#     print(epoch, v)\n",
    "\n",
    "distance(v, [0, 0, 0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x ranges from -50 to 49, y is always 20 * x + 5\n",
    "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
    "    slope, intercept = theta \n",
    "    predicted = slope * x + intercept    # The prediction of the model.\n",
    "    error = (predicted - y)              # error is (predicted - actual).\n",
    "    squared_error = error ** 2           # We'll minimize squared error \n",
    "    grad = [2 * error * x, 2 * error]    # using its gradient.\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2500/2500 [00:00<00:00, 2512.87it/s]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Start with random values for slope and intercept\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)] \n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "with tqdm.trange(2500) as t:\n",
    "    for epoch in t: \n",
    "        # Compute the mean of the gradients\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "        # Take a step in that direction\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    #     print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar('T')  # this allows us to type \"generic\" functions\n",
    "\n",
    "def minibatches(dataset: List[T],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[T]]:\n",
    "    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\"\n",
    "    # start indexes 0, batch_size, 2 * batch_size, ...\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "\n",
    "    if shuffle: random.shuffle(batch_starts)  # shuffle the batches\n",
    " \n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.999999963709264"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "#     print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "intercept\n",
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.001527534785854\n",
      "4.923970720282147\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "#     print(epoch, theta)\n",
    "\n",
    "slope,intercept = theta\n",
    "print(slope)\n",
    "print(intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = [random.uniform(-1, 1), random.uniform(-1, 1),random.uniform(-1, 1)]\n",
    "# # pick a random starting point\n",
    "# v = [random.uniform(-1, 1) for i in range(3)]\n",
    "# # print(v)\n",
    "\n",
    "# for epoch in range(5000):\n",
    "#     grad = sum_of_squares_gradient(theta)    # compute the gradient at v\n",
    "#     theta = gradient_step(theta, grad, -0.001)    # take a negative gradient step\n",
    "# #     print(epoch, v)\n",
    "# print(v)\n",
    "\n",
    "# # for epoch in range(100):\n",
    "# #     for i in range(len(inputs)):\n",
    "# #         grad = linear_gradient(inputs[i][0], inputs[i][1], theta)\n",
    "# #         theta = gradient_step(theta, grad, -learning_rate)\n",
    "# # #     print(epoch, theta)\n",
    "\n",
    "# # slope,intercept = theta\n",
    "# # print(slope)\n",
    "# # print(intercept)\n",
    "# # inputs[0][2]\n",
    "# # print(grad)\n",
    "# # print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = [random.uniform(-1, 1), random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# n = 1000\n",
    "# x = np.random.uniform(-2.0, 2.0, n)\n",
    "# y = 2.0 * x * x + 1.23456 * x + 4.5678 + np.random.normal(0.0, 1.0, n)\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     for i in range(len(inputs)):\n",
    "#         m, slope, intercept = theta \n",
    "#         predicted = m*np.square(inputs[i][0]) + slope * inputs[i][0] + intercept    # The prediction of the model.\n",
    "#         error = (inputs[i][1] - predicted)              # error is (predicted - actual).\n",
    "#         squared_error = error ** 2           # We'll minimize squared error \n",
    "#         grad = [-2 * error * inputs[i][0]**2,-2 * error * inputs[i][0], -2 * error]    # using its gradient.\n",
    "        \n",
    "# #         theta = gradient_step(theta, grad, -learning_rate)\n",
    "        \n",
    "# # slope,intercept = theta\n",
    "# # print(slope)\n",
    "# # print(intercept)\n",
    "# grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, -50]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_in = []\n",
    "y_in = []\n",
    "for i in range(len(inputs)):\n",
    "    x_in.append([1] + [inputs[i][0]])\n",
    "#     x_in.append([1] + [inputs[i][0]])\n",
    "    y_in.append(inputs[i][1])\n",
    "x_in[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "least squares fit: 100%|██████████████████████████████████████████████████████████| 5000/5000 [00:06<00:00, 757.49it/s]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "beta = least_squares_fit(x_in, y_in, learning_rate, 5000, 25)\n",
    "# assert 30.50 < beta[0] < 30.70  # constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.93726479, 20.00027134])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
