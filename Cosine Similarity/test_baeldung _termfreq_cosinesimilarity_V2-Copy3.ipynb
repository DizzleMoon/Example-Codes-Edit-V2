{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=np.inf)\n",
    "np.set_printoptions(precision=7)\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import pandas as pd\n",
    "from numpy import linalg as LA\n",
    "from sympy import * \n",
    "from collections import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences\n",
    "# Sentence_1 = 'We went to the pizza place and you ate no pizza at all.'\n",
    "# Sentence_2 = 'I ate pizza with you yesterday at home.'\n",
    "# Sentence_3 = 'Thereâ€™s no place like home'\n",
    "\n",
    "# Sentence_1 = 'The sky is blue'\n",
    "# Sentence_2 = 'The sky is not blue'\n",
    "\n",
    "Sentence_1 = 'The sun is the largest celestial body in the solar system'\n",
    "Sentence_2 = 'The solar system consists of the sun and eight revolving planets'\n",
    "Sentence_3 = 'Ra was the Egyptian Sun God'\n",
    "Sentence_4 = 'The Pyramids were the pinnacle of Egyptian architecture'\n",
    "Sentence_5 = 'The quick brown fox jumps over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 2, 'The': 1, 'sun': 1, 'is': 1, 'largest': 1, 'celestial': 1, 'body': 1, 'in': 1, 'solar': 1, 'system': 1})\n",
      "Counter({'Ra': 1, 'was': 1, 'the': 1, 'Egyptian': 1, 'Sun': 1, 'God': 1})\n",
      "Counter({'Ra': 1, 'was': 1, 'the': 1, 'Egyptian': 1, 'Sun': 1, 'God': 1})\n",
      "Counter({'The': 1, 'quick': 1, 'brown': 1, 'fox': 1, 'jumps': 1, 'over': 1, 'the': 1, 'lazy': 1, 'dog': 1})\n",
      "\n",
      "\n",
      "[1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Count\n",
    "Sen_1 = Sentence_1.split()\n",
    "# print(Sen_1)\n",
    "Sen_2 = Sentence_2.split()\n",
    "# print(Sen_2)\n",
    "Sen_3 = Sentence_3.split()\n",
    "# print(Sen_3)\n",
    "Sen_4 = Sentence_4.split()\n",
    "# print(Sen_3)\n",
    "Sen_5 = Sentence_5.split()\n",
    "# print(Sen_3)\n",
    "\n",
    "# Dictionary\n",
    "Sent_1 = Counter(Sen_1)\n",
    "print(Sent_1)\n",
    "Sent_2 = Counter(Sen_2)\n",
    "# print(Sent_2)\n",
    "Sent_3 = Counter(Sen_3)\n",
    "print(Sent_3)\n",
    "Sent_4 = Counter(Sen_4)\n",
    "print(Sent_3)\n",
    "Sent_5 = Counter(Sen_5)\n",
    "print(Sent_5)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "count = []\n",
    "for i in Sen_1:\n",
    "    cnt = Sen_1.count(i)\n",
    "    count.append(cnt)\n",
    "    \n",
    "print(count)\n",
    " \n",
    "print(Sent_1['pizza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining vectors for sentence 1 & sentence 2\n",
    "# sentences_1_2 = Sen_1 + Sen_2\n",
    "# print(sentences_1_2)\n",
    "\n",
    "# Dictionary\n",
    "# Sent_1_2 = Counter(sentences_1_2)\n",
    "# print(Sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 2, 'The': 1, 'sun': 1, 'is': 1, 'largest': 1, 'celestial': 1, 'body': 1, 'in': 1, 'solar': 1, 'system': 1})\n",
      "The\n",
      "solar\n",
      "system\n",
      "consists\n",
      "of\n",
      "the\n",
      "sun\n",
      "and\n",
      "eight\n",
      "revolving\n",
      "planets\n",
      "[('The', 0), ('and', 0), ('body', 1), ('celestial', 1), ('consists', 0), ('eight', 0), ('in', 1), ('is', 1), ('largest', 1), ('of', 0), ('planets', 0), ('revolving', 0), ('solar', 0), ('sun', 0), ('system', 0), ('the', 0)]\n",
      "[0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# doc_1 = Sent_1.keys()\n",
    "# print(doc_1)\n",
    "\n",
    "# for i in doc_1:\n",
    "#     print(i)\n",
    "\n",
    "doc_1 = Sent_1.copy()\n",
    "print(Sent_1)\n",
    "\n",
    "for key,value in Sent_2.items():\n",
    "    akey = str(key)\n",
    "#     print(key)\n",
    "    if akey not in zip(Sent_1.keys(),Sent_3.keys(),Sent_4.keys()):\n",
    "        print(key)\n",
    "        doc_1[key] = 0\n",
    "#         S1 = Sent_1[str(key)] + Sent_2[str(key)]\n",
    "#         print(S1)\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "\n",
    "# print(doc_1)\n",
    "print(sorted(doc_1.items()))\n",
    "\n",
    "doc_1_val = []\n",
    "for key,val in sorted(doc_1.items()):\n",
    "    doc_1_val.append(val)\n",
    "    \n",
    "print(doc_1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'The': 1, 'solar': 1, 'system': 1, 'consists': 1, 'of': 1, 'the': 1, 'sun': 1, 'and': 1, 'eight': 1, 'revolving': 1, 'planets': 1})\n",
      "The\n",
      "sun\n",
      "is\n",
      "the\n",
      "largest\n",
      "celestial\n",
      "body\n",
      "in\n",
      "solar\n",
      "system\n",
      "[('The', 0), ('and', 1), ('body', 0), ('celestial', 0), ('consists', 1), ('eight', 1), ('in', 0), ('is', 0), ('largest', 0), ('of', 1), ('planets', 1), ('revolving', 1), ('solar', 0), ('sun', 0), ('system', 0), ('the', 0)]\n",
      "[0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "doc_2 = Sent_2.copy()\n",
    "print(doc_2)\n",
    "\n",
    "for key,value in Sent_1.items():\n",
    "    akey = str(key)\n",
    "#     print(key)\n",
    "    if akey not in zip(Sent_2.keys(),Sent_3.keys(),Sent_4.keys(),Sent_5.keys()):\n",
    "        print(key)\n",
    "        doc_2[key] = 0\n",
    "#         S1 = Sent_1[str(key)] + Sent_2[str(key)]\n",
    "#         print(S1)\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "\n",
    "print(sorted(doc_2.items()))\n",
    "\n",
    "doc_2_val = []\n",
    "for key,val in sorted(doc_2.items()):\n",
    "    doc_2_val.append(val)\n",
    "    \n",
    "print(doc_2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Ra': 1, 'was': 1, 'the': 1, 'Egyptian': 1, 'Sun': 1, 'God': 1})\n",
      "Ra\n",
      "was\n",
      "the\n",
      "Egyptian\n",
      "Sun\n",
      "God\n",
      "[('Egyptian', 0), ('God', 0), ('Ra', 0), ('Sun', 0), ('the', 0), ('was', 0)]\n",
      "[0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "doc_3 = Sent_3.copy()\n",
    "print(doc_3)\n",
    "\n",
    "for key,value in Sent_3.items():\n",
    "    akey = str(key)\n",
    "#     print(key)\n",
    "    if akey not in zip(Sent_1.keys(),Sent_2.keys(),Sent_3.keys(),Sent_4.keys(),Sent_5.keys()):\n",
    "        print(key)\n",
    "        doc_3[key] = 0\n",
    "#         S1 = Sent_1[str(key)] + Sent_2[str(key)]\n",
    "#         print(S1)\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "\n",
    "print(sorted(doc_3.items()))\n",
    "\n",
    "doc_3_val = []\n",
    "for key,val in sorted(doc_3.items()):\n",
    "    doc_3_val.append(val)\n",
    "    \n",
    "print(doc_3_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'The': 1, 'Pyramids': 1, 'were': 1, 'the': 1, 'pinnacle': 1, 'of': 1, 'Egyptian': 1, 'architecture': 1})\n",
      "The\n",
      "Pyramids\n",
      "were\n",
      "the\n",
      "pinnacle\n",
      "of\n",
      "Egyptian\n",
      "architecture\n",
      "[('Egyptian', 0), ('Pyramids', 0), ('The', 0), ('architecture', 0), ('of', 0), ('pinnacle', 0), ('the', 0), ('were', 0)]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "doc_4 = Sent_4.copy()\n",
    "print(doc_4)\n",
    "\n",
    "for key,value in Sent_4.items():\n",
    "    akey = str(key)\n",
    "#     print(key)\n",
    "    if akey not in zip(Sent_1.keys(),Sent_2.keys(),Sent_3.keys(),Sent_5.keys()):\n",
    "        print(key)\n",
    "        doc_4[key] = 0\n",
    "#         S1 = Sent_1[str(key)] + Sent_2[str(key)]\n",
    "#         print(S1)\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "\n",
    "print(sorted(doc_4.items()))\n",
    "\n",
    "doc_4_val = []\n",
    "for key,val in sorted(doc_4.items()):\n",
    "    doc_4_val.append(val)\n",
    "    \n",
    "print(doc_4_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'The': 1, 'quick': 1, 'brown': 1, 'fox': 1, 'jumps': 1, 'over': 1, 'the': 1, 'lazy': 1, 'dog': 1})\n",
      "The\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jumps\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n",
      "[('The', 0), ('brown', 0), ('dog', 0), ('fox', 0), ('jumps', 0), ('lazy', 0), ('over', 0), ('quick', 0), ('the', 0)]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "doc_5 = Sent_5.copy()\n",
    "print(doc_5)\n",
    "\n",
    "for key,value in Sent_5.items():\n",
    "    akey = str(key)\n",
    "#     print(key)\n",
    "    if akey not in zip(Sent_1.keys(),Sent_2.keys(),Sent_3.keys(),Sent_4.keys()):\n",
    "        print(key)\n",
    "        doc_5[key] = 0\n",
    "#         S1 = Sent_1[str(key)] + Sent_2[str(key)]\n",
    "#         print(S1)\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "\n",
    "print(sorted(doc_5.items()))\n",
    "\n",
    "doc_5_val = []\n",
    "for key,val in sorted(doc_5.items()):\n",
    "    doc_5_val.append(val)\n",
    "    \n",
    "print(doc_5_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "(16,)\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "doc_1_val = np.array(doc_1_val)\n",
    "doc_2_val = np.array(doc_2_val)\n",
    "top = doc_1_val.dot(doc_2_val)\n",
    "bottom = np.linalg.norm(doc_1_val) * np.linalg.norm(doc_2_val)\n",
    "cos1 = top/bottom\n",
    "print(cos1)\n",
    "print(doc_1_val.shape)\n",
    "print(len(doc_1_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, inf, inf, inf, inf]\n",
      "[inf, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, inf, inf, inf, inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_30912/2807206980.py:19: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  idf = np.log10(2/df)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_30912/2807206980.py:27: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  idf = np.log10(2/df)\n"
     ]
    }
   ],
   "source": [
    "# IDF\n",
    "# Corpus size\n",
    "Corpus = 2\n",
    "\n",
    "# Document Frequency V1\n",
    "doc = np.concatenate((doc_1_val, doc_2_val))\n",
    "# print(doc)\n",
    "# print(len(doc))\n",
    "doc_2d = doc.reshape(2,int(len(doc)/2))\n",
    "# print(doc_2d[0][4])\n",
    "idf_1 = []\n",
    "for i in range(len(doc_1_val)):\n",
    "    df = 0\n",
    "    for j in range(2):\n",
    "#         print(doc_2d[i][j])\n",
    "        df = df + doc_2d[j][i]\n",
    "#         print(df)\n",
    "        \n",
    "    idf = np.log10(2/df)\n",
    "    idf_1.append(idf)\n",
    "print(idf_1)  \n",
    "\n",
    "idf_2 = []\n",
    "# Document Frequency V2\n",
    "for i in zip(doc_1_val, doc_2_val):\n",
    "    df = sum(i)\n",
    "    idf = np.log10(2/df)\n",
    "    idf_2.append(idf)\n",
    "print(idf_2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.0, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.0, 0.0, 0.0, nan, nan, nan, nan]\n",
      "[nan, 0.3010299956639812, 0.0, 0.0, 0.3010299956639812, 0.3010299956639812, 0.0, 0.0, 0.0, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, nan, nan, nan, nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_30912/2533052209.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  tf_idf = doc_1_val[i] * idf_2[i]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_30912/2533052209.py:12: RuntimeWarning: invalid value encountered in multiply\n",
      "  tf_idf = doc_2_val[i] * idf_2[i]\n"
     ]
    }
   ],
   "source": [
    "# tf_idf\n",
    "# Document 1\n",
    "tf_idf_1 = []\n",
    "for i in range(len(idf_2)):\n",
    "    tf_idf = doc_1_val[i] * idf_2[i]\n",
    "    tf_idf_1.append(tf_idf)\n",
    "print(tf_idf_1)\n",
    "\n",
    "# Document 2\n",
    "tf_idf_2 = []\n",
    "for i in range(len(idf_2)):\n",
    "    tf_idf = doc_2_val[i] * idf_2[i]\n",
    "    tf_idf_2.append(tf_idf)\n",
    "print(tf_idf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "tf_idf_1 = np.array(tf_idf_1)\n",
    "tf_idf_2 = np.array(tf_idf_2)\n",
    "top = tf_idf_1.dot(tf_idf_2)\n",
    "bottom = np.linalg.norm(tf_idf_1) * np.linalg.norm(tf_idf_2)\n",
    "cos1 = top/(bottom + 1e-20)\n",
    "print(cos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The sun is the largest celestial body in the solar system', 'The solar system consists of the sun and eight revolving planets', 'Ra was the Egyptian Sun God', 'The Pyramids were the pinnacle of Egyptian architecture', 'The quick brown fox jumps over the lazy dog']\n"
     ]
    }
   ],
   "source": [
    "Corpus = [Sentence_1,Sentence_2,Sentence_3,Sentence_4,Sentence_5]\n",
    "print(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer()\n",
      "  (0, 26)\t0.2720650001570024\n",
      "  (0, 24)\t0.2720650001570024\n",
      "  (0, 11)\t0.33721755509592094\n",
      "  (0, 2)\t0.33721755509592094\n",
      "  (0, 4)\t0.33721755509592094\n",
      "  (0, 14)\t0.33721755509592094\n",
      "  (0, 12)\t0.33721755509592094\n",
      "  (0, 25)\t0.2258385267674438\n",
      "  (0, 27)\t0.4820579154855759\n",
      "  (1, 19)\t0.3469067733890991\n",
      "  (1, 23)\t0.3469067733890991\n",
      "  (1, 8)\t0.3469067733890991\n",
      "  (1, 0)\t0.3469067733890991\n",
      "  (1, 16)\t0.27988220046765916\n",
      "  (1, 5)\t0.3469067733890991\n",
      "  (1, 26)\t0.27988220046765916\n",
      "  (1, 24)\t0.27988220046765916\n",
      "  (1, 25)\t0.23232750918188863\n",
      "  (1, 27)\t0.3306058725208975\n",
      "  (2, 10)\t0.48076438934193244\n",
      "  (2, 7)\t0.3878776821823183\n",
      "  (2, 28)\t0.48076438934193244\n",
      "  (2, 22)\t0.48076438934193244\n",
      "  (2, 25)\t0.321973514636116\n",
      "  (2, 27)\t0.22908680747650192\n",
      "  (3, 1)\t0.40128418717786946\n",
      "  (3, 18)\t0.40128418717786946\n",
      "  (3, 29)\t0.40128418717786946\n",
      "  (3, 20)\t0.40128418717786946\n",
      "  (3, 7)\t0.3237535555244\n",
      "  (3, 16)\t0.3237535555244\n",
      "  (3, 27)\t0.38242813057436686\n",
      "  (4, 6)\t0.3555988681343606\n",
      "  (4, 15)\t0.3555988681343606\n",
      "  (4, 17)\t0.3555988681343606\n",
      "  (4, 13)\t0.3555988681343606\n",
      "  (4, 9)\t0.3555988681343606\n",
      "  (4, 3)\t0.3555988681343606\n",
      "  (4, 21)\t0.3555988681343606\n",
      "  (4, 27)\t0.33888953195832316\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "print(tfidf_vectorizer)\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(Corpus)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 2, 'The': 1, 'sun': 1, 'is': 1, 'largest': 1, 'celestial': 1, 'body': 1, 'in': 1, 'solar': 1, 'system': 1})\n",
      "[('The', 1), ('body', 1), ('celestial', 1), ('in', 1), ('is', 1), ('largest', 1), ('solar', 1), ('sun', 1), ('system', 1), ('the', 2)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# doc_1 = Sent_1.keys()\n",
    "# print(doc_1)\n",
    "\n",
    "# for i in doc_1:\n",
    "#     print(i)\n",
    "\n",
    "doc_1 = Sent_1.copy()\n",
    "print(Sent_1)\n",
    "\n",
    "# for key,value in Sent_2.items():\n",
    "#     akey = str(key)\n",
    "# #     print(key)\n",
    "#     if akey not in Sent_1.keys():\n",
    "#         print(key)\n",
    "#         doc_1[key] = 0\n",
    "# #         S1 = Sent_1[str(key)] + Sent_2[str(key)]\n",
    "# #         print(S1)\n",
    "# #     print(key)\n",
    "# #     print(value)\n",
    "\n",
    "# print(doc_1)\n",
    "print(sorted(doc_1.items()))\n",
    "\n",
    "doc_1_val = []\n",
    "for key,val in sorted(doc_1.items()):\n",
    "    doc_1_val.append(val)\n",
    "    \n",
    "print(doc_1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'The': 1, 'solar': 1, 'system': 1, 'consists': 1, 'of': 1, 'the': 1, 'sun': 1, 'and': 1, 'eight': 1, 'revolving': 1, 'planets': 1})\n",
      "[('The', 1), ('and', 1), ('consists', 1), ('eight', 1), ('of', 1), ('planets', 1), ('revolving', 1), ('solar', 1), ('sun', 1), ('system', 1), ('the', 1)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# doc_1 = Sent_1.keys()\n",
    "# print(doc_1)\n",
    "\n",
    "# for i in doc_1:\n",
    "#     print(i)\n",
    "\n",
    "doc_2 = Sent_2.copy()\n",
    "print(Sent_2)\n",
    "\n",
    "# for key,value in Sent_2.items():\n",
    "#     akey = str(key)\n",
    "# #     print(key)\n",
    "#     if akey not in Sent_1.keys():\n",
    "#         print(key)\n",
    "#         doc_1[key] = 0\n",
    "# #         S1 = Sent_1[str(key)] + Sent_2[str(key)]\n",
    "# #         print(S1)\n",
    "# #     print(key)\n",
    "# #     print(value)\n",
    "\n",
    "# print(doc_1)\n",
    "print(sorted(doc_2.items()))\n",
    "\n",
    "doc_2_val = []\n",
    "for key,val in sorted(doc_2.items()):\n",
    "    doc_2_val.append(val)\n",
    "    \n",
    "print(doc_2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Ra': 1, 'was': 1, 'the': 1, 'Egyptian': 1, 'Sun': 1, 'God': 1})\n",
      "[('Egyptian', 1), ('God', 1), ('Ra', 1), ('Sun', 1), ('the', 1), ('was', 1)]\n",
      "[1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# doc_1 = Sent_1.keys()\n",
    "# print(doc_1)\n",
    "\n",
    "# for i in doc_1:\n",
    "#     print(i)\n",
    "\n",
    "doc_3 = Sent_3.copy()\n",
    "print(Sent_3)\n",
    "\n",
    "# for key,value in Sent_2.items():\n",
    "#     akey = str(key)\n",
    "# #     print(key)\n",
    "#     if akey not in Sent_1.keys():\n",
    "#         print(key)\n",
    "#         doc_1[key] = 0\n",
    "# #         S1 = Sent_1[str(key)] + Sent_2[str(key)]\n",
    "# #         print(S1)\n",
    "# #     print(key)\n",
    "# #     print(value)\n",
    "\n",
    "# print(doc_1)\n",
    "print(sorted(doc_3.items()))\n",
    "\n",
    "doc_3_val = []\n",
    "for key,val in sorted(doc_3.items()):\n",
    "    doc_3_val.append(val)\n",
    "    \n",
    "print(doc_3_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'The': 1, 'Pyramids': 1, 'were': 1, 'the': 1, 'pinnacle': 1, 'of': 1, 'Egyptian': 1, 'architecture': 1})\n",
      "[('Egyptian', 1), ('Pyramids', 1), ('The', 1), ('architecture', 1), ('of', 1), ('pinnacle', 1), ('the', 1), ('were', 1)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# doc_1 = Sent_1.keys()\n",
    "# print(doc_1)\n",
    "\n",
    "# for i in doc_1:\n",
    "#     print(i)\n",
    "\n",
    "doc_4 = Sent_4.copy()\n",
    "print(Sent_4)\n",
    "\n",
    "# for key,value in Sent_2.items():\n",
    "#     akey = str(key)\n",
    "# #     print(key)\n",
    "#     if akey not in Sent_1.keys():\n",
    "#         print(key)\n",
    "#         doc_1[key] = 0\n",
    "# #         S1 = Sent_1[str(key)] + Sent_2[str(key)]\n",
    "# #         print(S1)\n",
    "# #     print(key)\n",
    "# #     print(value)\n",
    "\n",
    "# print(doc_1)\n",
    "print(sorted(doc_4.items()))\n",
    "\n",
    "doc_4_val = []\n",
    "for key,val in sorted(doc_4.items()):\n",
    "    doc_4_val.append(val)\n",
    "    \n",
    "print(doc_4_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'The': 1, 'quick': 1, 'brown': 1, 'fox': 1, 'jumps': 1, 'over': 1, 'the': 1, 'lazy': 1, 'dog': 1})\n",
      "[('The', 1), ('brown', 1), ('dog', 1), ('fox', 1), ('jumps', 1), ('lazy', 1), ('over', 1), ('quick', 1), ('the', 1)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# doc_1 = Sent_1.keys()\n",
    "# print(doc_1)\n",
    "\n",
    "# for i in doc_1:\n",
    "#     print(i)\n",
    "\n",
    "doc_5 = Sent_5.copy()\n",
    "print(Sent_5)\n",
    "\n",
    "# for key,value in Sent_2.items():\n",
    "#     akey = str(key)\n",
    "# #     print(key)\n",
    "#     if akey not in Sent_1.keys():\n",
    "#         print(key)\n",
    "#         doc_1[key] = 0\n",
    "# #         S1 = Sent_1[str(key)] + Sent_2[str(key)]\n",
    "# #         print(S1)\n",
    "# #     print(key)\n",
    "# #     print(value)\n",
    "\n",
    "# print(doc_1)\n",
    "print(sorted(doc_5.items()))\n",
    "\n",
    "doc_5_val = []\n",
    "for key,val in sorted(doc_5.items()):\n",
    "    doc_5_val.append(val)\n",
    "    \n",
    "print(doc_5_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(2, 1)\n",
      "[inf, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, 0.3010299956639812, inf, inf, inf, inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.17609125905568127]\n"
     ]
    }
   ],
   "source": [
    "# Document Frequency \n",
    "for i in zip(doc_1_val, doc_2_val):\n",
    "    print(i)\n",
    "    df = sum(i)\n",
    "    idf = np.log10(2/df)\n",
    "    idf_2.append(idf)\n",
    "print(idf_2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer()\n",
      "  (0, 26)\t0.2720650001570024\n",
      "  (0, 24)\t0.2720650001570024\n",
      "  (0, 11)\t0.33721755509592094\n",
      "  (0, 2)\t0.33721755509592094\n",
      "  (0, 4)\t0.33721755509592094\n",
      "  (0, 14)\t0.33721755509592094\n",
      "  (0, 12)\t0.33721755509592094\n",
      "  (0, 25)\t0.2258385267674438\n",
      "  (0, 27)\t0.4820579154855759\n",
      "  (1, 19)\t0.3469067733890991\n",
      "  (1, 23)\t0.3469067733890991\n",
      "  (1, 8)\t0.3469067733890991\n",
      "  (1, 0)\t0.3469067733890991\n",
      "  (1, 16)\t0.27988220046765916\n",
      "  (1, 5)\t0.3469067733890991\n",
      "  (1, 26)\t0.27988220046765916\n",
      "  (1, 24)\t0.27988220046765916\n",
      "  (1, 25)\t0.23232750918188863\n",
      "  (1, 27)\t0.3306058725208975\n",
      "  (2, 10)\t0.48076438934193244\n",
      "  (2, 7)\t0.3878776821823183\n",
      "  (2, 28)\t0.48076438934193244\n",
      "  (2, 22)\t0.48076438934193244\n",
      "  (2, 25)\t0.321973514636116\n",
      "  (2, 27)\t0.22908680747650192\n",
      "  (3, 1)\t0.40128418717786946\n",
      "  (3, 18)\t0.40128418717786946\n",
      "  (3, 29)\t0.40128418717786946\n",
      "  (3, 20)\t0.40128418717786946\n",
      "  (3, 7)\t0.3237535555244\n",
      "  (3, 16)\t0.3237535555244\n",
      "  (3, 27)\t0.38242813057436686\n",
      "  (4, 6)\t0.3555988681343606\n",
      "  (4, 15)\t0.3555988681343606\n",
      "  (4, 17)\t0.3555988681343606\n",
      "  (4, 13)\t0.3555988681343606\n",
      "  (4, 9)\t0.3555988681343606\n",
      "  (4, 3)\t0.3555988681343606\n",
      "  (4, 21)\t0.3555988681343606\n",
      "  (4, 27)\t0.33888953195832316\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "print(tfidf_vectorizer)\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(Corpus)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
