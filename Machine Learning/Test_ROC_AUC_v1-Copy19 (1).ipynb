{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "# import pygal\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import urllib.request\n",
    "import requests\n",
    "import curl\n",
    "import pycurl\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from IPython import qt\n",
    "from matplotlib.pyplot import figure\n",
    "from py.xml import raw\n",
    "from requests.api import get\n",
    "from matplotlib import pyplot as plt\n",
    "# from scratch.working_with_data import rescale\n",
    "# from scratch.multiple_regression import least_squares_fit, predict\n",
    "# from scratch.gradient_descent import gradient_step\n",
    "\n",
    "# from stats import mean, median, de_mean, standard_deviation, correlation\n",
    "# from gradient_descent import minimize_stochastic, maximize_stochastic, maximize_batch\n",
    "# from vector import dot, vector_add\n",
    "# from normal import normal_cdf\n",
    "# from matrix import make_matrix, get_column, shape, matrix_multiply\n",
    "# from logistic_regression import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "\n",
    "from scipy.optimize import fmin_tnc\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from typing import*\n",
    "\n",
    "from collections import*\n",
    "# from scipy import*\n",
    "from sklearn.metrics import*\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "# import mnist\n",
    "# bltin_sum = np.sum\n",
    "\n",
    "import random\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn import*\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# def add(a, b): return a + b\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "Tensor = list\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c , v):\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "#     return np.sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "#     gen = \n",
    "    return np.sum(np.fromiter((v_i * w_i for v_i, w_i in zip(v, w)),float))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = data.shape[0]\n",
    "    \n",
    "    # Vector Mean\n",
    "#     n = len(data)\n",
    "#     m = np.sum(data,axis=0)\n",
    "#     means = np.multiply(1/n,m)\n",
    "    means = vector_mean(data)\n",
    "    \n",
    "    # Standard Deviaiton\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "    return means,stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = data.shape[0]\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    grad_step = np.add(v,step)\n",
    "    return grad_step\n",
    "\n",
    "# def predict(alpha, beta, x_i):\n",
    "#     pred = beta * x_i + alpha\n",
    "#     return pred\n",
    "\n",
    "# def error(x, y, beta):\n",
    "#     \"\"\"\n",
    "#     The error from predicting beta * x_i + alpha\n",
    "#     when the actual value is y_i\n",
    "#     \"\"\"\n",
    "#     err_fin = predict(alpha, beta, x_i) - y_i\n",
    "#     return err_fin\n",
    "\n",
    "def predict(x, beta):\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x, y, beta):\n",
    "    return predict(x, beta) - y \n",
    "\n",
    "def sqerror_gradient(x, y, beta):\n",
    "    err = error(x, y, beta)\n",
    "    err_fin = [2 * err * x_i for x_i in x]\n",
    "    return err_fin\n",
    "\n",
    "def least_squares_fit(xs, ys, learning_rate = 0.001, num_steps = 1000, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [np.random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def _negative_log_likelihood(x, y, beta):\n",
    "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))\n",
    "    \n",
    "def negative_log_likelihood(xs, ys, beta):\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))\n",
    "\n",
    "def _negative_log_partial_j(x, y, beta, j):\n",
    "    \"\"\"\n",
    "    The jth partial derivative for one data point.\n",
    "    Here i is the index of the data point.\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x, y, beta):\n",
    "    \"\"\"\n",
    "    The gradient for one data point.\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs, ys,beta):\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "                                                                \n",
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def sigmoid(t: float) -> float: \n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "# Gradient Descent - step\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "def squared_distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\"\n",
    "    return sum_of_squares(subtract(v, w))\n",
    "\n",
    "def distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes the distance between v and w\"\"\"\n",
    "    return math.sqrt(squared_distance(v, w))\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "\n",
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor.\n",
    "    Otherwise, tensor is 1-dimensional (that is, a vector).\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)  # just a list of floats, use Python sum\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
    "                   for tensor_i in tensor)   # and sum up those results.\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f elementwise\"\"\" \n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor] \n",
    "    \n",
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "\n",
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                   t1: Tensor,\n",
    "                   t2: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]\n",
    "    \n",
    "def inverse_normal_cdf(p: float,\n",
    "                       mu: float = 0,\n",
    "                       sigma: float = 1,\n",
    "                       tolerance: float = 0.00001) -> float:\n",
    "    \"\"\"Find approximate inverse using binary search\"\"\"\n",
    "\n",
    "    # if not standard, compute standard and rescale\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "\n",
    "    low_z = -10.0                      # normal_cdf(-10) is (very close to) 0\n",
    "    hi_z  =  10.0                      # normal_cdf(10)  is (very close to) 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2     # Consider the midpoint\n",
    "        mid_p = normal_cdf(mid_z)      # and the CDF's value there\n",
    "        if mid_p < p:\n",
    "            low_z = mid_z              # Midpoint too low, search above it\n",
    "        else:\n",
    "            hi_z = mid_z               # Midpoint too high, search below it\n",
    "\n",
    "    return mid_z\n",
    "\n",
    "def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -> float:\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "\n",
    "def num_differences(v1, v2):\n",
    "    assert len(v1) == len(v2)\n",
    "    return len([x1 for x1, x2 in zip(v1, v2) if x1 != x2])\n",
    "\n",
    "\n",
    "def cluster_means(k: int,\n",
    "                  inputs: List[Vector],\n",
    "                  assignments: List[int]) -> List[Vector]:\n",
    "    # clusters[i] contains the inputs whose assignment is i\n",
    "    clusters = [[] for i in range(k)]\n",
    "    for input, assignment in zip(inputs, assignments):\n",
    "        clusters[assignment].append(input)\n",
    "\n",
    "    # if a cluster is empty, just use a random point\n",
    "    return [vector_mean(cluster) if cluster else random.choice(inputs)\n",
    "            for cluster in clusters]\n",
    "\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k: int) -> None:\n",
    "        self.k = k                      # number of clusters\n",
    "        self.means = None\n",
    "\n",
    "    def classify(self, input: Vector) -> int:\n",
    "        \"\"\"return the index of the cluster closest to the input\"\"\"\n",
    "        return min(range(self.k),\n",
    "                   key=lambda i: squared_distance(input, self.means[i]))\n",
    "\n",
    "    def train(self, inputs: List[Vector]) -> None:\n",
    "        # Start with random assignments\n",
    "        assignments = [random.randrange(self.k) for _ in inputs]\n",
    "\n",
    "        with tqdm.tqdm(itertools.count()) as t:\n",
    "            for _ in t:\n",
    "                # Compute means and find new assignments\n",
    "                self.means = cluster_means(self.k, inputs, assignments)\n",
    "                new_assignments = [self.classify(input) for input in inputs]\n",
    "\n",
    "                # Check how many assignments changed and if we're done\n",
    "                num_changed = num_differences(assignments, new_assignments)\n",
    "                if num_changed == 0:\n",
    "                    return\n",
    "\n",
    "                # Otherwise keep the new assignments, and compute new means\n",
    "                assignments = new_assignments\n",
    "                self.means = cluster_means(self.k, inputs, assignments)\n",
    "                t.set_description(f\"changed: {num_changed} / {len(inputs)}\")\n",
    "                \n",
    "def squared_clustering_errors(inputs, k):\n",
    "    \"\"\"finds the total squared error from k-means clustering the\n",
    "inputs\"\"\"\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(inputs)\n",
    "    means = clusterer.means\n",
    "    assignments = [clusterer.classify(input) for input in inputs]\n",
    "\n",
    "    return sum(squared_distance(input, means[cluster])\n",
    "               for input, cluster in zip(inputs, assignments))\n",
    "\n",
    "def recolor(pixel):\n",
    "    cluster = clusterer.classify(pixel)        # index of the closest cluster\n",
    "    return clusterer.means[cluster]            # mean of the closest cluster\n",
    "\n",
    "def get_values(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        return [cluster.value]\n",
    "    else:\n",
    "        return [value\n",
    "                for child in cluster.children\n",
    "                for value in get_values(child)]\n",
    "\n",
    "\n",
    "def cluster_distance(cluster1,\n",
    "                     cluster2,\n",
    "                     distance_agg: Callable = min):\n",
    "    \"\"\"\n",
    "    compute all the pairwise distances between cluster1 and cluster2\n",
    "    and apply the aggregation function _distance_agg_ to the resulting\n",
    "list\n",
    "    \"\"\"\n",
    "    return distance_agg([distance(v1, v2) for v1 in get_values(cluster1) for v2 in get_values(cluster2)])\n",
    "\n",
    "def get_merge_order(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        return float('inf')  # was never merged\n",
    "    else:\n",
    "        return cluster.order\n",
    "    \n",
    "def get_children(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        raise TypeError(\"Leaf has no children\")\n",
    "    else:\n",
    "        return cluster.children\n",
    "    \n",
    "class Merged(NamedTuple):\n",
    "    children: tuple\n",
    "    order: int\n",
    "\n",
    "def bottom_up_cluster(inputs: List[Vector],\n",
    "                      distance_agg: Callable = min):\n",
    "    # Start with all leaves\n",
    "    clusters: List[Cluster] = [Leaf(input) for input in inputs]\n",
    "\n",
    "    def pair_distance(pair):\n",
    "        return cluster_distance(pair[0], pair[1], distance_agg)\n",
    " \n",
    "    # as long as we have more than one cluster left...\n",
    "    while len(clusters) > 1:\n",
    "        # find the two closest clusters\n",
    "        c1, c2 = min(((cluster1, cluster2) \n",
    "                      for i, cluster1 in enumerate(clusters)\n",
    "                      for cluster2 in clusters[:i]),\n",
    "                      key=pair_distance)\n",
    "\n",
    "        # remove them from the list of clusters\n",
    "        clusters = [c for c in clusters if c != c1 and c != c2]\n",
    "\n",
    "        # merge them, using merge_order = # of clusters left\n",
    "        merged_cluster = Merged((c1, c2), order=len(clusters))\n",
    "\n",
    "        # and add their merge\n",
    "        clusters.append(merged_cluster)\n",
    "\n",
    "    # when there's only one cluster left, return it\n",
    "    return clusters[0]\n",
    "\n",
    "def generate_clusters(base_cluster,\n",
    "                      num_clusters):\n",
    "    # start with a list with just the base cluster\n",
    "    clusters = [base_cluster]\n",
    "\n",
    "    # as long as we don't have enough clusters yet...\n",
    "    while len(clusters) < num_clusters:\n",
    "        # choose the last-merged of our clusters\n",
    "        next_cluster = min(clusters, key=get_merge_order)\n",
    "        # remove it from the list\n",
    "        clusters = [c for c in clusters if c != next_cluster]\n",
    "\n",
    "        # and add its children to the list (i.e., unmerge it)\n",
    "        clusters.extend(get_children(next_cluster))\n",
    "\n",
    "    # once we have enough clusters...\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "# Split data into train-test pools \n",
    "train, test, train_labels, test_labels = train_test_split(dataset['data'],\n",
    "                                                          dataset['target'],\n",
    "                                                          test_size=0.33)\n",
    "\n",
    "# Train model \n",
    "logregr = LogisticRegression().fit(train, train_labels)\n",
    "\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0008934907970151996,\n",
       " 0.1613672826717624,\n",
       " 1.2554870782066046e-09,\n",
       " 0.9953343746429169,\n",
       " 0.9999281403734658,\n",
       " 0.9997150674177936,\n",
       " 0.9834436919025757,\n",
       " 0.977796481006465,\n",
       " 0.999827566289287,\n",
       " 0.024089733397896634,\n",
       " 0.989412746811127,\n",
       " 0.01724621922147592,\n",
       " 1.8163255607797447e-17,\n",
       " 9.400618420014114e-14,\n",
       " 0.8663627078802434,\n",
       " 0.3117313621556746,\n",
       " 0.9996828602152869,\n",
       " 0.998621147851376,\n",
       " 0.9981330947117535,\n",
       " 0.004476184955668436,\n",
       " 0.210429693528803,\n",
       " 0.9850770204898834,\n",
       " 7.39306835216486e-06,\n",
       " 4.949528512342613e-20,\n",
       " 0.998280605637253,\n",
       " 0.45633534111735014,\n",
       " 0.008497416659162167,\n",
       " 0.9939640581317609,\n",
       " 0.9811278997755752,\n",
       " 1.4485553641295013e-09,\n",
       " 0.9999240919225765,\n",
       " 0.5250184265779444,\n",
       " 0.0001004050629665974,\n",
       " 8.511884206718271e-09,\n",
       " 0.8086714481286755,\n",
       " 0.9949313777266551,\n",
       " 0.019332108362881906,\n",
       " 0.6564426788957639,\n",
       " 0.9997586719371819,\n",
       " 2.4024482814815906e-07,\n",
       " 0.9984144898295064,\n",
       " 0.23662891695451074,\n",
       " 0.9982653190732745,\n",
       " 0.9969440114585392,\n",
       " 2.896470005438212e-07,\n",
       " 0.996924884450119,\n",
       " 0.575502651010292,\n",
       " 0.9995985206980276,\n",
       " 0.9897887702206422,\n",
       " 0.9986452817715797,\n",
       " 0.04413808294022675,\n",
       " 0.9981731505439434,\n",
       " 0.0016805549563752246,\n",
       " 0.9977229521543721,\n",
       " 0.9996748749134103,\n",
       " 1.4051546689059285e-09,\n",
       " 0.9882719924536156,\n",
       " 0.9966282022485647,\n",
       " 0.9995353542250501,\n",
       " 2.166442439377831e-07,\n",
       " 0.9919781321473754,\n",
       " 0.9999773525079972,\n",
       " 3.948823693130562e-07,\n",
       " 0.9997187928173519,\n",
       " 0.9989239524738545,\n",
       " 0.9847653557058841,\n",
       " 0.0005887428281008302,\n",
       " 0.9800323420430976,\n",
       " 0.09098036919185064,\n",
       " 0.997086213750896,\n",
       " 0.9883984525918671,\n",
       " 0.979479995723758,\n",
       " 3.695491742638341e-12,\n",
       " 0.9531461987820565,\n",
       " 0.9870035634051674,\n",
       " 0.9954359896464317,\n",
       " 0.9955662045402351,\n",
       " 0.9659055015275553,\n",
       " 0.9984635631691964,\n",
       " 3.0466193982186847e-07,\n",
       " 0.8230005529738845,\n",
       " 0.9997347041679293,\n",
       " 1.1822315973237349e-11,\n",
       " 0.999813269466057,\n",
       " 1.043003176220034e-07,\n",
       " 5.955726867886457e-06,\n",
       " 0.7326102013047825,\n",
       " 0.9999314374826452,\n",
       " 2.3562848444522466e-10,\n",
       " 0.9973065231160257,\n",
       " 0.0034489139739767027,\n",
       " 0.9797454991131017,\n",
       " 0.9994205553195009,\n",
       " 0.9997426081275099,\n",
       " 0.9885696949694127,\n",
       " 0.1532446042313933,\n",
       " 0.8554618427076621,\n",
       " 0.99921007284443,\n",
       " 7.474395532333821e-08,\n",
       " 4.937750738517009e-14,\n",
       " 0.9935862505361642,\n",
       " 0.00015575669132388492,\n",
       " 0.9204945808153342,\n",
       " 0.2907531609054477,\n",
       " 0.9545242578297188,\n",
       " 0.999802063976432,\n",
       " 0.995775334689363,\n",
       " 0.9987554663793944,\n",
       " 0.9981917065268304,\n",
       " 0.0018422373515481311,\n",
       " 0.9549232477411345,\n",
       " 0.9998376504501507,\n",
       " 0.37730478734057576,\n",
       " 8.223731315328325e-06,\n",
       " 0.002461518142448561,\n",
       " 0.9848583299633404,\n",
       " 0.999206569004786,\n",
       " 0.8831795526173173,\n",
       " 0.01794921544144584,\n",
       " 0.9999140854552776,\n",
       " 0.07428207421543419,\n",
       " 0.9989461886254339,\n",
       " 4.580230128675644e-09,\n",
       " 0.0024169367174193277,\n",
       " 0.9862678672642783,\n",
       " 0.22869723821253163,\n",
       " 0.9984340875192943,\n",
       " 0.9992060942791618,\n",
       " 0.9920855042110425,\n",
       " 0.9999316125138064,\n",
       " 0.9889220692056928,\n",
       " 0.9996451855724698,\n",
       " 9.499195957072626e-16,\n",
       " 0.9979659090518974,\n",
       " 1.7245113016050804e-05,\n",
       " 0.9999059117269317,\n",
       " 0.9935096276115929,\n",
       " 0.999213730331598,\n",
       " 0.9893321731784569,\n",
       " 0.012264953560549529,\n",
       " 0.8663463644684679,\n",
       " 0.9955920268210944,\n",
       " 1.088290277587604e-06,\n",
       " 6.921947202932142e-13,\n",
       " 0.9495924850796702,\n",
       " 0.000587115545500526,\n",
       " 7.207915526374259e-05,\n",
       " 0.940747729955511,\n",
       " 0.9927340205190327,\n",
       " 1.4259518076613209e-08,\n",
       " 2.346528555891491e-08,\n",
       " 0.9329605368937466,\n",
       " 0.11950997972734309,\n",
       " 0.29684599383624277,\n",
       " 2.8822886837880886e-07,\n",
       " 0.7148002331161336,\n",
       " 1.441683256798688e-11,\n",
       " 0.9852291683384997,\n",
       " 0.9995716149159583,\n",
       " 0.9690394196092648,\n",
       " 0.9996747541014385,\n",
       " 0.9978848464703202,\n",
       " 0.9935790489849543,\n",
       " 2.565802455299871e-14,\n",
       " 0.9975655566012515,\n",
       " 0.9999268946683019,\n",
       " 0.9060965215938657,\n",
       " 0.9956349237816143,\n",
       " 0.9850763266398984,\n",
       " 0.4061600227350402,\n",
       " 0.999975629586925,\n",
       " 0.6900680821124037,\n",
       " 0.0016857050222548779,\n",
       " 0.98965018451842,\n",
       " 0.993292499578704,\n",
       " 0.9911164763676401,\n",
       " 0.8787544217514615,\n",
       " 0.9999590090411105,\n",
       " 0.02728300973304566,\n",
       " 0.9984984658921939,\n",
       " 0.8382736062728207,\n",
       " 0.5528748637551538,\n",
       " 0.5410045635279025,\n",
       " 0.6906758359568261,\n",
       " 0.9986929550495558,\n",
       " 0.6169925069386302,\n",
       " 0.9907439600542195,\n",
       " 0.9973845550659929]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename, listify \n",
    "actuals = list(test_labels)\n",
    "\n",
    "# Predict probablities of test data [0,1]\n",
    "scores = list(logregr.predict_proba(test)[:,1])\n",
    "\n",
    "# Equivalently\n",
    "import math\n",
    "def sigmoid(x): \n",
    "    return 1 / (1 + math.exp(-x))\n",
    "# scores = [sigmoid(logregr.coef_@test_i + logregr.intercept_) for test_i in test]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict binary outcomes (0,1)\n",
    "predictions = list(logregr.predict(test))\n",
    "\n",
    "# Equivalently \n",
    "predictions = [1 if s>0.5 else 0 for s in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.904\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = %.3f\" % (sum([p==a for p, a in zip(predictions, actuals)])/len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "ConfusionMatrix = collections.namedtuple('conf', ['tp','fp','tn','fn']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ConfusionMatrix(actuals, scores, threshold=0.5, positive_label=1):\n",
    "    tp=fp=tn=fn=0\n",
    "    bool_actuals = [act==positive_label for act in actuals]\n",
    "    for truth, score in zip(bool_actuals, scores):\n",
    "        if score > threshold:                      # predicted positive \n",
    "            if truth:                              # actually positive \n",
    "                tp += 1\n",
    "            else:                                  # actually negative              \n",
    "                fp += 1          \n",
    "        else:                                      # predicted negative \n",
    "            if not truth:                          # actually negative \n",
    "                tn += 1                          \n",
    "            else:                                  # actually positive \n",
    "                fn += 1\n",
    "\n",
    "    return ConfusionMatrix(tp, fp, tn, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACC(conf_mtrx):\n",
    "    return (conf_mtrx.tp + conf_mtrx.tn) / (conf_mtrx.fp + conf_mtrx.tn + conf_mtrx.tp + conf_mtrx.fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FPR(conf_mtrx):\n",
    "    return conf_mtrx.fp / (conf_mtrx.fp + conf_mtrx.tn) if (conf_mtrx.fp + conf_mtrx.tn)!=0 else 0\n",
    "def TPR(conf_mtrx):\n",
    "    return conf_mtrx.tp / (conf_mtrx.tp + conf_mtrx.fn) if (conf_mtrx.tp + conf_mtrx.fn)!=0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(actuals, scores, **fxns):\n",
    "    # generate thresholds over score domain \n",
    "    low = min(scores)\n",
    "    high = max(scores)\n",
    "    step = (abs(low) + abs(high)) / 1000\n",
    "    thresholds = np.arange(low-step, high+step, step)\n",
    "\n",
    "    # calculate confusion matrices for all thresholds\n",
    "    confusionMatrices = []\n",
    "    for threshold in thresholds:\n",
    "        confusionMatrices.append(calc_ConfusionMatrix(actuals, scores, threshold))\n",
    "\n",
    "    # apply functions to confusion matrices \n",
    "    results = {fname:list(map(fxn, confusionMatrices)) for fname, fxn in fxns.items()}\n",
    "\n",
    "    results[\"THR\"] = thresholds\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = apply(actuals,  scores, ACC=ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(actuals, scores):\n",
    "    return apply(actuals, scores, FPR=FPR, TPR=TPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 0.5131578947368421,\n",
       " 0.47368421052631576,\n",
       " 0.4473684210526316,\n",
       " 0.4342105263157895,\n",
       " 0.42105263157894735,\n",
       " 0.42105263157894735,\n",
       " 0.42105263157894735,\n",
       " 0.42105263157894735,\n",
       " 0.40789473684210525,\n",
       " 0.40789473684210525,\n",
       " 0.40789473684210525,\n",
       " 0.40789473684210525,\n",
       " 0.39473684210526316,\n",
       " 0.39473684210526316,\n",
       " 0.39473684210526316,\n",
       " 0.39473684210526316,\n",
       " 0.39473684210526316,\n",
       " 0.3815789473684211,\n",
       " 0.3815789473684211,\n",
       " 0.3684210526315789,\n",
       " 0.3684210526315789,\n",
       " 0.3684210526315789,\n",
       " 0.3684210526315789,\n",
       " 0.3684210526315789,\n",
       " 0.35526315789473684,\n",
       " 0.35526315789473684,\n",
       " 0.35526315789473684,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.34210526315789475,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.32894736842105265,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3157894736842105,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.3026315789473684,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.2894736842105263,\n",
       " 0.27631578947368424,\n",
       " 0.27631578947368424,\n",
       " 0.27631578947368424,\n",
       " 0.27631578947368424,\n",
       " 0.27631578947368424,\n",
       " 0.27631578947368424,\n",
       " 0.27631578947368424,\n",
       " 0.27631578947368424,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.2631578947368421,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.23684210526315788,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.2236842105263158,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.21052631578947367,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.19736842105263158,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.18421052631578946,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.17105263157894737,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.15789473684210525,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.14473684210526316,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.13157894736842105,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.10526315789473684,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.09210526315789473,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.07894736842105263,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.06578947368421052,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.05263157894736842,\n",
       " 0.039473684210526314,\n",
       " 0.039473684210526314,\n",
       " 0.039473684210526314,\n",
       " 0.039473684210526314,\n",
       " 0.039473684210526314,\n",
       " 0.039473684210526314,\n",
       " 0.039473684210526314,\n",
       " 0.039473684210526314,\n",
       " 0.039473684210526314,\n",
       " 0.039473684210526314,\n",
       " 0.039473684210526314,\n",
       " 0.039473684210526314,\n",
       " 0.02631578947368421,\n",
       " 0.02631578947368421,\n",
       " 0.02631578947368421,\n",
       " 0.02631578947368421,\n",
       " 0.02631578947368421,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.013157894736842105,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab = ROC(actuals,scores)\n",
    "ab['FPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13b70ee4460>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ab['FPR'],ab['TPR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
