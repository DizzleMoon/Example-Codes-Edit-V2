{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "# import pygal\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import urllib.request\n",
    "import requests\n",
    "import curl\n",
    "import pycurl\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from IPython import qt\n",
    "from matplotlib.pyplot import figure\n",
    "from py.xml import raw\n",
    "from requests.api import get\n",
    "from matplotlib import pyplot as plt\n",
    "# from scratch.working_with_data import rescale\n",
    "# from scratch.multiple_regression import least_squares_fit, predict\n",
    "# from scratch.gradient_descent import gradient_step\n",
    "\n",
    "# from stats import mean, median, de_mean, standard_deviation, correlation\n",
    "# from gradient_descent import minimize_stochastic, maximize_stochastic, maximize_batch\n",
    "# from vector import dot, vector_add\n",
    "# from normal import normal_cdf\n",
    "# from matrix import make_matrix, get_column, shape, matrix_multiply\n",
    "# from logistic_regression import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "\n",
    "from scipy.optimize import fmin_tnc\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from typing import*\n",
    "\n",
    "from collections import*\n",
    "# from scipy import*\n",
    "from sklearn.metrics import*\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "# import mnist\n",
    "# bltin_sum = np.sum\n",
    "\n",
    "import random\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn import*\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# def add(a, b): return a + b\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "Tensor = list\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c , v):\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "#     return np.sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "#     gen = \n",
    "    return np.sum(np.fromiter((v_i * w_i for v_i, w_i in zip(v, w)),float))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = data.shape[0]\n",
    "    \n",
    "    # Vector Mean\n",
    "#     n = len(data)\n",
    "#     m = np.sum(data,axis=0)\n",
    "#     means = np.multiply(1/n,m)\n",
    "    means = vector_mean(data)\n",
    "    \n",
    "    # Standard Deviaiton\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "    return means,stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = data.shape[0]\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    grad_step = np.add(v,step)\n",
    "    return grad_step\n",
    "\n",
    "# def predict(alpha, beta, x_i):\n",
    "#     pred = beta * x_i + alpha\n",
    "#     return pred\n",
    "\n",
    "# def error(x, y, beta):\n",
    "#     \"\"\"\n",
    "#     The error from predicting beta * x_i + alpha\n",
    "#     when the actual value is y_i\n",
    "#     \"\"\"\n",
    "#     err_fin = predict(alpha, beta, x_i) - y_i\n",
    "#     return err_fin\n",
    "\n",
    "def predict(x, beta):\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x, y, beta):\n",
    "    return predict(x, beta) - y \n",
    "\n",
    "def sqerror_gradient(x, y, beta):\n",
    "    err = error(x, y, beta)\n",
    "    err_fin = [2 * err * x_i for x_i in x]\n",
    "    return err_fin\n",
    "\n",
    "def least_squares_fit(xs, ys, learning_rate = 0.001, num_steps = 1000, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [np.random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def _negative_log_likelihood(x, y, beta):\n",
    "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))\n",
    "    \n",
    "def negative_log_likelihood(xs, ys, beta):\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))\n",
    "\n",
    "def _negative_log_partial_j(x, y, beta, j):\n",
    "    \"\"\"\n",
    "    The jth partial derivative for one data point.\n",
    "    Here i is the index of the data point.\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x, y, beta):\n",
    "    \"\"\"\n",
    "    The gradient for one data point.\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs, ys,beta):\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "                                                                \n",
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def sigmoid(t: float) -> float: \n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "# Gradient Descent - step\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "def squared_distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\"\n",
    "    return sum_of_squares(subtract(v, w))\n",
    "\n",
    "def distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes the distance between v and w\"\"\"\n",
    "    return math.sqrt(squared_distance(v, w))\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "\n",
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor.\n",
    "    Otherwise, tensor is 1-dimensional (that is, a vector).\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)  # just a list of floats, use Python sum\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
    "                   for tensor_i in tensor)   # and sum up those results.\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f elementwise\"\"\" \n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor] \n",
    "    \n",
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "\n",
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                   t1: Tensor,\n",
    "                   t2: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]\n",
    "    \n",
    "def inverse_normal_cdf(p: float,\n",
    "                       mu: float = 0,\n",
    "                       sigma: float = 1,\n",
    "                       tolerance: float = 0.00001) -> float:\n",
    "    \"\"\"Find approximate inverse using binary search\"\"\"\n",
    "\n",
    "    # if not standard, compute standard and rescale\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "\n",
    "    low_z = -10.0                      # normal_cdf(-10) is (very close to) 0\n",
    "    hi_z  =  10.0                      # normal_cdf(10)  is (very close to) 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2     # Consider the midpoint\n",
    "        mid_p = normal_cdf(mid_z)      # and the CDF's value there\n",
    "        if mid_p < p:\n",
    "            low_z = mid_z              # Midpoint too low, search above it\n",
    "        else:\n",
    "            hi_z = mid_z               # Midpoint too high, search below it\n",
    "\n",
    "    return mid_z\n",
    "\n",
    "def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -> float:\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "\n",
    "def num_differences(v1, v2):\n",
    "    assert len(v1) == len(v2)\n",
    "    return len([x1 for x1, x2 in zip(v1, v2) if x1 != x2])\n",
    "\n",
    "\n",
    "def cluster_means(k: int,\n",
    "                  inputs: List[Vector],\n",
    "                  assignments: List[int]) -> List[Vector]:\n",
    "    # clusters[i] contains the inputs whose assignment is i\n",
    "    clusters = [[] for i in range(k)]\n",
    "    for input, assignment in zip(inputs, assignments):\n",
    "        clusters[assignment].append(input)\n",
    "\n",
    "    # if a cluster is empty, just use a random point\n",
    "    return [vector_mean(cluster) if cluster else random.choice(inputs)\n",
    "            for cluster in clusters]\n",
    "\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k: int) -> None:\n",
    "        self.k = k                      # number of clusters\n",
    "        self.means = None\n",
    "\n",
    "    def classify(self, input: Vector) -> int:\n",
    "        \"\"\"return the index of the cluster closest to the input\"\"\"\n",
    "        return min(range(self.k),\n",
    "                   key=lambda i: squared_distance(input, self.means[i]))\n",
    "\n",
    "    def train(self, inputs: List[Vector]) -> None:\n",
    "        # Start with random assignments\n",
    "        assignments = [random.randrange(self.k) for _ in inputs]\n",
    "\n",
    "        with tqdm.tqdm(itertools.count()) as t:\n",
    "            for _ in t:\n",
    "                # Compute means and find new assignments\n",
    "                self.means = cluster_means(self.k, inputs, assignments)\n",
    "                new_assignments = [self.classify(input) for input in inputs]\n",
    "\n",
    "                # Check how many assignments changed and if we're done\n",
    "                num_changed = num_differences(assignments, new_assignments)\n",
    "                if num_changed == 0:\n",
    "                    return\n",
    "\n",
    "                # Otherwise keep the new assignments, and compute new means\n",
    "                assignments = new_assignments\n",
    "                self.means = cluster_means(self.k, inputs, assignments)\n",
    "                t.set_description(f\"changed: {num_changed} / {len(inputs)}\")\n",
    "                \n",
    "def squared_clustering_errors(inputs, k):\n",
    "    \"\"\"finds the total squared error from k-means clustering the\n",
    "inputs\"\"\"\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(inputs)\n",
    "    means = clusterer.means\n",
    "    assignments = [clusterer.classify(input) for input in inputs]\n",
    "\n",
    "    return sum(squared_distance(input, means[cluster])\n",
    "               for input, cluster in zip(inputs, assignments))\n",
    "\n",
    "def recolor(pixel):\n",
    "    cluster = clusterer.classify(pixel)        # index of the closest cluster\n",
    "    return clusterer.means[cluster]            # mean of the closest cluster\n",
    "\n",
    "def get_values(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        return [cluster.value]\n",
    "    else:\n",
    "        return [value\n",
    "                for child in cluster.children\n",
    "                for value in get_values(child)]\n",
    "\n",
    "\n",
    "def cluster_distance(cluster1,\n",
    "                     cluster2,\n",
    "                     distance_agg: Callable = min):\n",
    "    \"\"\"\n",
    "    compute all the pairwise distances between cluster1 and cluster2\n",
    "    and apply the aggregation function _distance_agg_ to the resulting\n",
    "list\n",
    "    \"\"\"\n",
    "    return distance_agg([distance(v1, v2) for v1 in get_values(cluster1) for v2 in get_values(cluster2)])\n",
    "\n",
    "def get_merge_order(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        return float('inf')  # was never merged\n",
    "    else:\n",
    "        return cluster.order\n",
    "    \n",
    "def get_children(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        raise TypeError(\"Leaf has no children\")\n",
    "    else:\n",
    "        return cluster.children\n",
    "    \n",
    "class Merged(NamedTuple):\n",
    "    children: tuple\n",
    "    order: int\n",
    "\n",
    "def bottom_up_cluster(inputs: List[Vector],\n",
    "                      distance_agg: Callable = min):\n",
    "    # Start with all leaves\n",
    "    clusters: List[Cluster] = [Leaf(input) for input in inputs]\n",
    "\n",
    "    def pair_distance(pair):\n",
    "        return cluster_distance(pair[0], pair[1], distance_agg)\n",
    " \n",
    "    # as long as we have more than one cluster left...\n",
    "    while len(clusters) > 1:\n",
    "        # find the two closest clusters\n",
    "        c1, c2 = min(((cluster1, cluster2) \n",
    "                      for i, cluster1 in enumerate(clusters)\n",
    "                      for cluster2 in clusters[:i]),\n",
    "                      key=pair_distance)\n",
    "\n",
    "        # remove them from the list of clusters\n",
    "        clusters = [c for c in clusters if c != c1 and c != c2]\n",
    "\n",
    "        # merge them, using merge_order = # of clusters left\n",
    "        merged_cluster = Merged((c1, c2), order=len(clusters))\n",
    "\n",
    "        # and add their merge\n",
    "        clusters.append(merged_cluster)\n",
    "\n",
    "    # when there's only one cluster left, return it\n",
    "    return clusters[0]\n",
    "\n",
    "def generate_clusters(base_cluster,\n",
    "                      num_clusters):\n",
    "    # start with a list with just the base cluster\n",
    "    clusters = [base_cluster]\n",
    "\n",
    "    # as long as we don't have enough clusters yet...\n",
    "    while len(clusters) < num_clusters:\n",
    "        # choose the last-merged of our clusters\n",
    "        next_cluster = min(clusters, key=get_merge_order)\n",
    "        # remove it from the list\n",
    "        clusters = [c for c in clusters if c != next_cluster]\n",
    "\n",
    "        # and add its children to the list (i.e., unmerge it)\n",
    "        clusters.extend(get_children(next_cluster))\n",
    "\n",
    "    # once we have enough clusters...\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "# Split data into train-test pools \n",
    "train, test, train_labels, test_labels = train_test_split(dataset['data'],\n",
    "                                                          dataset['target'],\n",
    "                                                          test_size=0.33)\n",
    "\n",
    "# Train model \n",
    "logregr = LogisticRegression().fit(train, train_labels)\n",
    "\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28965970447136247,\n",
       " 0.9941941842905381,\n",
       " 0.9997321471708763,\n",
       " 0.9495913654000099,\n",
       " 0.8875713052822441,\n",
       " 0.996233875920816,\n",
       " 0.9175859254873678,\n",
       " 0.8269797665623624,\n",
       " 0.9555755877523422,\n",
       " 0.9958412642179286,\n",
       " 5.427688796159216e-07,\n",
       " 0.33728761573512045,\n",
       " 0.999267296567496,\n",
       " 0.9855209080196369,\n",
       " 0.9850195690116662,\n",
       " 0.24562584968614903,\n",
       " 0.9957039909320989,\n",
       " 0.26921289594448156,\n",
       " 0.9892696857330596,\n",
       " 0.9989127970003899,\n",
       " 0.9966889307485368,\n",
       " 0.0008380055827887534,\n",
       " 0.9993812805257664,\n",
       " 0.8338151829714407,\n",
       " 0.9986637638820968,\n",
       " 0.9510749367837542,\n",
       " 0.7043410238596959,\n",
       " 1.732238560287501e-30,\n",
       " 0.8651547421573966,\n",
       " 0.9996977628749374,\n",
       " 0.9998183529478998,\n",
       " 4.762807341232129e-13,\n",
       " 0.0006190534518555784,\n",
       " 0.010982629866233449,\n",
       " 2.9481406587096157e-15,\n",
       " 0.0011434799507703533,\n",
       " 0.3378573741553376,\n",
       " 0.8285676549747114,\n",
       " 0.6427408230895504,\n",
       " 0.4893900675832151,\n",
       " 0.00219721351990391,\n",
       " 3.866825591455546e-07,\n",
       " 0.07117821924472975,\n",
       " 5.925761320308472e-15,\n",
       " 0.8234671034157489,\n",
       " 0.9997891737454384,\n",
       " 0.983958564544543,\n",
       " 0.9972576341990966,\n",
       " 0.9530136277757844,\n",
       " 0.9993875990510784,\n",
       " 0.9985463833182742,\n",
       " 0.995926371471466,\n",
       " 0.4631157721662046,\n",
       " 0.020969463422276142,\n",
       " 0.9916992638401047,\n",
       " 0.9995442465807305,\n",
       " 0.986142733018796,\n",
       " 0.0011442467497390731,\n",
       " 6.256059175438273e-10,\n",
       " 0.9951608836046497,\n",
       " 0.01425246559658919,\n",
       " 0.8601431134069683,\n",
       " 0.9610916807731801,\n",
       " 0.9922475907901073,\n",
       " 0.283108641473655,\n",
       " 0.8072918822081656,\n",
       " 0.99891366530409,\n",
       " 4.4366113739241415e-08,\n",
       " 0.896249876977697,\n",
       " 0.991302962155851,\n",
       " 0.9796527394608048,\n",
       " 7.653395149110611e-06,\n",
       " 0.9994358153785073,\n",
       " 7.531406150822208e-07,\n",
       " 5.0458573791068675e-06,\n",
       " 0.9784019302321236,\n",
       " 0.9991558683858598,\n",
       " 0.9992302535015626,\n",
       " 0.9996721223579778,\n",
       " 0.9670642015288521,\n",
       " 0.9987098357524099,\n",
       " 0.9932405583179403,\n",
       " 5.443626422030032e-05,\n",
       " 0.03650439724835711,\n",
       " 0.9918051881623348,\n",
       " 0.11839231307210521,\n",
       " 0.9642089217457432,\n",
       " 0.6391257554868235,\n",
       " 0.9893797118809441,\n",
       " 0.9904789260900635,\n",
       " 0.9844991741398577,\n",
       " 0.999699804805278,\n",
       " 0.9818927502855128,\n",
       " 0.07356722151472962,\n",
       " 0.9979575624674032,\n",
       " 0.9983430558487006,\n",
       " 0.7226132970539988,\n",
       " 0.9969544564025995,\n",
       " 4.271259711489711e-06,\n",
       " 6.450947484661852e-06,\n",
       " 0.9999232752383129,\n",
       " 0.0009586307089628518,\n",
       " 0.9787757957467451,\n",
       " 0.07293337480816275,\n",
       " 0.3229688468674953,\n",
       " 0.9925512162371074,\n",
       " 8.044149497663492e-06,\n",
       " 0.9997316992687809,\n",
       " 0.003933627190679345,\n",
       " 0.9928658894861996,\n",
       " 0.3778403730610204,\n",
       " 0.0024400209771474005,\n",
       " 1.2147357778236873e-10,\n",
       " 0.9592185345583027,\n",
       " 0.9998462504151919,\n",
       " 0.00022306408152840762,\n",
       " 0.99439340392369,\n",
       " 1.896332310189058e-05,\n",
       " 0.904270409229548,\n",
       " 5.463819422946188e-06,\n",
       " 0.9739852758258142,\n",
       " 2.992871295667732e-07,\n",
       " 0.8478749586788603,\n",
       " 0.1283446217812113,\n",
       " 0.0007491125632981347,\n",
       " 0.9960982067399196,\n",
       " 0.9981399339218826,\n",
       " 0.3042480695243592,\n",
       " 0.9275817456240083,\n",
       " 0.9840855643740064,\n",
       " 4.928055051439305e-07,\n",
       " 0.9371571430105291,\n",
       " 0.012568423766338349,\n",
       " 0.8180968158457007,\n",
       " 0.9996152073481207,\n",
       " 0.9977439566931198,\n",
       " 0.9713285772599379,\n",
       " 4.486878995827234e-09,\n",
       " 4.2939223683949794e-20,\n",
       " 0.10354180110352941,\n",
       " 0.20956772811517232,\n",
       " 0.991899454746801,\n",
       " 0.9792640798012426,\n",
       " 0.995949798587476,\n",
       " 0.9767215509063796,\n",
       " 0.9986176168319377,\n",
       " 0.9993257868602632,\n",
       " 3.2270065929753808e-15,\n",
       " 0.999360092762063,\n",
       " 0.9964750396487679,\n",
       " 0.9980148070001267,\n",
       " 0.996311308144497,\n",
       " 0.9955557485954982,\n",
       " 0.9975544341090966,\n",
       " 0.9974597652275127,\n",
       " 0.9956106070623075,\n",
       " 0.9985009672179329,\n",
       " 0.009740916015897064,\n",
       " 0.998122680779446,\n",
       " 0.98841896394767,\n",
       " 4.439883319750603e-08,\n",
       " 0.9998288569511088,\n",
       " 0.9995309934325703,\n",
       " 1.2032777684014663e-14,\n",
       " 0.9782433212802568,\n",
       " 0.08003108314344669,\n",
       " 6.82797704774058e-08,\n",
       " 3.931250299703377e-16,\n",
       " 0.9352738959478228,\n",
       " 1.5498464621289167e-07,\n",
       " 0.9980424057254453,\n",
       " 0.9723933277210354,\n",
       " 0.9972138636903053,\n",
       " 6.582541691855982e-06,\n",
       " 0.9994729422151111,\n",
       " 0.004249003115087905,\n",
       " 0.9973013228770694,\n",
       " 0.4380448551264765,\n",
       " 0.993650884715661,\n",
       " 0.9973163603400584,\n",
       " 0.3966426937968025,\n",
       " 2.3711146280744465e-10,\n",
       " 0.9997676856880295,\n",
       " 0.9997074056070921,\n",
       " 0.9939813225597925,\n",
       " 7.399255093414192e-09,\n",
       " 0.532399900763551,\n",
       " 0.9940566351524402]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename, listify \n",
    "actuals = list(test_labels)\n",
    "\n",
    "# Predict probablities of test data [0,1]\n",
    "scores = list(logregr.predict_proba(test)[:,1])\n",
    "\n",
    "# Equivalently\n",
    "import math\n",
    "def sigmoid(x): \n",
    "    return 1 / (1 + math.exp(-x))\n",
    "# scores = [sigmoid(logregr.coef_@test_i + logregr.intercept_) for test_i in test]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict binary outcomes (0,1)\n",
    "predictions = list(logregr.predict(test))\n",
    "\n",
    "# Equivalently \n",
    "predictions = [1 if s>0.5 else 0 for s in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.931\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = %.3f\" % (sum([p==a for p, a in zip(predictions, actuals)])/len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "ConfusionMatrix = collections.namedtuple('conf', ['tp','fp','tn','fn']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ConfusionMatrix(actuals, scores, threshold=0.5, positive_label=1):\n",
    "    tp=fp=tn=fn=0\n",
    "    bool_actuals = [act==positive_label for act in actuals]\n",
    "    for truth, score in zip(bool_actuals, scores):\n",
    "        if score > threshold:                      # predicted positive \n",
    "            if truth:                              # actually positive \n",
    "                tp += 1\n",
    "            else:                                  # actually negative              \n",
    "                fp += 1          \n",
    "        else:                                      # predicted negative \n",
    "            if not truth:                          # actually negative \n",
    "                tn += 1                          \n",
    "            else:                                  # actually positive \n",
    "                fn += 1\n",
    "\n",
    "    return ConfusionMatrix(tp, fp, tn, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACC(conf_mtrx):\n",
    "    return (conf_mtrx.tp + conf_mtrx.tn) / (conf_mtrx.fp + conf_mtrx.tn + conf_mtrx.tp + conf_mtrx.fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FPR(conf_mtrx):\n",
    "    return conf_mtrx.fp / (conf_mtrx.fp + conf_mtrx.tn) if (conf_mtrx.fp + conf_mtrx.tn)!=0 else 0\n",
    "def TPR(conf_mtrx):\n",
    "    return conf_mtrx.tp / (conf_mtrx.tp + conf_mtrx.fn) if (conf_mtrx.tp + conf_mtrx.fn)!=0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(actuals, scores, **fxns):\n",
    "    # generate thresholds over score domain \n",
    "    low = min(scores)\n",
    "    high = max(scores)\n",
    "    step = (abs(low) + abs(high)) / 1000\n",
    "    thresholds = np.arange(low-step, high+step, step)\n",
    "\n",
    "    # calculate confusion matrices for all thresholds\n",
    "    confusionMatrices = []\n",
    "    for threshold in thresholds:\n",
    "        confusionMatrices.append(calc_ConfusionMatrix(actuals, scores, threshold))\n",
    "\n",
    "    # apply functions to confusion matrices \n",
    "    results = {fname:list(map(fxn, confusionMatrices)) for fname, fxn in fxns.items()}\n",
    "\n",
    "    results[\"THR\"] = thresholds\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = apply(actuals,  scores, ACC=ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(actuals, scores):\n",
    "    return apply(actuals, scores, FPR=FPR, TPR=TPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 0.5135135135135135,\n",
       " 0.4864864864864865,\n",
       " 0.4594594594594595,\n",
       " 0.44594594594594594,\n",
       " 0.43243243243243246,\n",
       " 0.43243243243243246,\n",
       " 0.43243243243243246,\n",
       " 0.43243243243243246,\n",
       " 0.43243243243243246,\n",
       " 0.4189189189189189,\n",
       " 0.40540540540540543,\n",
       " 0.40540540540540543,\n",
       " 0.3918918918918919,\n",
       " 0.3918918918918919,\n",
       " 0.3783783783783784,\n",
       " 0.3783783783783784,\n",
       " 0.3783783783783784,\n",
       " 0.3783783783783784,\n",
       " 0.3783783783783784,\n",
       " 0.3783783783783784,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.36486486486486486,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.33783783783783783,\n",
       " 0.32432432432432434,\n",
       " 0.32432432432432434,\n",
       " 0.32432432432432434,\n",
       " 0.32432432432432434,\n",
       " 0.32432432432432434,\n",
       " 0.32432432432432434,\n",
       " 0.32432432432432434,\n",
       " 0.32432432432432434,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.3108108108108108,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.2972972972972973,\n",
       " 0.28378378378378377,\n",
       " 0.28378378378378377,\n",
       " 0.28378378378378377,\n",
       " 0.28378378378378377,\n",
       " 0.28378378378378377,\n",
       " 0.28378378378378377,\n",
       " 0.28378378378378377,\n",
       " 0.28378378378378377,\n",
       " 0.28378378378378377,\n",
       " 0.28378378378378377,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.2702702702702703,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.25675675675675674,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.24324324324324326,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.22972972972972974,\n",
       " 0.21621621621621623,\n",
       " 0.21621621621621623,\n",
       " 0.21621621621621623,\n",
       " 0.21621621621621623,\n",
       " 0.21621621621621623,\n",
       " 0.21621621621621623,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.20270270270270271,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.1891891891891892,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.16216216216216217,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.14864864864864866,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.13513513513513514,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.12162162162162163,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.10810810810810811,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.0945945945945946,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.08108108108108109,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.06756756756756757,\n",
       " 0.05405405405405406,\n",
       " 0.05405405405405406,\n",
       " 0.05405405405405406,\n",
       " 0.05405405405405406,\n",
       " 0.05405405405405406,\n",
       " 0.05405405405405406,\n",
       " 0.05405405405405406,\n",
       " 0.05405405405405406,\n",
       " 0.05405405405405406,\n",
       " 0.05405405405405406,\n",
       " 0.05405405405405406,\n",
       " 0.04054054054054054,\n",
       " 0.04054054054054054,\n",
       " 0.04054054054054054,\n",
       " 0.04054054054054054,\n",
       " 0.04054054054054054,\n",
       " 0.04054054054054054,\n",
       " 0.04054054054054054,\n",
       " 0.04054054054054054,\n",
       " 0.04054054054054054,\n",
       " 0.04054054054054054,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.02702702702702703,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab = ROC(actuals,scores)\n",
    "ab['FPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x176aee674c0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOZ0lEQVR4nO3dX4idd53H8fdnEwsrWuuaUdKk3WSXVB3BFh1bXdbduuKadJEgeNFWlC1KLGvFy5aFtRe9WZEFkVZDKKF4oRHWYuMSLQuLdqHbbqYQ2yalMptiOk2hUxUL9aKk/e7FHOX0dDLnmfaZOTm/eb9g6Hme5zfnfL/M8Okvv3n+pKqQJE2/P5l0AZKkfhjoktQIA12SGmGgS1IjDHRJasTWSX3wtm3bateuXZP6eEmaSo888sjzVTWz0rGJBfquXbuYn5+f1MdL0lRK8qvzHXPJRZIaYaBLUiMMdElqhIEuSY0w0CWpEWMDPcnhJM8lefw8x5PkW0kWkjya5AP9lylJGqfLDP0eYO8qx/cBewZfB4DvvPGyJElrNfY89Kp6IMmuVYbsB75by/fhfSjJJUm2V9WzfRWpC9P3Hj7DfSeemXQZ0tSZvfRibv/U+3p/3z7W0HcATw9tLw72vUaSA0nmk8wvLS318NGapPtOPMOpZ1+YdBmSBvq4UjQr7FvxqRlVdQg4BDA3N+eTNVYxDbPfU8++wOz2i/nBlz4y6VIk0c8MfRG4bGh7J3C2h/fd1KZh9ju7/WL2X7XiP8YkTUAfM/SjwC1JjgDXAL9z/fy11jrjdvYraa3GBnqS7wPXAtuSLAK3A28CqKqDwDHgOmAB+D1w03oVO01GA/zhp34DwDW7/6zT9zv7lbRWXc5yuWHM8QK+3FtFjfjDksns9ouB5SDff9UObrzm8glXJqlVE7t97jTrsnzikomkjeal/69Dlz9YumQiaaNtihl636cAOvuWdCHaFDP0vk8BdPYt6UK0KWbogDNqSc1rMtBHl1iGzzaRpFY1ueQyusTiEomkzaDJGTq4xCJp82lyhi5Jm5GBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRTdzLxbsrSlIjM3TvrihJjczQwbsrSlITM3RJkoEuSc0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJakSnQE+yN8mTSRaS3LbC8bcl+XGSXyQ5meSm/kuVJK1mbKAn2QLcBewDZoEbksyODPsycKqqrgSuBf4tyUU91ypJWkWXGfrVwEJVna6ql4AjwP6RMQW8NUmAtwC/Ac71WqkkaVVdAn0H8PTQ9uJg37A7gfcCZ4HHgK9W1Sujb5TkQJL5JPNLS0uvs2RJ0kq6BHpW2Fcj258ETgCXAlcBdyZ5zQ3Jq+pQVc1V1dzMzMwaS5UkraZLoC8Clw1t72R5Jj7sJuDeWrYAPAW8p58SJUlddAn048CeJLsHf+i8Hjg6MuYM8HGAJO8C3g2c7rNQSdLqxj7goqrOJbkFuB/YAhyuqpNJbh4cPwjcAdyT5DGWl2hurarn17FuSdKITk8sqqpjwLGRfQeHXp8F/r7f0iRJa+GVopLUCANdkhoxdQ+J/t7DZ7jvxDOv2nfq2ReY3f6asyQlaVOZuhn6fSee4dSzL7xq3+z2i9l/1ei1TpK0uUzdDB2WA/wHX/rIpMuQpAvK1M3QJUkrM9AlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqRGdAj3J3iRPJllIctt5xlyb5ESSk0l+3m+ZkqRxto4bkGQLcBfwCWAROJ7kaFWdGhpzCfBtYG9VnUnyznWqV5J0Hl1m6FcDC1V1uqpeAo4A+0fG3AjcW1VnAKrquX7LlCSN0yXQdwBPD20vDvYNuwJ4e5KfJXkkyedXeqMkB5LMJ5lfWlp6fRVLklbUJdCzwr4a2d4KfBD4B+CTwL8kueI131R1qKrmqmpuZmZmzcVKks5v7Bo6yzPyy4a2dwJnVxjzfFW9CLyY5AHgSuCXvVQpSRqrywz9OLAnye4kFwHXA0dHxtwHfDTJ1iRvBq4Bnui3VEnSasbO0KvqXJJbgPuBLcDhqjqZ5ObB8YNV9USSnwKPAq8Ad1fV4+tZuCTp1bosuVBVx4BjI/sOjmx/A/hGf6VJktbCK0UlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWpEp0BPsjfJk0kWkty2yrgPJXk5yWf6K1GS1MXYQE+yBbgL2AfMAjckmT3PuK8D9/ddpCRpvC4z9KuBhao6XVUvAUeA/SuM+wrwQ+C5HuuTJHXUJdB3AE8PbS8O9v1Rkh3Ap4GDq71RkgNJ5pPMLy0trbVWSdIqugR6VthXI9vfBG6tqpdXe6OqOlRVc1U1NzMz07FESVIXWzuMWQQuG9reCZwdGTMHHEkCsA24Lsm5qvpRH0VKksbrEujHgT1JdgPPANcDNw4PqKrdf3id5B7gPwxzSdpYYwO9qs4luYXls1e2AIer6mSSmwfHV103lyRtjC4zdKrqGHBsZN+KQV5V//jGy5IkrZVXikpSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1IhOgZ5kb5InkywkuW2F459N8ujg68EkV/ZfqiRpNWMDPckW4C5gHzAL3JBkdmTYU8DfVtX7gTuAQ30XKklaXZcZ+tXAQlWdrqqXgCPA/uEBVfVgVf12sPkQsLPfMiVJ43QJ9B3A00Pbi4N95/MF4CcrHUhyIMl8kvmlpaXuVUqSxuoS6FlhX604MPkYy4F+60rHq+pQVc1V1dzMzEz3KiVJY23tMGYRuGxoeydwdnRQkvcDdwP7qurX/ZQnSeqqywz9OLAnye4kFwHXA0eHByS5HLgX+FxV/bL/MiVJ44ydoVfVuSS3APcDW4DDVXUyyc2D4weBrwHvAL6dBOBcVc2tX9mSpFFdllyoqmPAsZF9B4defxH4Yr+lSZLWwitFJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqRKdAT7I3yZNJFpLctsLxJPnW4PijST7Qf6mSpNVsHTcgyRbgLuATwCJwPMnRqjo1NGwfsGfwdQ3wncF/ezd76cXr8baSNPXGBjpwNbBQVacBkhwB9gPDgb4f+G5VFfBQkkuSbK+qZ/su+PZPva/vt5SkJnRZctkBPD20vTjYt9YxJDmQZD7J/NLS0lprlSStokugZ4V99TrGUFWHqmququZmZma61CdJ6qhLoC8Clw1t7wTOvo4xkqR11CXQjwN7kuxOchFwPXB0ZMxR4PODs10+DPxuPdbPJUnnN/aPolV1LsktwP3AFuBwVZ1McvPg+EHgGHAdsAD8Hrhp/UqWJK2ky1kuVNUxlkN7eN/BodcFfLnf0iRJa+GVopLUCANdkhqR5dWSCXxwsgT86nV++zbg+R7LmQb2vDnY8+bwRnr+86pa8bzviQX6G5FkvqrmJl3HRrLnzcGeN4f16tklF0lqhIEuSY2Y1kA/NOkCJsCeNwd73hzWpeepXEOXJL3WtM7QJUkjDHRJasQFHeib8dF3HXr+7KDXR5M8mOTKSdTZp3E9D437UJKXk3xmI+tbD116TnJtkhNJTib5+UbX2LcOv9tvS/LjJL8Y9DzV94RKcjjJc0keP8/x/vOrqi7IL5ZvBPZ/wF8AFwG/AGZHxlwH/ITl+7F/GHh40nVvQM9/Bbx98HrfZuh5aNx/sXxPoc9Muu4N+DlfwvJTwS4fbL9z0nVvQM//DHx98HoG+A1w0aRrfwM9/w3wAeDx8xzvPb8u5Bn6Hx99V1UvAX949N2wPz76rqoeAi5Jsn2jC+3R2J6r6sGq+u1g8yGW7z0/zbr8nAG+AvwQeG4ji1snXXq+Ebi3qs4AVNW0992l5wLemiTAW1gO9HMbW2Z/quoBlns4n97z60IO9N4efTdF1trPF1j+P/w0G9tzkh3Ap4GDtKHLz/kK4O1JfpbkkSSf37Dq1keXnu8E3svyw3EeA75aVa9sTHkT0Xt+dbp97oT09ui7KdK5nyQfYznQ/3pdK1p/XXr+JnBrVb28PHmbel163gp8EPg48KfA/yR5qKp+ud7FrZMuPX8SOAH8HfCXwH8m+e+qemGda5uU3vPrQg70zfjou079JHk/cDewr6p+vUG1rZcuPc8BRwZhvg24Lsm5qvrRhlTYv66/289X1YvAi0keAK4EpjXQu/R8E/CvtbzAvJDkKeA9wP9uTIkbrvf8upCXXDbjo+/G9pzkcuBe4HNTPFsbNrbnqtpdVbuqahfw78A/TXGYQ7ff7fuAjybZmuTNwDXAExtcZ5+69HyG5X+RkORdwLuB0xta5cbqPb8u2Bl6bcJH33Xs+WvAO4BvD2as52qK71TXseemdOm5qp5I8lPgUeAV4O6qWvH0t2nQ8ed8B3BPksdYXo64taqm9ra6Sb4PXAtsS7II3A68CdYvv7z0X5IacSEvuUiS1sBAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY34fysFDWAz038/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ab['FPR'],ab['TPR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
