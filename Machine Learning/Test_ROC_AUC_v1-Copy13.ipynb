{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import urllib.request\n",
    "import requests\n",
    "import curl\n",
    "import pycurl\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from IPython import qt\n",
    "from matplotlib.pyplot import figure\n",
    "from py.xml import raw\n",
    "from requests.api import get\n",
    "from matplotlib import pyplot as plt\n",
    "# from scratch.working_with_data import rescale\n",
    "# from scratch.multiple_regression import least_squares_fit, predict\n",
    "# from scratch.gradient_descent import gradient_step\n",
    "\n",
    "# from stats import mean, median, de_mean, standard_deviation, correlation\n",
    "# from gradient_descent import minimize_stochastic, maximize_stochastic, maximize_batch\n",
    "# from vector import dot, vector_add\n",
    "# from normal import normal_cdf\n",
    "# from matrix import make_matrix, get_column, shape, matrix_multiply\n",
    "# from logistic_regression import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "\n",
    "from scipy.optimize import fmin_tnc\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from typing import*\n",
    "\n",
    "from collections import*\n",
    "# from scipy import*\n",
    "from sklearn.metrics import*\n",
    "\n",
    "from numpy import *\n",
    "import random\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn import*\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# def add(a, b): return a + b\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "Tensor = list\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c , v):\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "#     return np.sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "#     gen = \n",
    "    return np.sum(np.fromiter((v_i * w_i for v_i, w_i in zip(v, w)),float))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = data.shape[0]\n",
    "    \n",
    "    # Vector Mean\n",
    "#     n = len(data)\n",
    "#     m = np.sum(data,axis=0)\n",
    "#     means = np.multiply(1/n,m)\n",
    "    means = vector_mean(data)\n",
    "    \n",
    "    # Standard Deviaiton\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "    return means,stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = data.shape[0]\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    grad_step = np.add(v,step)\n",
    "    return grad_step\n",
    "\n",
    "# def predict(alpha, beta, x_i):\n",
    "#     pred = beta * x_i + alpha\n",
    "#     return pred\n",
    "\n",
    "# def error(x, y, beta):\n",
    "#     \"\"\"\n",
    "#     The error from predicting beta * x_i + alpha\n",
    "#     when the actual value is y_i\n",
    "#     \"\"\"\n",
    "#     err_fin = predict(alpha, beta, x_i) - y_i\n",
    "#     return err_fin\n",
    "\n",
    "def predict(x, beta):\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x, y, beta):\n",
    "    return predict(x, beta) - y \n",
    "\n",
    "def sqerror_gradient(x, y, beta):\n",
    "    err = error(x, y, beta)\n",
    "    err_fin = [2 * err * x_i for x_i in x]\n",
    "    return err_fin\n",
    "\n",
    "def least_squares_fit(xs, ys, learning_rate = 0.001, num_steps = 1000, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [np.random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def _negative_log_likelihood(x, y, beta):\n",
    "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))\n",
    "    \n",
    "def negative_log_likelihood(xs, ys, beta):\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))\n",
    "\n",
    "def _negative_log_partial_j(x, y, beta, j):\n",
    "    \"\"\"\n",
    "    The jth partial derivative for one data point.\n",
    "    Here i is the index of the data point.\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x, y, beta):\n",
    "    \"\"\"\n",
    "    The gradient for one data point.\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs, ys,beta):\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "                                                                \n",
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def sigmoid(t: float) -> float: \n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "# Gradient Descent - step\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "def squared_distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\"\n",
    "    return sum_of_squares(subtract(v, w))\n",
    "\n",
    "def distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes the distance between v and w\"\"\"\n",
    "    return math.sqrt(squared_distance(v, w))\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "\n",
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor.\n",
    "    Otherwise, tensor is 1-dimensional (that is, a vector).\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)  # just a list of floats, use Python sum\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
    "                   for tensor_i in tensor)   # and sum up those results.\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f elementwise\"\"\" \n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor] \n",
    "    \n",
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "\n",
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                   t1: Tensor,\n",
    "                   t2: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]\n",
    "    \n",
    "def inverse_normal_cdf(p: float,\n",
    "                       mu: float = 0,\n",
    "                       sigma: float = 1,\n",
    "                       tolerance: float = 0.00001) -> float:\n",
    "    \"\"\"Find approximate inverse using binary search\"\"\"\n",
    "\n",
    "    # if not standard, compute standard and rescale\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "\n",
    "    low_z = -10.0                      # normal_cdf(-10) is (very close to) 0\n",
    "    hi_z  =  10.0                      # normal_cdf(10)  is (very close to) 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2     # Consider the midpoint\n",
    "        mid_p = normal_cdf(mid_z)      # and the CDF's value there\n",
    "        if mid_p < p:\n",
    "            low_z = mid_z              # Midpoint too low, search above it\n",
    "        else:\n",
    "            hi_z = mid_z               # Midpoint too high, search below it\n",
    "\n",
    "    return mid_z\n",
    "\n",
    "def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -> float:\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "\n",
    "def num_differences(v1, v2):\n",
    "    assert len(v1) == len(v2)\n",
    "    return len([x1 for x1, x2 in zip(v1, v2) if x1 != x2])\n",
    "\n",
    "\n",
    "def cluster_means(k: int,\n",
    "                  inputs: List[Vector],\n",
    "                  assignments: List[int]) -> List[Vector]:\n",
    "    # clusters[i] contains the inputs whose assignment is i\n",
    "    clusters = [[] for i in range(k)]\n",
    "    for input, assignment in zip(inputs, assignments):\n",
    "        clusters[assignment].append(input)\n",
    "\n",
    "    # if a cluster is empty, just use a random point\n",
    "    return [vector_mean(cluster) if cluster else random.choice(inputs)\n",
    "            for cluster in clusters]\n",
    "\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k: int) -> None:\n",
    "        self.k = k                      # number of clusters\n",
    "        self.means = None\n",
    "\n",
    "    def classify(self, input: Vector) -> int:\n",
    "        \"\"\"return the index of the cluster closest to the input\"\"\"\n",
    "        return min(range(self.k),\n",
    "                   key=lambda i: squared_distance(input, self.means[i]))\n",
    "\n",
    "    def train(self, inputs: List[Vector]) -> None:\n",
    "        # Start with random assignments\n",
    "        assignments = [random.randrange(self.k) for _ in inputs]\n",
    "\n",
    "        with tqdm.tqdm(itertools.count()) as t:\n",
    "            for _ in t:\n",
    "                # Compute means and find new assignments\n",
    "                self.means = cluster_means(self.k, inputs, assignments)\n",
    "                new_assignments = [self.classify(input) for input in inputs]\n",
    "\n",
    "                # Check how many assignments changed and if we're done\n",
    "                num_changed = num_differences(assignments, new_assignments)\n",
    "                if num_changed == 0:\n",
    "                    return\n",
    "\n",
    "                # Otherwise keep the new assignments, and compute new means\n",
    "                assignments = new_assignments\n",
    "                self.means = cluster_means(self.k, inputs, assignments)\n",
    "                t.set_description(f\"changed: {num_changed} / {len(inputs)}\")\n",
    "                \n",
    "def squared_clustering_errors(inputs, k):\n",
    "    \"\"\"finds the total squared error from k-means clustering the\n",
    "inputs\"\"\"\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(inputs)\n",
    "    means = clusterer.means\n",
    "    assignments = [clusterer.classify(input) for input in inputs]\n",
    "\n",
    "    return sum(squared_distance(input, means[cluster])\n",
    "               for input, cluster in zip(inputs, assignments))\n",
    "\n",
    "def recolor(pixel):\n",
    "    cluster = clusterer.classify(pixel)        # index of the closest cluster\n",
    "    return clusterer.means[cluster]            # mean of the closest cluster\n",
    "\n",
    "def get_values(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        return [cluster.value]\n",
    "    else:\n",
    "        return [value\n",
    "                for child in cluster.children\n",
    "                for value in get_values(child)]\n",
    "\n",
    "\n",
    "def cluster_distance(cluster1,\n",
    "                     cluster2,\n",
    "                     distance_agg: Callable = min):\n",
    "    \"\"\"\n",
    "    compute all the pairwise distances between cluster1 and cluster2\n",
    "    and apply the aggregation function _distance_agg_ to the resulting\n",
    "list\n",
    "    \"\"\"\n",
    "    return distance_agg([distance(v1, v2) for v1 in get_values(cluster1) for v2 in get_values(cluster2)])\n",
    "\n",
    "def get_merge_order(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        return float('inf')  # was never merged\n",
    "    else:\n",
    "        return cluster.order\n",
    "    \n",
    "def get_children(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        raise TypeError(\"Leaf has no children\")\n",
    "    else:\n",
    "        return cluster.children\n",
    "    \n",
    "class Merged(NamedTuple):\n",
    "    children: tuple\n",
    "    order: int\n",
    "\n",
    "def bottom_up_cluster(inputs: List[Vector],\n",
    "                      distance_agg: Callable = min):\n",
    "    # Start with all leaves\n",
    "    clusters: List[Cluster] = [Leaf(input) for input in inputs]\n",
    "\n",
    "    def pair_distance(pair):\n",
    "        return cluster_distance(pair[0], pair[1], distance_agg)\n",
    " \n",
    "    # as long as we have more than one cluster left...\n",
    "    while len(clusters) > 1:\n",
    "        # find the two closest clusters\n",
    "        c1, c2 = min(((cluster1, cluster2) \n",
    "                      for i, cluster1 in enumerate(clusters)\n",
    "                      for cluster2 in clusters[:i]),\n",
    "                      key=pair_distance)\n",
    "\n",
    "        # remove them from the list of clusters\n",
    "        clusters = [c for c in clusters if c != c1 and c != c2]\n",
    "\n",
    "        # merge them, using merge_order = # of clusters left\n",
    "        merged_cluster = Merged((c1, c2), order=len(clusters))\n",
    "\n",
    "        # and add their merge\n",
    "        clusters.append(merged_cluster)\n",
    "\n",
    "    # when there's only one cluster left, return it\n",
    "    return clusters[0]\n",
    "\n",
    "def generate_clusters(base_cluster,\n",
    "                      num_clusters):\n",
    "    # start with a list with just the base cluster\n",
    "    clusters = [base_cluster]\n",
    "\n",
    "    # as long as we don't have enough clusters yet...\n",
    "    while len(clusters) < num_clusters:\n",
    "        # choose the last-merged of our clusters\n",
    "        next_cluster = min(clusters, key=get_merge_order)\n",
    "        # remove it from the list\n",
    "        clusters = [c for c in clusters if c != next_cluster]\n",
    "\n",
    "        # and add its children to the list (i.e., unmerge it)\n",
    "        clusters.extend(get_children(next_cluster))\n",
    "\n",
    "    # once we have enough clusters...\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionOVR(object):\n",
    "    def __init__(self, eta=0.1, n_iter=50):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        self.w = []\n",
    "        m = X.shape[0]\n",
    "\n",
    "        for i in np.unique(y):\n",
    "            y_copy = np.where(y == i, 1, 0)\n",
    "            w = np.ones(X.shape[1])\n",
    "#             w = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "            for _ in range(self.n_iter):\n",
    "                output = X.dot(w)\n",
    "                errors = y_copy - self._sigmoid(output)\n",
    "                w += self.eta / m * errors.dot(X)\n",
    "            self.w.append((w, i))\n",
    "        return self\n",
    "    \n",
    "#      def predict(self, X):\n",
    "#         output = np.insert(X, 0, 1, axis=1).dot(self.w)\n",
    "#         return (np.floor(self._sigmoid(output) + .5)).astype(int)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_one(i) for i in np.insert(X, 0, 1, axis=1)]\n",
    "    \n",
    "    def _predict_one(self, x):\n",
    "        return max((x.dot(w), c) for w, c in self.w)[1]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.sum(self.predict(X) == y) / len(y)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import some data to play with\n",
    "# iris = datasets.load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# # Binarize the output\n",
    "# y = label_binarize(y, classes=[0, 1, 2])\n",
    "# n_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_5396/3648027858.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  rows = np.array(rows)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# Setup rows array to read iris dataset\n",
    "rows = []\n",
    "\n",
    "filename = 'iris.csv'\n",
    "# reading csv file \n",
    "with open(filename, 'r') as csvfile: \n",
    "    # creating a csv reader object \n",
    "    csvreader = csv.reader(csvfile) \n",
    "# extracting each data row one by one\n",
    "    for row in csvreader: \n",
    "        rows.append(row) \n",
    "        \n",
    "def parse_iris_row(row):\n",
    "    \"\"\"\n",
    "    sepal_length, sepal_width, petal_length, petal_width, class\n",
    "    \"\"\"\n",
    "    measurements = [float(value) for value in row[:-1]]\n",
    "    # class is e.g. \"Iris-virginica\"; we just want \"virginica\"\n",
    "    label = [value.split('-')[-1]for value in row]\n",
    "\n",
    "    return label\n",
    "\n",
    "# Conver rows list to array\n",
    "rows = np.array(rows)\n",
    "label = rows[0][-1].split('-')[-1]\n",
    "label\n",
    "\n",
    "# Create iris_data list\n",
    "iris_data = [parse_iris_row(row) for row in rows]\n",
    "\n",
    "\n",
    "# Iris dataframe\n",
    "iris_data_pd = pd.DataFrame(iris_data)\n",
    "iris_pd = iris_data_pd.iloc[:,0:4]\n",
    "iris_point = iris_pd.to_numpy().tolist()\n",
    "# iris_point = iris_pd.to_numpy()\n",
    "iris_point\n",
    "\n",
    "iris_target_pd = iris_data_pd.iloc[:,4]\n",
    "iris_target = iris_target_pd.to_numpy().tolist()\n",
    "\n",
    "# Convert labels from text to numeral\n",
    "for i in range(len(iris_target)):\n",
    "    if iris_target[i] == 'setosa':\n",
    "        iris_target[i] = 0\n",
    "    elif iris_target[i] == 'versicolor':\n",
    "        iris_target[i] = 1\n",
    "    elif iris_target[i] == 'virginica':\n",
    "        iris_target[i] = 2\n",
    "\n",
    "# Subset dataframe into lists/arrays\n",
    "iris_point_2 = iris_point[:-1]\n",
    "iris_target_2 = iris_target[:-1]\n",
    "iris_actual = iris_target_2\n",
    "print(iris_actual)\n",
    "# type(iris_point_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = load_breast_cancer()\n",
    "dataset2 = load_iris()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split2(xs, ys, test_pct):\n",
    "    # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split2(dataset['data'],\n",
    "                                                          dataset['target'],\n",
    "                                                          0.25)\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logregr: LogisticRegression()\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.linear_model import *\n",
    "# from sklearn.model_selection import train_test_split\n",
    "dataset = load_breast_cancer()\n",
    "dataset2 = load_iris()\n",
    "\n",
    "def logistic_prime(x: float) -> float:\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "# Split data into train-test pools \n",
    "# train, test, train_labels, test_labels = train_test_split2(np.array(iris_point_2),\n",
    "#                                                           np.array(iris_target_2),\n",
    "#                                                           0.33)\n",
    "train1, test1, train1_labels, test1_labels = train_test_split(np.array(iris_point_2),\n",
    "                                                          np.array(iris_target_2),\n",
    "                                                          test_size=0.7)\n",
    "\n",
    "# # Train model \n",
    "logregr = LogisticRegression().fit(train1, train1_labels)\n",
    "print('logregr:', logregr)\n",
    "\n",
    "# ab_x_tst = []\n",
    "# ab_y_tst = []\n",
    "# for i in range(len(train)):\n",
    "#     alog = logistic(train[i][0])\n",
    "#     ab_y_tst.append(alog)\n",
    "#     alog_prime = logistic_prime(train[i][0])\n",
    "#     ab_x_tst.append(alog_prime)\n",
    "    \n",
    "# plt.scatter(ab_x_tst,ab_y_tst)\n",
    "\n",
    "type(dataset2['data'])\n",
    "# train1 = train.dtype('u8')\n",
    "train = train1.astype('float')\n",
    "test = test1.astype('float')\n",
    "train_labels = train1_labels.astype('float')\n",
    "test_labels = test1_labels.astype('float')\n",
    "# train = np.array(train,dtype('float'))\n",
    "# test = np.array(test,dtype('float'))\n",
    "# train_labels = np.array(train_labels,dtype('float'))\n",
    "# test_labels = np.array(test_labels,dtype('float'))\n",
    "# test\n",
    "\n",
    "# Train model \n",
    "# logregr = LogisticRegression().fit(train, train_labels)\n",
    "# print('logregr:', logregr)\n",
    "# test_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.16057764,  0.30857189,  1.96858275, -2.77072036, -0.73799366])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi = LogisticRegressionOVR(n_iter=1000).fit(train, train_labels)\n",
    "# log_sig = logi._sigmoid(test)\n",
    "# log_sig\n",
    "# logi.w\n",
    "\n",
    "logi.w\n",
    "log_point = logi.w\n",
    "log_point = log_point[0][0]\n",
    "log_point_2 = log_point\n",
    "log_point_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_v2(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "\n",
    "random.seed(0)\n",
    "test_size = 0.33\n",
    "x_train, x_test, y_train, y_test = train_test_split_v2(dataset['data'], dataset['target'],test_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split2(xs, ys, test_pct):\n",
    "    # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split2(dataset['data'],\n",
    "                                                          dataset['target'],\n",
    "                                                          0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "z = np.dot(test.reshape(84,5),log_point_2.reshape(5,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "    \n",
    "h = sigmoid(z)\n",
    "scores = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "[array([8.89186696e-08]), array([9.77570539e-08]), array([2.17805188e-07]), array([2.80591793e-07]), array([6.58006334e-07]), array([7.41871334e-07]), array([1.5170994e-06]), array([1.91766255e-06]), array([3.18053867e-06]), array([5.21084538e-06]), array([6.25442852e-06]), array([9.60690371e-06]), array([9.93029853e-06]), array([1.87588874e-05]), array([0.00010162]), array([0.00012347]), array([0.00015253]), array([0.00018162]), array([0.00025053]), array([0.0002767]), array([0.0002785]), array([0.00033213]), array([0.00039428]), array([0.00045172]), array([0.00051306]), array([0.00053662]), array([0.00057557]), array([0.00060613]), array([0.00065482]), array([0.00087968]), array([0.00153787]), array([0.00262902]), array([0.00314672]), array([0.00345469]), array([0.00486348]), array([0.0177449]), array([0.01981255]), array([0.02967249]), array([0.03724826]), array([0.06696112]), array([0.63573719]), array([0.80384633]), array([0.81911994]), array([0.90689385]), array([0.93310342]), array([0.93984926]), array([0.95181244]), array([0.976517]), array([0.97659221]), array([0.97863922]), array([0.98095549]), array([0.9830692]), array([0.98601683]), array([0.99118394]), array([0.99359467]), array([0.99362852]), array([0.99445394]), array([0.99492513]), array([0.99594461]), array([0.99643561]), array([0.99704732]), array([0.99718031]), array([0.99745226]), array([0.99795468]), array([0.99815221]), array([0.99816918]), array([0.99841302]), array([0.99876102]), array([0.99882431]), array([0.99892303]), array([0.99936519]), array([0.99943698]), array([0.99963962]), array([0.99968754]), array([0.99971231]), array([0.99974531]), array([0.99975946]), array([0.99981641]), array([0.99981883]), array([0.99988869]), array([0.99989517]), array([0.99991413]), array([0.99991736]), array([0.99997664])]\n"
     ]
    }
   ],
   "source": [
    "h = sigmoid(z)\n",
    "scores = sorted(h)\n",
    "\n",
    "print(len(scores))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename, listify \n",
    "# train, test, train_labels, test_labels \n",
    "actuals = list(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions with k-nearest neighbors on the Iris Flowers Dataset\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "# \t\tprint('[%s] => %d' % (value, i))\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "# Find the min and max values for each column\n",
    "# def dataset_minmax(dataset):\n",
    "# \tminmax = list()\n",
    "# \tfor i in range(len(dataset[0])):\n",
    "# \t\tcol_values = [row[i] for row in dataset]\n",
    "# \t\tvalue_min = min(col_values)\n",
    "# \t\tvalue_max = max(col_values)\n",
    "# \t\tminmax.append([value_min, value_max])\n",
    "# \treturn minmax\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_mean_std(dataset):\n",
    "\tmean_std = list()\n",
    "\tfor i in range(len(dataset[0])):\n",
    "\t\tcol_values = [row[i] for row in dataset]\n",
    "\t\tvalue_mean = np.mean(col_values)\n",
    "\t\tvalue_std = np.std(col_values)\n",
    "\t\tmean_std.append([value_mean, value_std])\n",
    "\treturn mean_std\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "# def normalize_dataset(dataset, minmax):\n",
    "# \tfor row in dataset:\n",
    "# \t\tfor i in range(len(row)):\n",
    "# \t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "            \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, mean_std):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)):\n",
    "\t\t\trow[i] = (row[i] - mean_std[i][0]) / (mean_std[i][1])\n",
    " \n",
    " \n",
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    " \n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors\n",
    " \n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction\n",
    " \n",
    "# Make a prediction with KNN on Iris Dataset\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# define model parameter\n",
    "num_neighbors = 3\n",
    "# define a new record\n",
    "row = [4.7,7.9,3.2,0.3]\n",
    "row1 = [0.7,1.5,8.4,2.0]\n",
    "# predict the label\n",
    "label = predict_classification(dataset, test[0].tolist(), num_neighbors)\n",
    "# print('Data=%s, Predicted: %s' % (row, label))\n",
    "# print(dataset)\n",
    "\n",
    "\n",
    "labels1 = []\n",
    "for j in range(len(test)):\n",
    "    labels1.append(predict_classification(dataset, test[j], num_neighbors))\n",
    "# labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 1.19417941,  0.36433176,  2.10393047, -2.99319861, -0.84623149]),\n",
       "  0.0),\n",
       " (array([ 0.39295093,  0.94697113, -2.28935156,  0.20323611, -1.00491131]),\n",
       "  1.0),\n",
       " (array([-0.19098979, -2.91811699, -1.9030465 ,  3.8637099 ,  3.26705559]),\n",
       "  2.0)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi = LogisticRegressionOVR(n_iter=1500).fit(train, train_labels)\n",
    "log_sig = logi._sigmoid(test)\n",
    "log_sig\n",
    "logi.w\n",
    "# plt.plot(log_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "# Method 1\n",
    "predictions = list(logregr.predict(test))\n",
    "# Method 2\n",
    "predictions2 = list(logi.predict(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.943\n"
     ]
    }
   ],
   "source": [
    "# Method 1\n",
    "print(\"Accuracy = %.3f\" % (sum([p==a for p, a in zip(predictions, actuals)])/len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.914\n"
     ]
    }
   ],
   "source": [
    "# Method 2\n",
    "print(\"Accuracy = %.3f\" % (sum([p==a for p, a in zip(predictions2, actuals)])/len(predictions2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9142857142857143"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Accuracy/Alternative accuracy formula\n",
    "logi_acc = logi.score(test,test_labels)\n",
    "logi_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "ConfusionMatrix = collections.namedtuple('conf', ['tp','fp','tn','fn']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ConfusionMatrix(actuals, scores, threshold=0.22, positive_label=2):\n",
    "    tp=fp=tn=fn=0\n",
    "    bool_actuals = [act==positive_label for act in actuals]\n",
    "    for truth, score in zip(bool_actuals, scores):\n",
    "        if score > threshold:                      # predicted positive \n",
    "            if truth:                              # actually positive \n",
    "                tp += 1\n",
    "            else:                                  # actually negative              \n",
    "                fp += 1          \n",
    "        else:                                      # predicted negative \n",
    "            if not truth:                          # actually negative \n",
    "                tn += 1                          \n",
    "            else:                                  # actually positive \n",
    "                fn += 1\n",
    "\n",
    "    return ConfusionMatrix(tp, fp, tn, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACC(conf_mtrx):\n",
    "    return (conf_mtrx.tp + conf_mtrx.tn) / (conf_mtrx.fp + conf_mtrx.tn + conf_mtrx.tp + conf_mtrx.fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FPR(conf_mtrx):\n",
    "    return conf_mtrx.fp / (conf_mtrx.fp + conf_mtrx.tn) if (conf_mtrx.fp + conf_mtrx.tn)!=0 else 0\n",
    "def TPR(conf_mtrx):\n",
    "    return conf_mtrx.tp / (conf_mtrx.tp + conf_mtrx.fn) if (conf_mtrx.tp + conf_mtrx.fn)!=0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(actuals, scores, **fxns):\n",
    "    # generate thresholds over score domain \n",
    "    low = min(scores)\n",
    "    high = max(scores)\n",
    "    step = (abs(low) + abs(high)) / 1000\n",
    "    thresholds = np.arange(low-step, high+step, step)\n",
    "\n",
    "    # calculate confusion matrices for all thresholds\n",
    "    confusionMatrices = []\n",
    "    for threshold in thresholds:\n",
    "        confusionMatrices.append(calc_ConfusionMatrix(actuals, scores, threshold, positive_label=1))\n",
    "\n",
    "    # apply functions to confusion matrices \n",
    "    results = {fname:list(map(fxn, confusionMatrices)) for fname, fxn in fxns.items()}\n",
    "\n",
    "    results[\"THR\"] = thresholds\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = apply(actuals,scores, ACC=ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "def ROC(actuals, scores):\n",
    "    return apply(sorted(actuals), sorted(scores), FPR=FPR, TPR=TPR)\n",
    "\n",
    "roc_curve = ROC(sorted(actuals),sorted(scores))\n",
    "# roc_curve['TPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x193532d8ee0>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQmklEQVR4nO3db2xd9XnA8e8TOwmE2AkQQ5x/JKMB7K1ka03aTf1DV3VNmLSoEtKAqqyoVYRWqr4ETVo7qXvRqprWVaWNIhShvmkmrahNp7RoU9UyibElTBRI0lA3tMRNaBxKSQiExPazF/fGvTiO73Fybcc/fz+Shc89J/bzw9GXw/G990RmIkma/ebN9ACSpNYw6JJUCIMuSYUw6JJUCIMuSYVon6lvvGzZsly7du1MfXtJmpWefvrp45nZNd6+GQv62rVr2bt370x9e0malSLiVxfa5yUXSSqEQZekQhh0SSqEQZekQhh0SSpE06BHxI6IOBYRz19gf0TE1yKiPyKejYh3tX5MSVIzVc7QHwU2TbB/M7C+/rEV+OaljyVJmqymz0PPzCciYu0Eh2wBvpW19+F9KiKWRkR3Zh5t1ZC6dC+/dpp/3XOY4ZGRmR5FmvP61l7DB24a97VBl6QVLyxaCRxu2B6oP3Ze0CNiK7WzeNasWdOCb62qvvvMr/nn/3wBgIgZHkaa4+7/4I2XbdDHy8O4d83IzO3AdoC+vj7vrDGNhkdq/7oP/uMmFra3zfA0kqZCK57lMgCsbtheBRxpwdeVJE1CK4K+C7i3/myX9wKvef1ckqZf00suEfFt4HZgWUQMAF8A5gNk5jZgN3AH0A+8Adw3VcNKki6syrNc7m6yP4HPtGwiSdJF8ZWiklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklQIgy5JhTDoklSISkGPiE0RcTAi+iPioXH2L4mI70fETyNiX0Tc1/pRJUkTaRr0iGgDHgY2A73A3RHRO+awzwD7M3MDcDvwTxGxoMWzSpImUOUMfSPQn5mHMvMMsBPYMuaYBDoiIoDFwG+BoZZOKkmaUJWgrwQON2wP1B9r9HWgBzgCPAd8LjNHxn6hiNgaEXsjYu/g4OBFjixJGk+VoMc4j+WY7Y8CzwArgD8Gvh4Rnef9ocztmdmXmX1dXV2THFWSNJEqQR8AVjdsr6J2Jt7oPuCxrOkHXgRuac2IkqQqqgR9D7A+ItbVf9F5F7BrzDEvAR8GiIjrgZuBQ60cVJI0sfZmB2TmUEQ8ADwOtAE7MnNfRNxf378N+CLwaEQ8R+0SzYOZeXwK55YkjdE06ACZuRvYPeaxbQ2fHwH+orWjSZImw1eKSlIhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFcKgS1IhDLokFaJS0CNiU0QcjIj+iHjoAsfcHhHPRMS+iPhJa8eUJDXT3uyAiGgDHgY+AgwAeyJiV2bubzhmKfANYFNmvhQR103RvJKkC6hyhr4R6M/MQ5l5BtgJbBlzzD3AY5n5EkBmHmvtmJKkZqoEfSVwuGF7oP5Yo5uAqyPixxHxdETcO94XioitEbE3IvYODg5e3MSSpHFVCXqM81iO2W4H3g38JfBR4O8j4qbz/lDm9szsy8y+rq6uSQ8rSbqwptfQqZ2Rr27YXgUcGeeY45l5CjgVEU8AG4AXWjKlJKmpKmfoe4D1EbEuIhYAdwG7xhzzPeD9EdEeEYuA9wAHWjuqJGkiTc/QM3MoIh4AHgfagB2ZuS8i7q/v35aZByLih8CzwAjwSGY+P5WDS5LersolFzJzN7B7zGPbxmx/BfhK60aTJE2GrxSVpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEJUCnpEbIqIgxHRHxEPTXDcbRExHBF3tm5ESVIVTYMeEW3Aw8BmoBe4OyJ6L3Dcl4HHWz2kJKm5KmfoG4H+zDyUmWeAncCWcY77LPAd4FgL55MkVVQl6CuBww3bA/XHRkXESuBjwLaJvlBEbI2IvRGxd3BwcLKzSpImUCXoMc5jOWb7q8CDmTk80RfKzO2Z2ZeZfV1dXRVHlCRV0V7hmAFgdcP2KuDImGP6gJ0RAbAMuCMihjLzu60YUpLUXJWg7wHWR8Q64NfAXcA9jQdk5rpzn0fEo8C/G3NJml5Ng56ZQxHxALVnr7QBOzJzX0TcX98/4XVzSdL0qHKGTmbuBnaPeWzckGfmJy99LEnSZPlKUUkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEIYdEkqhEGXpEJUCnpEbIqIgxHRHxEPjbP/4xHxbP3jyYjY0PpRJUkTaRr0iGgDHgY2A73A3RHRO+awF4EPZuatwBeB7a0eVJI0sSpn6BuB/sw8lJlngJ3AlsYDMvPJzHy1vvkUsKq1Y0qSmqkS9JXA4YbtgfpjF/Ip4Afj7YiIrRGxNyL2Dg4OVp9SktRUlaDHOI/luAdGfIha0B8cb39mbs/Mvszs6+rqqj6lJKmp9grHDACrG7ZXAUfGHhQRtwKPAJsz85XWjCdJqqrKGfoeYH1ErIuIBcBdwK7GAyJiDfAY8InMfKH1Y0qSmml6hp6ZQxHxAPA40AbsyMx9EXF/ff824PPAtcA3IgJgKDP7pm5sSdJYVS65kJm7gd1jHtvW8PmngU+3djRJ0mT4SlFJKoRBl6RCGHRJKoRBl6RCGHRJKoRBl6RCGHRJKoRBl6RCGHRJKoRBl6RCGHRJKoRBl6RCGHRJKoRBl6RCGHRJKoRBl6RCGHRJKoRBl6RCGPQ5YGh4hBNvnp3pMSRNsUr3FNXsceL0WQ4cOcGBoyfYf/QEB46e5OBvTnJmaIQr5s+jrXYTb0kFMuizVGYy8Oqb9WifYP+RWsAHXn1z9JhrrlpAb3cnf/OnN9C7opO+G66hvc3/KZNKZdBngdNnh/n5b14fPes+F/GTp4cAiIB1117FhtVLuXvjGnq7O+ld0cl1HQsJz8ilOcOgX2Zeef2tt511Hzh6kv7B1xkeSQAWLWjjluUd/NWGFfSu6KSnu5NblnewaIE/SmmuswIzZHgk+eUrp+rRrp95HznBsZNvjR7TveQKero7+Ujv9fTUz7pvuGYR8+Z51i3pfAZ9Gpx6a4ifvXyC/UdPjgb84MsnefPsMADt84J3XLeY971j2ehZd093J9dctWCGJ5c0mxj0FspMXj5x+m1n3QeOnuSXr5wia1dMWHLlfHq6O7h74xp6ujvoXdHJO65bzML2tpkdXtKsZ9Av0pmhEX4x+Pros0vOBfx3b/z++d43XLuInuWdfOxPVo5eMlmx5Ap/USlpShj0Cn73xpnRs+1zZ98/P3aSs8O10+6F7fO4ZXkHm/9oeS3c3Z3cvLyDjivmz/DkkuYSg95gZCQ5/Oob5/2i8shrp0eP6epYSE93Jx+4qYue7g7+cEUna6+9yud3S5pxczbop88Oc/Dlk6PRPlC/bHLqTO0XlfMCbuxazG3rrhk96+7p7qSrY+EMTy5J45sTQT928vToc7rPXe8+NPg69ad2s3hhOz3dHdz57lWj17pvur6DK+b7i0pJs0eloEfEJuBfgDbgkcz80pj9Ud9/B/AG8MnM/L8Wz9rU0PAILx4/NfpqynNn3sdfPzN6zMqlV9K7opM73tlde0Vldyerrr7S53ZLmvWaBj0i2oCHgY8AA8CeiNiVmfsbDtsMrK9/vAf4Zv2fU+bE6bP87OjJ37+i8uXac7vfGhoBYEHbPNZfv5gP3Xzd6Fl3z/JOlizyF5WSylTlDH0j0J+ZhwAiYiewBWgM+hbgW5mZwFMRsTQiujPzaKsH/tHPfsM/7NrPS799Y/Sxc29CdW/9Tah6uju5sWsx8/1FpaQ5pErQVwKHG7YHOP/se7xjVgJvC3pEbAW2AqxZs2ayswKwbPFC3rlqCX9922rfhEqSGlQJ+nilzIs4hszcDmwH6OvrO29/FbeuWsrD97zrYv6oJBWtyjWJAWB1w/Yq4MhFHCNJmkJVgr4HWB8R6yJiAXAXsGvMMbuAe6PmvcBrU3H9XJJ0YU0vuWTmUEQ8ADxO7WmLOzJzX0TcX9+/DdhN7SmL/dSetnjf1I0sSRpPpeehZ+ZuatFufGxbw+cJfKa1o0mSJsPn9UlSIQy6JBXCoEtSIQy6JBUiMi/q9T2X/o0jBoFfXeQfXwYcb+E4s4Frnhtc89xwKWu+ITO7xtsxY0G/FBGxNzP7ZnqO6eSa5wbXPDdM1Zq95CJJhTDoklSI2Rr07TM9wAxwzXODa54bpmTNs/IauiTpfLP1DF2SNIZBl6RCXNZBj4hNEXEwIvoj4qFx9kdEfK2+/9mImPV3vqiw5o/X1/psRDwZERtmYs5WarbmhuNui4jhiLhzOuebClXWHBG3R8QzEbEvIn4y3TO2WoW/20si4vsR8dP6mmf1u7ZGxI6IOBYRz19gf+v7lZmX5Qe1t+r9BfAHwALgp0DvmGPuAH5A7Y5J7wX+Z6bnnoY1/xlwdf3zzXNhzQ3H/Yjau37eOdNzT8PPeSm1+/auqW9fN9NzT8Oa/w74cv3zLuC3wIKZnv0S1vwB4F3A8xfY3/J+Xc5n6KM3p87MM8C5m1M3Gr05dWY+BSyNiO7pHrSFmq45M5/MzFfrm09RuzvUbFbl5wzwWeA7wLHpHG6KVFnzPcBjmfkSQGbO9nVXWXMCHVG7QfBiakEfmt4xWyczn6C2hgtpeb8u56Bf6MbTkz1mNpnsej5F7b/ws1nTNUfESuBjwDbKUOXnfBNwdUT8OCKejoh7p226qVFlzV8HeqjdvvI54HOZOTI9482Ilver0g0uZkjLbk49i1ReT0R8iFrQ3zelE029Kmv+KvBgZg7XTt5mvSprbgfeDXwYuBL474h4KjNfmOrhpkiVNX8UeAb4c+BG4D8i4r8y88QUzzZTWt6vyznoc/Hm1JXWExG3Ao8AmzPzlWmabapUWXMfsLMe82XAHRExlJnfnZYJW6/q3+3jmXkKOBURTwAbgNka9Cprvg/4UtYuMPdHxIvALcD/Ts+I067l/bqcL7nMxZtTN11zRKwBHgM+MYvP1ho1XXNmrsvMtZm5Fvg34G9nccyh2t/t7wHvj4j2iFgEvAc4MM1ztlKVNb9E7f9IiIjrgZuBQ9M65fRqeb8u2zP0nIM3p6645s8D1wLfqJ+xDuUsfqe6imsuSpU1Z+aBiPgh8CwwAjySmeM+/W02qPhz/iLwaEQ8R+1yxIOZOWvfVjcivg3cDiyLiAHgC8B8mLp++dJ/SSrE5XzJRZI0CQZdkgph0CWpEAZdkgph0CWpEAZdkgph0CWpEP8PTb7WAEiGEeIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot ROC curve\n",
    "plt.plot(roc_curve['FPR'],roc_curve['TPR'])\n",
    "# plt.plot(np.dot(roc_curve['FPR'],len(test)/6),roc_curve['TPR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make Predictions with k-nearest neighbors on the Iris Flowers Dataset\n",
    "# from csv import reader\n",
    "# from math import sqrt\n",
    " \n",
    "# # Load a CSV file\n",
    "# def load_csv(filename):\n",
    "# \tdataset = list()\n",
    "# \twith open(filename, 'r') as file:\n",
    "# \t\tcsv_reader = reader(file)\n",
    "# \t\tfor row in csv_reader:\n",
    "# \t\t\tif not row:\n",
    "# \t\t\t\tcontinue\n",
    "# \t\t\tdataset.append(row)\n",
    "# \treturn dataset\n",
    " \n",
    "# # Convert string column to float\n",
    "# def str_column_to_float(dataset, column):\n",
    "# \tfor row in dataset:\n",
    "# \t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# # Convert string column to integer\n",
    "# def str_column_to_int(dataset, column):\n",
    "# \tclass_values = [row[column] for row in dataset]\n",
    "# \tunique = set(class_values)\n",
    "# \tlookup = dict()\n",
    "# \tfor i, value in enumerate(unique):\n",
    "# \t\tlookup[value] = i\n",
    "# \t\tprint('[%s] => %d' % (value, i))\n",
    "# \tfor row in dataset:\n",
    "# \t\trow[column] = lookup[row[column]]\n",
    "# \treturn lookup\n",
    " \n",
    "# # Find the min and max values for each column\n",
    "# # def dataset_minmax(dataset):\n",
    "# # \tminmax = list()\n",
    "# # \tfor i in range(len(dataset[0])):\n",
    "# # \t\tcol_values = [row[i] for row in dataset]\n",
    "# # \t\tvalue_min = min(col_values)\n",
    "# # \t\tvalue_max = max(col_values)\n",
    "# # \t\tminmax.append([value_min, value_max])\n",
    "# # \treturn minmax\n",
    "\n",
    "# # Find the min and max values for each column\n",
    "# def dataset_mean_std(dataset):\n",
    "# \tmean_std = list()\n",
    "# \tfor i in range(len(dataset[0])):\n",
    "# \t\tcol_values = [row[i] for row in dataset]\n",
    "# \t\tvalue_mean = np.mean(col_values)\n",
    "# \t\tvalue_std = np.std(col_values)\n",
    "# \t\tmean_std.append([value_mean, value_std])\n",
    "# \treturn mean_std\n",
    "\n",
    "# # Rescale dataset columns to the range 0-1\n",
    "# # def normalize_dataset(dataset, minmax):\n",
    "# # \tfor row in dataset:\n",
    "# # \t\tfor i in range(len(row)):\n",
    "# # \t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "            \n",
    "# # Rescale dataset columns to the range 0-1\n",
    "# def normalize_dataset(dataset, mean_std):\n",
    "# \tfor row in dataset:\n",
    "# \t\tfor i in range(len(row)):\n",
    "# \t\t\trow[i] = (row[i] - mean_std[i][0]) / (mean_std[i][1])\n",
    " \n",
    " \n",
    "# # Calculate the Euclidean distance between two vectors\n",
    "# def euclidean_distance(row1, row2):\n",
    "# \tdistance = 0.0\n",
    "# \tfor i in range(len(row1)-1):\n",
    "# \t\tdistance += (row1[i] - row2[i])**2\n",
    "# \treturn sqrt(distance)\n",
    " \n",
    "# # Locate the most similar neighbors\n",
    "# def get_neighbors(train, test_row, num_neighbors):\n",
    "# \tdistances = list()\n",
    "# \tfor train_row in train:\n",
    "# \t\tdist = euclidean_distance(test_row, train_row)\n",
    "# \t\tdistances.append((train_row, dist))\n",
    "# \tdistances.sort(key=lambda tup: tup[1])\n",
    "# \tneighbors = list()\n",
    "# \tfor i in range(num_neighbors):\n",
    "# \t\tneighbors.append(distances[i][0])\n",
    "# \treturn neighbors\n",
    " \n",
    "# # Make a prediction with neighbors\n",
    "# def predict_classification(train, test_row, num_neighbors):\n",
    "# \tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "# \toutput_values = [row[-1] for row in neighbors]\n",
    "# \tprediction = max(set(output_values), key=output_values.count)\n",
    "# \treturn prediction\n",
    " \n",
    "# # Make a prediction with KNN on Iris Dataset\n",
    "# filename = 'iris.csv'\n",
    "# dataset = load_csv(filename)\n",
    "# for i in range(len(dataset[0])-1):\n",
    "# \tstr_column_to_float(dataset, i)\n",
    "# # convert class column to integers\n",
    "# str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# # define model parameter\n",
    "# num_neighbors = 5\n",
    "# # define a new record\n",
    "# row = [4.7,7.9,3.2,0.3]\n",
    "# row1 = [0.7,1.5,8.4,2.0]\n",
    "# # predict the label\n",
    "# label = predict_classification(dataset, row1, num_neighbors)\n",
    "# print('Data=%s, Predicted: %s' % (row, label))\n",
    "# # print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19353320850>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYGklEQVR4nO3dfZBUV5nH8e/T3fMCwztMEl4DSSCRjZKYMS9GE92IkqjLaq0lcTUaTVEpE5NdtRSt1XVLt2otLTW7RlmMMVomoVxFwyprsupu4m42EfJKACGEGBgYwkAyAwx0T788+0f3zDRNz0wDPfT0ub9PVYe5957ufrg586vD6XP7mrsjIiL1L1brAkREpDoU6CIigVCgi4gEQoEuIhIIBbqISCAStXrjadOm+dy5c2v19iIidemJJ57Y7+6t5Y7VLNDnzp3Lhg0bavX2IiJ1ycxeGuyYplxERAKhQBcRCYQCXUQkEAp0EZFAKNBFRAIxbKCb2d1mts/MnhvkuJnZP5vZdjN71sxeX/0yRURkOJWM0O8Blgxx/FpgfuGxHPjuqZclIiInath16O7+iJnNHaLJUuBHnv8e3sfMbJKZTXf3jmoVKSKjUy7n9GZz+UcmRzqbI5tz3CGbc3JFX8/tgLuTyXn+WA7SuRyZrJPJ5sh6/nl97bzwpJw7OYdM0ftkc15oB07+eK7vdQvv2ffWhVcq2Xfs9kCNfty+gecO1FfuuaWvc+xzB56HO21zp3DVgrLXBp2SalxYNBPYVbTdXth3XKCb2XLyo3jmzJlThbcWkXQ2R0dXkr0Hk2Rzfsz+I71ZjqYz9KSypAqB25vJkcpk6UllOZTMcDiV5khvtj+Qe7M5UukcqUyOZDpLb1FIuztZzwdnJjd4+MnQbr763FEb6FZmX9n/ze6+ClgF0NbWpq4gcoLcnef3HeaRbZ38/vn9vNB5mI7uY4O8EjGDcU0Jxjc3MK4pwZjGOI2JGC1NCSbHYzQ1xGhKxGluiNEQjxEzKzwgFjMSMSNeeDTEYzQlYjQmYiRiMRIxIxYrtDXDrPh9B54XNyMRzz+/b59BoX3+eTHL74uZ0ZDIt20stDcDK7Qzg3jhtWOF1wGwwpsXh1RfPX2trEyC9b12+f3lX7dc22O3h2pdHdUI9HZgdtH2LGBPFV5XJLLcnYNHM7R3HeGFzh627zvMC/sO8+TOV+noTgJw3hnjuOTsycyZMpbZk8cyfVIzidjAx2KJuDG2Mc7YxgRjG+M0J+I0JKwoEEc+YOT0qkagrwVuNbPVwGVAt+bPRYaWzuZ4+WCSvd1Jdncdpf3VvscR9nQdpaM7yZHebH/7mMGcKWO5eM4kbp/fypsXtDJz0pga/g1kNBo20M3sfuAtwDQzawf+HmgAcPeVwDrgOmA7cAS4caSKFak3PakMt9z3JM+/fBjIj7x7s86BntRx889TWhqZOWkM888Yz1WFwJ4xaQzntLYwd2oLzQ3xGvwNpJ5Ussrl+mGOO3BL1SoSCYS783e/eI5HtnXy7kUzSMRimEEiZpwxoZnpE/OPGZPGMHPSGFqaavblpxII9SCREfKTDbv4+VO7+eTiBdx2zfxalyMRoEv/RUbAlo6DfPGBTbzpvGnc8tbzal2ORIQCXaTKDqcy3HLvk0wY08A3338R8ZhWk8jpoUAXqaKXDvTw0R+s508Herhj2UW0jm+qdUkSIZpDF6mCTDbH9//nRb75m200xGJ8/X2LeOO502pdlkSMAl3kFCTTWX67ZR8rH36Bjbu7WbzwTL689ELOmthc69IkghToIifh2fYu7nt8J7/a2MGhZIbpE5u58wOv57rXnqUrMKVmFOgiJ2hjezfv/c6jNCZiLLnwLN578SyuOHeqPvyUmlOgi5yAI70Zbl/9FK3jm1h325uZ3NJY65JE+inQRU7AV361hRcP9HDvTZcpzGXU0bJFkQo9tGkv9z2+k+VXnaMVLDIqKdBFKrDvUJIVazbyZzMm8KnF59e6HJGyFOgiFfjGQ9voSWW4Y9lFNCb0ayOjk3qmSAX+8OIrXL2glfPOGF/rUkQGpUAXGUb3kTQ79vewaPakWpciMiQFusgwnmnvAuAiBbqMcgp0kWE8s6sLM3jtrIm1LkVkSAp0kWE8097Fua3jmNDcUOtSRIakQBcZgrvz9K5uFs2aVOtSRIalQBcZwp7uJPsPp7hotqZbZPRToIsM4ZldXQBa4SJ1QYEuMoRndnXRGI9xwVkTal2KyLAU6CJDeHpXFwtnTNDVoVIX1EtFBpHNORt3d2v9udQNBbrIIJ7fd4gjvVkW6QNRqRMKdJFB9H8gqiWLUicU6CKDeHpXNxOaE8yd2lLrUkQqokAXGcQzu7pYNHsSMd0rVOqEAl2kjKO9Wba+fEjTLVJXFOgiZTy8bR/ZnOuCIqkrCnSREqv/sJNP3P8U50xr4Ypzp9a6HJGKVRToZrbEzLaa2XYzW1Hm+EQz+3cze8bMNpnZjdUvVWRkZbI5vrR2EyvWbOTyc6by849fybimRK3LEqnYsL3VzOLAncBioB1Yb2Zr3X1zUbNbgM3u/m4zawW2mtm97t47IlWLjICP3/skD21+mY+9aR6fu/YCEnH9A1bqSyU99lJgu7vvKAT0amBpSRsHxpuZAeOAV4BMVSsVGUE9qQwPbX6Zj145jy+8a6HCXOpSJb12JrCraLu9sK/Yt4HXAHuAjcDt7p4rfSEzW25mG8xsQ2dn50mWLFJ9Hd1JAF6nuxJJHask0MstwvWS7XcATwMzgIuAb5vZcV9P5+6r3L3N3dtaW1tPsFSRkbO3EOhnTWyucSUiJ6+SQG8HZhdtzyI/Ei92I7DG87YDLwIXVKdEkZHX0X0UgOkKdKljlQT6emC+mc0zs0ZgGbC2pM1O4BoAMzsTOB/YUc1CRUZS35TLmRMU6FK/hl3l4u4ZM7sVeBCIA3e7+yYzu7lwfCXwZeAeM9tIforms+6+fwTrFqmqju4kU1saaW6I17oUkZNW0SJbd18HrCvZt7Lo5z3A26tbmsjps7f7qObPpe5pbZYI+RG65s+l3inQRYC9B5MaoUvdU6BL5B3tzdJ1JM30iWNqXYrIKVGgS+RpyaKEQoEukaeLiiQUCnSJvL416JpykXqnQJfI05SLhEKBLpHX0Z1k8tgGXVQkdU+BLpG3tzvJWZpukQAo0CXydFGRhEKBLpHX0X1UgS5BUKBLpCXTWV49klagSxAU6BJpA2vQNYcu9U+BLpHWtwZ9hkboEgAFukRa3xp0XSUqIVCgS6R16LJ/CYgCXSJtb3eSiWMaGNtY0b1eREY1BbpEmtagS0gU6BJpew9qDbqEQ4EukdbRpcv+JRwKdImsZDrLgZ5ejdAlGAp0iax9B1OAvjZXwqFAl8ga+B50TblIGBToEllagy6hUaBLZA3cek6BLmFQoEtk7e0+yoTmBC1NuqhIwqBAl8ja39PLtHFNtS5DpGoU6BJZqXRW9xGVoCjQJbJSmRzNDfoVkHCoN0tkpdI5mhIaoUs4Kgp0M1tiZlvNbLuZrRikzVvM7Gkz22RmD1e3TJHqS2ayNGmELgEZ9uN9M4sDdwKLgXZgvZmtdffNRW0mAd8Blrj7TjM7Y4TqFamaVDpHs0boEpBKhieXAtvdfYe79wKrgaUlbT4ArHH3nQDuvq+6ZYpUX0ojdAlMJb15JrCraLu9sK/YAmCymf23mT1hZjeUeyEzW25mG8xsQ2dn58lVLFIlyXSOpoQCXcJRSW+2Mvu8ZDsBXAK8E3gH8AUzW3Dck9xXuXubu7e1traecLEi1ZTKaNmihKWSS+TagdlF27OAPWXa7Hf3HqDHzB4BFgHbqlKlyAhIZTRCl7BU0pvXA/PNbJ6ZNQLLgLUlbR4A3mxmCTMbC1wGbKluqSLV4+4k01ktW5SgDDtCd/eMmd0KPAjEgbvdfZOZ3Vw4vtLdt5jZr4FngRxwl7s/N5KFi5yKTM7JObqwSIJS0bcSufs6YF3JvpUl218Dvla90kRGTjKdBdAIXYKi4YlEUiqTA9CyRQmKerNEUl+g68IiCYkCXSKpf8pFI3QJiHqzRFIqXZhy0QhdAqJAl0hKZTRCl/CoN0skJftH6PoVkHCoN0sk9Y3Qdem/hESBLpHUv2xRI3QJiHqzRJIuLJIQKdAlkvrXoetDUQmIerNE0sCUi0boEg4FukRSShcWSYDUmyWSdOm/hEiBLpGUSmcxg4Z4uRtyidQnBbpEUrJwtyIzBbqEQ4EukZRK636iEh4FukRSMq37iUp41KMlklIZ3U9UwqNAl0hKZXK6qEiCox4tkZRMa4Qu4VGgSySlMppDl/CoR0sk5adcNEKXsCjQJZLyUy7q/hIW9WiJpFQmp+9xkeCoR0skpTJZfY+LBEeBLpGUTGuELuFRj5ZISmnZogRIgS6RpDl0CZF6tESOuxfWoWuELmFRoEvkDNx+Tt1fwqIeLZGTSvfdIFojdAlLRYFuZkvMbKuZbTezFUO0e4OZZc3sr6pXokh1pTKF+4lqhC6BGbZHm1kcuBO4FlgIXG9mCwdp91XgwWoXKVJN/fcT1QhdAlPJEOVSYLu773D3XmA1sLRMu08APwP2VbE+kapLpjVClzBV0qNnAruKttsL+/qZ2UzgPcDKoV7IzJab2QYz29DZ2XmitYpUhT4UlVBV0qPL3UXXS7a/BXzW3bNDvZC7r3L3Nndva21trbBEkerqm0PXlIuEJlFBm3ZgdtH2LGBPSZs2YHXhDurTgOvMLOPuv6hGkSLVlExrhC5hqiTQ1wPzzWwesBtYBnyguIG7z+v72czuAX6pMJfRqn+Vi0boEphhA93dM2Z2K/nVK3HgbnffZGY3F44POW8uMtoMrEPXCF3CUskIHXdfB6wr2Vc2yN39I6delsjISfavQ9cIXcKiIYpETkpz6BIo9WiJHF1YJKFSoEvk6MIiCZV6tESOLiySUKlHS+Qk01kSMSMRV/eXsKhHS+Tkb26hri/hUa+WyEllsrqoSIKkQJfISaZzNGuELgFSr5bIyd8gWiN0CY8CXSInlc5qDl2CpF4tkZPUCF0CpUCXyNEIXUKlXi2Ro2WLEir1aomcZDqr73GRICnQJXJ6NUKXQKlXS+Tkp1w0QpfwKNAlcvJTLur6Eh71aokcjdAlVAp0iZz8d7mo60t41KslUrI5J511mjVClwAp0CVSUn03iNYIXQKkXi2RkizcIFrftighUq+WSBkYoWvKRcKjQJdISaV1P1EJl3q1REqyMELXpf8SIgW6RIpG6BIy9WqJlFSmL9A1QpfwKNAlUpLpvikXdX0Jj3q1RIpG6BIyBbpEii4skpBV1KvNbImZbTWz7Wa2oszxvzazZwuPR81sUfVLFTl1AxcWaYQu4Rk20M0sDtwJXAssBK43s4UlzV4Ernb31wFfBlZVu1CRatAIXUJWSa++FNju7jvcvRdYDSwtbuDuj7r7q4XNx4BZ1S1TpDq0bFFCVkmvngnsKtpuL+wbzMeA/yh3wMyWm9kGM9vQ2dlZeZUiVaILiyRklQS6ldnnZRuavZV8oH+23HF3X+Xube7e1traWnmVIlXSN0JvjGuELuFJVNCmHZhdtD0L2FPayMxeB9wFXOvuB6pTnkh1JTNZGuMxYrFy4xSR+lbJMGU9MN/M5plZI7AMWFvcwMzmAGuAD7n7tuqXKVIdqXROH4hKsIYdobt7xsxuBR4E4sDd7r7JzG4uHF8JfBGYCnzHzAAy7t42cmWLnBzdT1RCVsmUC+6+DlhXsm9l0c83ATdVtzSR6kuls1rhIsFSz5ZISWVy+h4XCZZ6tkRKKpPVlIsES4EukZLUh6ISMPVsiZRUJqvvcZFgKdAlUlIZjdAlXOrZEilJrXKRgKlnS6TkV7loykXCpECXSEmlcxqhS7DUsyVSklq2KAFToEukpNK6sEjCpZ4tkeHuGqFL0BToEhnprOOORugSLPVsiYz++4lqhC6BUqBLZCT77ieqEboESj1bIqNvhK5L/yVUCnSJjFRGI3QJm3q2REYy3TeHrm4vYVLPlsgYGKFrykXCpECXyEj1fSiqEboESj1bIiOpZYsSOAW6RMbuV48CMLZRgS5hUqBLJHQfSfOt3zzPa2dOZMGZ42tdjsiISNS6AJHT4asP/pFXelLcc+MbiMes1uWIjAiN0CV4T7z0Kvc9vpMbr5zHhTMn1rockRGjQJegpbM5Pr9mIzMmNvPJxQtqXY7IiNKUiwTte7/fwdaXD/G9G9poaVJ3l7Cph0twejM5Htq8lx8/9hKP7XiFty88k8ULz6x1WSIjToEuwehJZfjB/77IPY++xP7DKWZNHsNnlpzPDVfMrXVpIqeFAl3qXm8mx/1/2Mm//O559h/u5a3nt3LDFXO5akGrVrRIpCjQpe48vuMAmzsO0tGdZE/XUZ7a2cXurqNcOm8K//qhC7jk7Mm1LlGkJhToUjcOJtN86YFNrHlqNwCNiRjTJzZzTmsL//ieC7l6QStmGpFLdFUU6Ga2BLgDiAN3ufs/lRy3wvHrgCPAR9z9ySrXKhH2fy8c4NP/9gx7Dya57Zr5fPiKs5nS0qgAFykybKCbWRy4E1gMtAPrzWytu28uanYtML/wuAz4buFPibBczkllciTTWZKZLEd7sxxKZgqPNEd6s/Rmc6TS+T/TWSebczI5pzeTo/toL6/05B8bXnqVuVNb+OnNV3DxHE2piJRTyQj9UmC7u+8AMLPVwFKgONCXAj9ydwceM7NJZjbd3TuqXfDD2zr5yi83D9/wJPiIvGqZ9/H83ee97+djjpXW5P37io+VPu+444Wj/c/t/3lgf/H7D7xH0ese81wv3nXMe5bWmPV8MJ+KhrgxcUwjU1oamDy2keVvPofb3zafsY2aJRQZTCW/HTOBXUXb7Rw/+i7XZiZwTKCb2XJgOcCcOXNOtFYAxjUlmH/muJN6biWM0/NPeDMwy79b6UKM0mkE6/9Pvj6zgf2lMw7F9fe3s6Kj1v9ShZ8H9hW/rxW930Db0tcbqLevRsOIxyBuRixmNCZiNCfiNDfEGdMYY3xTA+ObE0wY08DYxjhNiTiNiRiNiRiJmJGIGfGYaSpF5CRUEujlfrNKh1+VtMHdVwGrANra2k5qCHfJ2ZO55OxLTuapIiJBq+S7XNqB2UXbs4A9J9FGRERGUCWBvh6Yb2bzzKwRWAasLWmzFrjB8i4Hukdi/lxERAY37JSLu2fM7FbgQfLLFu92901mdnPh+EpgHfkli9vJL1u8ceRKFhGRcipaMuDu68iHdvG+lUU/O3BLdUsTEZEToe9DFxEJhAJdRCQQCnQRkUAo0EVEAmFeeq356Xpjs07gpZN8+jRgfxXLCZXO0/B0joanc1SZ03Weznb31nIHahbop8LMNrh7W63rGO10noanczQ8naPKjIbzpCkXEZFAKNBFRAJRr4G+qtYF1Amdp+HpHA1P56gyNT9PdTmHLiIix6vXEbqIiJRQoIuIBKLuAt3MlpjZVjPbbmYral3PaGBms83sv8xsi5ltMrPbC/unmNl/mtnzhT8jfzNOM4ub2VNm9svCts5RicItJH9qZn8s9KkrdJ6OZWZ/W/hde87M7jez5tFwjuoq0ItuWH0tsBC43swW1raqUSEDfMrdXwNcDtxSOC8rgN+6+3zgt4XtqLsd2FK0rXN0vDuAX7v7BcAi8udL56nAzGYCtwFt7n4h+a8VX8YoOEd1FegU3bDa3XuBvhtWR5q7d7j7k4WfD5H/BZxJ/tz8sNDsh8Bf1qTAUcLMZgHvBO4q2q1zVMTMJgBXAd8HcPded+9C56lUAhhjZglgLPk7tNX8HNVboA92M2opMLO5wMXA48CZfXeOKvx5Rg1LGw2+BXwGyBXt0zk61jlAJ/CDwtTUXWbWgs5TP3ffDXwd2Al0kL9D20OMgnNUb4Fe0c2oo8rMxgE/A/7G3Q/Wup7RxMzeBexz9ydqXcsolwBeD3zX3S8Geojw9Eo5hbnxpcA8YAbQYmYfrG1VefUW6LoZ9SDMrIF8mN/r7msKu182s+mF49OBfbWqbxS4EvgLM/sT+am6PzezH6NzVKodaHf3xwvbPyUf8DpPA94GvOjune6eBtYAb2QUnKN6C/RKblgdOWZm5Oc8t7j7N4oOrQU+XPj5w8ADp7u20cLdP+fus9x9Lvl+8zt3/yA6R8dw973ALjM7v7DrGmAzOk/FdgKXm9nYwu/eNeQ/t6r5Oaq7K0XN7Dryc6F9N6z+x9pWVHtm9ibg98BGBuaHP09+Hv0nwBzynfB97v5KTYocRczsLcCn3f1dZjYVnaNjmNlF5D84bgR2kL/pewydp35m9g/A+8mvMHsKuAkYR43PUd0FuoiIlFdvUy4iIjIIBbqISCAU6CIigVCgi4gEQoEuIhIIBbqISCAU6CIigfh/ReI74IW1Zo4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sorted(h))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
