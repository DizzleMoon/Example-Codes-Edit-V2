{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import urllib.request\n",
    "import requests\n",
    "import curl\n",
    "import pycurl\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# from IPython import qt\n",
    "from matplotlib.pyplot import figure\n",
    "from py.xml import raw\n",
    "from requests.api import get\n",
    "from matplotlib import pyplot as plt\n",
    "# from scratch.working_with_data import rescale\n",
    "# from scratch.multiple_regression import least_squares_fit, predict\n",
    "# from scratch.gradient_descent import gradient_step\n",
    "\n",
    "# from stats import mean, median, de_mean, standard_deviation, correlation\n",
    "# from gradient_descent import minimize_stochastic, maximize_stochastic, maximize_batch\n",
    "# from vector import dot, vector_add\n",
    "# from normal import normal_cdf\n",
    "# from matrix import make_matrix, get_column, shape, matrix_multiply\n",
    "# from logistic_regression import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import partial, reduce\n",
    "\n",
    "from scipy.optimize import fmin_tnc\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from typing import*\n",
    "\n",
    "from collections import*\n",
    "# from scipy import*\n",
    "from sklearn.metrics import*\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "# bltin_sum = np.sum\n",
    "\n",
    "import random\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn import*\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# def add(a, b): return a + b\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "Tensor = list\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c , v):\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "#     return np.sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "#     gen = \n",
    "    return np.sum(np.fromiter((v_i * w_i for v_i, w_i in zip(v, w)),float))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = data.shape[0]\n",
    "    \n",
    "    # Vector Mean\n",
    "#     n = len(data)\n",
    "#     m = np.sum(data,axis=0)\n",
    "#     means = np.multiply(1/n,m)\n",
    "    means = vector_mean(data)\n",
    "    \n",
    "    # Standard Deviaiton\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "    return means,stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = data.shape[0]\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    grad_step = np.add(v,step)\n",
    "    return grad_step\n",
    "\n",
    "# def predict(alpha, beta, x_i):\n",
    "#     pred = beta * x_i + alpha\n",
    "#     return pred\n",
    "\n",
    "# def error(x, y, beta):\n",
    "#     \"\"\"\n",
    "#     The error from predicting beta * x_i + alpha\n",
    "#     when the actual value is y_i\n",
    "#     \"\"\"\n",
    "#     err_fin = predict(alpha, beta, x_i) - y_i\n",
    "#     return err_fin\n",
    "\n",
    "def predict(x, beta):\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def error(x, y, beta):\n",
    "    return predict(x, beta) - y \n",
    "\n",
    "def sqerror_gradient(x, y, beta):\n",
    "    err = error(x, y, beta)\n",
    "    err_fin = [2 * err * x_i for x_i in x]\n",
    "    return err_fin\n",
    "\n",
    "def least_squares_fit(xs, ys, learning_rate = 0.001, num_steps = 1000, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [np.random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def _negative_log_likelihood(x, y, beta):\n",
    "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))\n",
    "    \n",
    "def negative_log_likelihood(xs, ys, beta):\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))\n",
    "\n",
    "def _negative_log_partial_j(x, y, beta, j):\n",
    "    \"\"\"\n",
    "    The jth partial derivative for one data point.\n",
    "    Here i is the index of the data point.\n",
    "    \"\"\"\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x, y, beta):\n",
    "    \"\"\"\n",
    "    The gradient for one data point.\n",
    "    \"\"\"\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs, ys,beta):\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "                                                                \n",
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def sigmoid(t: float) -> float: \n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "# Gradient Descent - step\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "def squared_distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\"\n",
    "    return sum_of_squares(subtract(v, w))\n",
    "\n",
    "def distance(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes the distance between v and w\"\"\"\n",
    "    return math.sqrt(squared_distance(v, w))\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "\n",
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor.\n",
    "    Otherwise, tensor is 1-dimensional (that is, a vector).\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)  # just a list of floats, use Python sum\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
    "                   for tensor_i in tensor)   # and sum up those results.\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f elementwise\"\"\" \n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor] \n",
    "    \n",
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "\n",
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                   t1: Tensor,\n",
    "                   t2: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]\n",
    "    \n",
    "def inverse_normal_cdf(p: float,\n",
    "                       mu: float = 0,\n",
    "                       sigma: float = 1,\n",
    "                       tolerance: float = 0.00001) -> float:\n",
    "    \"\"\"Find approximate inverse using binary search\"\"\"\n",
    "\n",
    "    # if not standard, compute standard and rescale\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "\n",
    "    low_z = -10.0                      # normal_cdf(-10) is (very close to) 0\n",
    "    hi_z  =  10.0                      # normal_cdf(10)  is (very close to) 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2     # Consider the midpoint\n",
    "        mid_p = normal_cdf(mid_z)      # and the CDF's value there\n",
    "        if mid_p < p:\n",
    "            low_z = mid_z              # Midpoint too low, search above it\n",
    "        else:\n",
    "            hi_z = mid_z               # Midpoint too high, search below it\n",
    "\n",
    "    return mid_z\n",
    "\n",
    "def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -> float:\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "\n",
    "def num_differences(v1, v2):\n",
    "    assert len(v1) == len(v2)\n",
    "    return len([x1 for x1, x2 in zip(v1, v2) if x1 != x2])\n",
    "\n",
    "\n",
    "def cluster_means(k: int,\n",
    "                  inputs: List[Vector],\n",
    "                  assignments: List[int]) -> List[Vector]:\n",
    "    # clusters[i] contains the inputs whose assignment is i\n",
    "    clusters = [[] for i in range(k)]\n",
    "    for input, assignment in zip(inputs, assignments):\n",
    "        clusters[assignment].append(input)\n",
    "\n",
    "    # if a cluster is empty, just use a random point\n",
    "    return [vector_mean(cluster) if cluster else random.choice(inputs)\n",
    "            for cluster in clusters]\n",
    "\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k: int) -> None:\n",
    "        self.k = k                      # number of clusters\n",
    "        self.means = None\n",
    "\n",
    "    def classify(self, input: Vector) -> int:\n",
    "        \"\"\"return the index of the cluster closest to the input\"\"\"\n",
    "        return min(range(self.k),\n",
    "                   key=lambda i: squared_distance(input, self.means[i]))\n",
    "\n",
    "    def train(self, inputs: List[Vector]) -> None:\n",
    "        # Start with random assignments\n",
    "        assignments = [random.randrange(self.k) for _ in inputs]\n",
    "\n",
    "        with tqdm.tqdm(itertools.count()) as t:\n",
    "            for _ in t:\n",
    "                # Compute means and find new assignments\n",
    "                self.means = cluster_means(self.k, inputs, assignments)\n",
    "                new_assignments = [self.classify(input) for input in inputs]\n",
    "\n",
    "                # Check how many assignments changed and if we're done\n",
    "                num_changed = num_differences(assignments, new_assignments)\n",
    "                if num_changed == 0:\n",
    "                    return\n",
    "\n",
    "                # Otherwise keep the new assignments, and compute new means\n",
    "                assignments = new_assignments\n",
    "                self.means = cluster_means(self.k, inputs, assignments)\n",
    "                t.set_description(f\"changed: {num_changed} / {len(inputs)}\")\n",
    "                \n",
    "def squared_clustering_errors(inputs, k):\n",
    "    \"\"\"finds the total squared error from k-means clustering the\n",
    "inputs\"\"\"\n",
    "    clusterer = KMeans(k)\n",
    "    clusterer.train(inputs)\n",
    "    means = clusterer.means\n",
    "    assignments = [clusterer.classify(input) for input in inputs]\n",
    "\n",
    "    return sum(squared_distance(input, means[cluster])\n",
    "               for input, cluster in zip(inputs, assignments))\n",
    "\n",
    "def recolor(pixel):\n",
    "    cluster = clusterer.classify(pixel)        # index of the closest cluster\n",
    "    return clusterer.means[cluster]            # mean of the closest cluster\n",
    "\n",
    "def get_values(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        return [cluster.value]\n",
    "    else:\n",
    "        return [value\n",
    "                for child in cluster.children\n",
    "                for value in get_values(child)]\n",
    "\n",
    "\n",
    "def cluster_distance(cluster1,\n",
    "                     cluster2,\n",
    "                     distance_agg: Callable = min):\n",
    "    \"\"\"\n",
    "    compute all the pairwise distances between cluster1 and cluster2\n",
    "    and apply the aggregation function _distance_agg_ to the resulting\n",
    "list\n",
    "    \"\"\"\n",
    "    return distance_agg([distance(v1, v2) for v1 in get_values(cluster1) for v2 in get_values(cluster2)])\n",
    "\n",
    "def get_merge_order(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        return float('inf')  # was never merged\n",
    "    else:\n",
    "        return cluster.order\n",
    "    \n",
    "def get_children(cluster):\n",
    "    if isinstance(cluster, Leaf):\n",
    "        raise TypeError(\"Leaf has no children\")\n",
    "    else:\n",
    "        return cluster.children\n",
    "    \n",
    "class Merged(NamedTuple):\n",
    "    children: tuple\n",
    "    order: int\n",
    "\n",
    "def bottom_up_cluster(inputs: List[Vector],\n",
    "                      distance_agg: Callable = min):\n",
    "    # Start with all leaves\n",
    "    clusters: List[Cluster] = [Leaf(input) for input in inputs]\n",
    "\n",
    "    def pair_distance(pair):\n",
    "        return cluster_distance(pair[0], pair[1], distance_agg)\n",
    " \n",
    "    # as long as we have more than one cluster left...\n",
    "    while len(clusters) > 1:\n",
    "        # find the two closest clusters\n",
    "        c1, c2 = min(((cluster1, cluster2) \n",
    "                      for i, cluster1 in enumerate(clusters)\n",
    "                      for cluster2 in clusters[:i]),\n",
    "                      key=pair_distance)\n",
    "\n",
    "        # remove them from the list of clusters\n",
    "        clusters = [c for c in clusters if c != c1 and c != c2]\n",
    "\n",
    "        # merge them, using merge_order = # of clusters left\n",
    "        merged_cluster = Merged((c1, c2), order=len(clusters))\n",
    "\n",
    "        # and add their merge\n",
    "        clusters.append(merged_cluster)\n",
    "\n",
    "    # when there's only one cluster left, return it\n",
    "    return clusters[0]\n",
    "\n",
    "def generate_clusters(base_cluster,\n",
    "                      num_clusters):\n",
    "    # start with a list with just the base cluster\n",
    "    clusters = [base_cluster]\n",
    "\n",
    "    # as long as we don't have enough clusters yet...\n",
    "    while len(clusters) < num_clusters:\n",
    "        # choose the last-merged of our clusters\n",
    "        next_cluster = min(clusters, key=get_merge_order)\n",
    "        # remove it from the list\n",
    "        clusters = [c for c in clusters if c != next_cluster]\n",
    "\n",
    "        # and add its children to the list (i.e., unmerge it)\n",
    "        clusters.extend(get_children(next_cluster))\n",
    "\n",
    "    # once we have enough clusters...\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import some data to play with\n",
    "# iris = datasets.load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# # Binarize the output\n",
    "# y = label_binarize(y, classes=[0, 1, 2])\n",
    "# n_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_3948/2505186207.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  rows = np.array(rows)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "rows = []\n",
    "\n",
    "filename = 'iris.csv'\n",
    "# reading csv file \n",
    "with open(filename, 'r') as csvfile: \n",
    "    # creating a csv reader object \n",
    "    csvreader = csv.reader(csvfile) \n",
    "#     iris_data = [parse_iris_row(row) for row in csvreader]\n",
    "\n",
    "# extracting each data row one by one\n",
    "    for row in csvreader: \n",
    "        rows.append(row) \n",
    "        \n",
    "# print(rows)\n",
    "\n",
    "def parse_iris_row(row):\n",
    "    \"\"\"\n",
    "    sepal_length, sepal_width, petal_length, petal_width, class\n",
    "    \"\"\"\n",
    "    measurements = [float(value) for value in row[:-1]]\n",
    "    # class is e.g. \"Iris-virginica\"; we just want \"virginica\"\n",
    "    label = [value.split('-')[-1]for value in row]\n",
    "\n",
    "    return label\n",
    "\n",
    "# with open('iris.data') as f:\n",
    "#     reader = csv.reader(f)\n",
    "# iris_data = [parse_iris_row(row) for row in rows]\n",
    "# for row in rows:\n",
    "#     print(row[:-1])\n",
    "rows = np.array(rows)\n",
    "label = rows[0][-1].split('-')[-1]\n",
    "label\n",
    "\n",
    "iris_data = [parse_iris_row(row) for row in rows]\n",
    "# iris_data\n",
    "\n",
    "iris_data_pd = pd.DataFrame(iris_data)\n",
    "iris_pd = iris_data_pd.iloc[:,0:4]\n",
    "iris_point = iris_pd.to_numpy().tolist()\n",
    "# iris_point = iris_pd.to_numpy()\n",
    "iris_point\n",
    "\n",
    "iris_target_pd = iris_data_pd.iloc[:,4]\n",
    "# iris_point = iris_pd.to_numpy().tolist()\n",
    "iris_target = iris_target_pd.to_numpy().tolist()\n",
    "\n",
    "\n",
    "for i in range(len(iris_target)):\n",
    "    if iris_target[i] == 'setosa':\n",
    "        iris_target[i] = 0\n",
    "    elif iris_target[i] == 'versicolor':\n",
    "        iris_target[i] = 1\n",
    "    elif iris_target[i] == 'virginica':\n",
    "        iris_target[i] = 2\n",
    "\n",
    "\n",
    "iris_point_2 = iris_point[:-1]\n",
    "iris_point_2\n",
    "iris_target_2 = iris_target[:-1]\n",
    "iris_actual = iris_target_2\n",
    "print(iris_actual)\n",
    "# type(iris_point_2)\n",
    "# iris_point_2\n",
    "# iris_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = load_breast_cancer()\n",
    "dataset2 = load_iris()\n",
    "\n",
    "# type(dataset2['data'])\n",
    "dataset2['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split2(xs, ys, test_pct):\n",
    "    # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split2(dataset['data'],\n",
    "                                                          dataset['target'],\n",
    "                                                          0.25)\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logregr: LogisticRegression()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 2., 0., 0., 1., 0., 2., 0., 1., 1., 2., 2., 1., 1., 2., 1., 0.,\n",
       "       0., 2., 0., 0., 0., 1., 2., 1., 2., 1., 2., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 2., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 2., 2., 1., 0.,\n",
       "       2., 2., 1., 1., 0., 0., 2., 0., 2., 0., 2., 1., 1., 2., 1., 0., 2.,\n",
       "       2., 0., 2., 1., 0., 2., 2., 1., 1., 0., 2., 1., 1., 2., 0., 2., 2.,\n",
       "       0., 0., 0., 2., 1., 1., 2., 0., 1., 1., 1., 0., 2., 1., 2., 2., 1.,\n",
       "       0., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.linear_model import *\n",
    "# from sklearn.model_selection import train_test_split\n",
    "dataset = load_breast_cancer()\n",
    "dataset2 = load_iris()\n",
    "\n",
    "def logistic_prime(x: float) -> float:\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "# Split data into train-test pools \n",
    "# train, test, train_labels, test_labels = train_test_split2(np.array(iris_point_2),\n",
    "#                                                           np.array(iris_target_2),\n",
    "#                                                           0.33)\n",
    "train1, test1, train1_labels, test1_labels = train_test_split(np.array(iris_point_2),\n",
    "                                                          np.array(iris_target_2),\n",
    "                                                          test_size=0.7)\n",
    "\n",
    "# # Train model \n",
    "logregr = LogisticRegression().fit(train1, train1_labels)\n",
    "print('logregr:', logregr)\n",
    "\n",
    "# ab_x_tst = []\n",
    "# ab_y_tst = []\n",
    "# for i in range(len(train)):\n",
    "#     alog = logistic(train[i][0])\n",
    "#     ab_y_tst.append(alog)\n",
    "#     alog_prime = logistic_prime(train[i][0])\n",
    "#     ab_x_tst.append(alog_prime)\n",
    "    \n",
    "# plt.scatter(ab_x_tst,ab_y_tst)\n",
    "\n",
    "type(dataset2['data'])\n",
    "# train1 = train.dtype('u8')\n",
    "train = train1.astype('float')\n",
    "test = test1.astype('float')\n",
    "train_labels = train1_labels.astype('float')\n",
    "test_labels = test1_labels.astype('float')\n",
    "# train = np.array(train,dtype('float'))\n",
    "# test = np.array(test,dtype('float'))\n",
    "# train_labels = np.array(train_labels,dtype('float'))\n",
    "# test_labels = np.array(test_labels,dtype('float'))\n",
    "# test\n",
    "\n",
    "# Train model \n",
    "# logregr = LogisticRegression().fit(train, train_labels)\n",
    "# print('logregr:', logregr)\n",
    "# test_labels.tolist()\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_test_split_v2(xs, ys, test_pct):\n",
    "#      # Generate the indices and split them\n",
    "#     idxs = [i for i in range(len(xs))]\n",
    "#     train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "#     return ([xs[i] for i in train_idxs],  # x_train \n",
    "#             [xs[i] for i in test_idxs],   # x_test\n",
    "#             [ys[i] for i in train_idxs],  # y_train\n",
    "#             [ys[i] for i in test_idxs])   # y_test\n",
    "\n",
    "# random.seed(0)\n",
    "# test_size = 0.33\n",
    "# x_train, x_test, y_train, y_test = train_test_split_v2(dataset['data'], dataset['target'],test_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def train_test_split2(xs, ys, test_pct):\n",
    "#     # Generate the indices and split them\n",
    "#     idxs = [i for i in range(len(xs))]\n",
    "#     train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "#     return ([xs[i] for i in train_idxs],  # x_train \n",
    "#             [xs[i] for i in test_idxs],   # x_test\n",
    "#             [ys[i] for i in train_idxs],  # y_train\n",
    "#             [ys[i] for i in test_idxs])   # y_test\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split2(dataset['data'],\n",
    "#                                                           dataset['target'],\n",
    "#                                                           test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x268f1ea2a90>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rename, listify \n",
    "# train, test, train_labels, test_labels \n",
    "actuals = list(test_labels)\n",
    "\n",
    "# Predict probablities of test data [0,1]\n",
    "scores = list(logregr.predict_proba(test)[:,1])\n",
    "# scores_2 = scores\n",
    "scores_full = list(logregr.predict_proba(test))\n",
    "\n",
    "index = 2\n",
    "ind2 = len(test[1])\n",
    "ind22 = ind2-2\n",
    "\n",
    "scores_2 = list(logregr.predict_proba(test)[:,ind22])\n",
    "\n",
    "# Equivalently\n",
    "import math\n",
    "def sigmoid(x): \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "# scores = [sigmoid(logregr.coef_@test_i + logregr.intercept_) for test_i in test]\n",
    "print(len(scores))\n",
    "# scores.shape\n",
    "# print(scores)\n",
    "# print(scores_2[104])\n",
    "# print(scores_full[ind2-1])\n",
    "# print(scores[index])\n",
    "# print(scores[ind2-1])\n",
    "# actuals\n",
    "# test\n",
    "# ind22\n",
    "plt.plot(sorted(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions with k-nearest neighbors on the Iris Flowers Dataset\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "# \t\tprint('[%s] => %d' % (value, i))\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "# Find the min and max values for each column\n",
    "# def dataset_minmax(dataset):\n",
    "# \tminmax = list()\n",
    "# \tfor i in range(len(dataset[0])):\n",
    "# \t\tcol_values = [row[i] for row in dataset]\n",
    "# \t\tvalue_min = min(col_values)\n",
    "# \t\tvalue_max = max(col_values)\n",
    "# \t\tminmax.append([value_min, value_max])\n",
    "# \treturn minmax\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_mean_std(dataset):\n",
    "\tmean_std = list()\n",
    "\tfor i in range(len(dataset[0])):\n",
    "\t\tcol_values = [row[i] for row in dataset]\n",
    "\t\tvalue_mean = np.mean(col_values)\n",
    "\t\tvalue_std = np.std(col_values)\n",
    "\t\tmean_std.append([value_mean, value_std])\n",
    "\treturn mean_std\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "# def normalize_dataset(dataset, minmax):\n",
    "# \tfor row in dataset:\n",
    "# \t\tfor i in range(len(row)):\n",
    "# \t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "            \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, mean_std):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)):\n",
    "\t\t\trow[i] = (row[i] - mean_std[i][0]) / (mean_std[i][1])\n",
    " \n",
    " \n",
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    " \n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors\n",
    " \n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction\n",
    " \n",
    "# Make a prediction with KNN on Iris Dataset\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# define model parameter\n",
    "num_neighbors = 3\n",
    "# define a new record\n",
    "row = [4.7,7.9,3.2,0.3]\n",
    "row1 = [0.7,1.5,8.4,2.0]\n",
    "# predict the label\n",
    "label = predict_classification(dataset, test[0].tolist(), num_neighbors)\n",
    "# print('Data=%s, Predicted: %s' % (row, label))\n",
    "# print(dataset)\n",
    "\n",
    "\n",
    "labels1 = []\n",
    "for j in range(len(test)):\n",
    "    labels1.append(predict_classification(dataset, test[j], num_neighbors))\n",
    "# labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionOVR(object):\n",
    "    def __init__(self, eta=0.1, n_iter=50):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        self.w = []\n",
    "        m = X.shape[0]\n",
    "\n",
    "        for i in np.unique(y):\n",
    "            y_copy = np.where(y == i, 1, 0)\n",
    "#             w = np.ones(X.shape[1])\n",
    "            w = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "            for _ in range(self.n_iter):\n",
    "                output = X.dot(w)\n",
    "                errors = y_copy - self._sigmoid(output)\n",
    "                w += self.eta / m * errors.dot(X)\n",
    "            self.w.append((w, i))\n",
    "        return self\n",
    "    \n",
    "#      def predict(self, X):\n",
    "#         output = np.insert(X, 0, 1, axis=1).dot(self.w)\n",
    "#         return (np.floor(self._sigmoid(output) + .5)).astype(int)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_one(i) for i in np.insert(X, 0, 1, axis=1)]\n",
    "    \n",
    "    def _predict_one(self, x):\n",
    "        return max((x.dot(w), c) for w, c in self.w)[1]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.sum(self.predict(X) == y) / len(y)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 1.00023881,  0.29627953,  1.92564133, -2.90270875, -1.01109027]),\n",
       "  0.0),\n",
       " (array([ 1.31881842,  0.8512732 , -2.20636545,  0.14255992, -1.06879364]),\n",
       "  1.0),\n",
       " (array([-0.42498545, -2.19485993, -2.03082358,  3.2961568 ,  2.68164414]),\n",
       "  2.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi = LogisticRegressionOVR(n_iter=1000).fit(train, train_labels)\n",
    "log_sig = logi._sigmoid(test)\n",
    "log_sig\n",
    "logi.w\n",
    "# plt.plot(log_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict binary outcomes (0,1)\n",
    "predictions = list(logregr.predict(test))\n",
    "\n",
    "predictions2 = list(logi.predict(test))\n",
    "\n",
    "# Equivalently \n",
    "# predictions = [1 if s>0.5 else 0 for s in scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.905\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = %.3f\" % (sum([p==a for p, a in zip(predictions2, actuals)])/len(predictions2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.962\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = %.3f\" % (sum([p==a for p, a in zip(predictions, actuals)])/len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(\"Accuracy = %.3f\" % (sum([p==a for p, a in zip(labels1, test_labels)])/len(labels1)))\n",
    "# print(\"Accuracy = %.3f\" % (sum([p==a for p, a in zip(labels1, test_labels)])/len(labels1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.5       , 0.5       , 0.73105858,\n",
       "       0.5       , 0.88079708, 0.5       , 0.73105858, 0.73105858,\n",
       "       0.88079708, 0.88079708, 0.73105858, 0.73105858, 0.88079708,\n",
       "       0.73105858, 0.5       , 0.5       , 0.88079708, 0.5       ,\n",
       "       0.5       , 0.5       , 0.73105858, 0.88079708, 0.73105858,\n",
       "       0.88079708, 0.73105858, 0.88079708, 0.73105858, 0.73105858,\n",
       "       0.5       , 0.5       , 0.73105858, 0.73105858, 0.73105858,\n",
       "       0.88079708, 0.73105858, 0.5       , 0.5       , 0.5       ,\n",
       "       0.5       , 0.5       , 0.73105858, 0.73105858, 0.5       ,\n",
       "       0.5       , 0.5       , 0.88079708, 0.88079708, 0.73105858,\n",
       "       0.5       , 0.88079708, 0.88079708, 0.73105858, 0.73105858,\n",
       "       0.5       , 0.5       , 0.88079708, 0.5       , 0.88079708,\n",
       "       0.5       , 0.88079708, 0.73105858, 0.73105858, 0.88079708,\n",
       "       0.73105858, 0.5       , 0.88079708, 0.88079708, 0.5       ,\n",
       "       0.88079708, 0.73105858, 0.5       , 0.88079708, 0.88079708,\n",
       "       0.73105858, 0.73105858, 0.5       , 0.88079708, 0.73105858,\n",
       "       0.73105858, 0.88079708, 0.5       , 0.88079708, 0.88079708,\n",
       "       0.5       , 0.5       , 0.5       , 0.88079708, 0.73105858,\n",
       "       0.73105858, 0.88079708, 0.5       , 0.73105858, 0.73105858,\n",
       "       0.73105858, 0.5       , 0.88079708, 0.73105858, 0.88079708,\n",
       "       0.88079708, 0.73105858, 0.5       , 0.73105858, 0.73105858])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored = logi._sigmoid(test_labels)\n",
    "scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9047619047619048"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2 = logi.score(test,test_labels)\n",
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = logi.score(test,test_labels)\n",
    "# scores\n",
    "\n",
    "# scored = []\n",
    "# for i in range(len(test)):\n",
    "#     scored.append(logi.score(test[i],test_labels))\n",
    "\n",
    "# scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2\n",
       "0    1.0  2.0  2.0\n",
       "1    2.0  2.0  2.0\n",
       "2    0.0  0.0  0.0\n",
       "3    0.0  0.0  0.0\n",
       "4    1.0  1.0  1.0\n",
       "..   ...  ...  ...\n",
       "100  2.0  2.0  2.0\n",
       "101  1.0  1.0  2.0\n",
       "102  0.0  0.0  0.0\n",
       "103  1.0  1.0  1.0\n",
       "104  1.0  1.0  2.0\n",
       "\n",
       "[105 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(labels1)\n",
    "# print(actuals)\n",
    "\n",
    "labels_pd = pd.DataFrame([actuals,labels1,predictions2])\n",
    "labels_pd.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "ConfusionMatrix = collections.namedtuple('conf', ['tp','fp','tn','fn']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ConfusionMatrix(actuals, scored, threshold=0.5, positive_label=1):\n",
    "    tp=fp=tn=fn=0\n",
    "    bool_actuals = [act==positive_label for act in actuals]\n",
    "    for truth, score in zip(bool_actuals, scores):\n",
    "        if score > threshold:                      # predicted positive \n",
    "            if truth:                              # actually positive \n",
    "                tp += 1\n",
    "            else:                                  # actually negative              \n",
    "                fp += 1          \n",
    "        else:                                      # predicted negative \n",
    "            if not truth:                          # actually negative \n",
    "                tn += 1                          \n",
    "            else:                                  # actually positive \n",
    "                fn += 1\n",
    "\n",
    "    return ConfusionMatrix(tp, fp, tn, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACC(conf_mtrx):\n",
    "    return (conf_mtrx.tp + conf_mtrx.tn) / (conf_mtrx.fp + conf_mtrx.tn + conf_mtrx.tp + conf_mtrx.fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FPR(conf_mtrx):\n",
    "    return conf_mtrx.fp / (conf_mtrx.fp + conf_mtrx.tn) if (conf_mtrx.fp + conf_mtrx.tn)!=0 else 0\n",
    "def TPR(conf_mtrx):\n",
    "    return conf_mtrx.tp / (conf_mtrx.tp + conf_mtrx.fn) if (conf_mtrx.tp + conf_mtrx.fn)!=0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(actuals, scores, **fxns):\n",
    "    # generate thresholds over score domain \n",
    "    low = np.min(scores)\n",
    "    high = np.max(scores)\n",
    "    step = (abs(low) + abs(high)) / 1000\n",
    "    thresholds = np.arange(low-step, high+step, step)\n",
    "\n",
    "    # calculate confusion matrices for all thresholds\n",
    "    confusionMatrices = []\n",
    "    for threshold in thresholds:\n",
    "        confusionMatrices.append(calc_ConfusionMatrix(actuals, scores, threshold))\n",
    "\n",
    "    # apply functions to confusion matrices \n",
    "    results = {fname:list(map(fxn, confusionMatrices)) for fname, fxn in fxns.items()}\n",
    "\n",
    "    results[\"THR\"] = thresholds\n",
    "    print('thresholds:',results[\"THR\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds: [0.4986192  0.5        0.5013808  0.50276159 0.50414239 0.50552319\n",
      " 0.50690399 0.50828478 0.50966558 0.51104638 0.51242717 0.51380797\n",
      " 0.51518877 0.51656956 0.51795036 0.51933116 0.52071196 0.52209275\n",
      " 0.52347355 0.52485435 0.52623514 0.52761594 0.52899674 0.53037754\n",
      " 0.53175833 0.53313913 0.53451993 0.53590072 0.53728152 0.53866232\n",
      " 0.54004312 0.54142391 0.54280471 0.54418551 0.5455663  0.5469471\n",
      " 0.5483279  0.54970869 0.55108949 0.55247029 0.55385109 0.55523188\n",
      " 0.55661268 0.55799348 0.55937427 0.56075507 0.56213587 0.56351667\n",
      " 0.56489746 0.56627826 0.56765906 0.56903985 0.57042065 0.57180145\n",
      " 0.57318225 0.57456304 0.57594384 0.57732464 0.57870543 0.58008623\n",
      " 0.58146703 0.58284782 0.58422862 0.58560942 0.58699022 0.58837101\n",
      " 0.58975181 0.59113261 0.5925134  0.5938942  0.595275   0.5966558\n",
      " 0.59803659 0.59941739 0.60079819 0.60217898 0.60355978 0.60494058\n",
      " 0.60632138 0.60770217 0.60908297 0.61046377 0.61184456 0.61322536\n",
      " 0.61460616 0.61598695 0.61736775 0.61874855 0.62012935 0.62151014\n",
      " 0.62289094 0.62427174 0.62565253 0.62703333 0.62841413 0.62979493\n",
      " 0.63117572 0.63255652 0.63393732 0.63531811 0.63669891 0.63807971\n",
      " 0.6394605  0.6408413  0.6422221  0.6436029  0.64498369 0.64636449\n",
      " 0.64774529 0.64912608 0.65050688 0.65188768 0.65326848 0.65464927\n",
      " 0.65603007 0.65741087 0.65879166 0.66017246 0.66155326 0.66293406\n",
      " 0.66431485 0.66569565 0.66707645 0.66845724 0.66983804 0.67121884\n",
      " 0.67259963 0.67398043 0.67536123 0.67674203 0.67812282 0.67950362\n",
      " 0.68088442 0.68226521 0.68364601 0.68502681 0.68640761 0.6877884\n",
      " 0.6891692  0.69055    0.69193079 0.69331159 0.69469239 0.69607319\n",
      " 0.69745398 0.69883478 0.70021558 0.70159637 0.70297717 0.70435797\n",
      " 0.70573876 0.70711956 0.70850036 0.70988116 0.71126195 0.71264275\n",
      " 0.71402355 0.71540434 0.71678514 0.71816594 0.71954674 0.72092753\n",
      " 0.72230833 0.72368913 0.72506992 0.72645072 0.72783152 0.72921231\n",
      " 0.73059311 0.73197391 0.73335471 0.7347355  0.7361163  0.7374971\n",
      " 0.73887789 0.74025869 0.74163949 0.74302029 0.74440108 0.74578188\n",
      " 0.74716268 0.74854347 0.74992427 0.75130507 0.75268587 0.75406666\n",
      " 0.75544746 0.75682826 0.75820905 0.75958985 0.76097065 0.76235144\n",
      " 0.76373224 0.76511304 0.76649384 0.76787463 0.76925543 0.77063623\n",
      " 0.77201702 0.77339782 0.77477862 0.77615942 0.77754021 0.77892101\n",
      " 0.78030181 0.7816826  0.7830634  0.7844442  0.785825   0.78720579\n",
      " 0.78858659 0.78996739 0.79134818 0.79272898 0.79410978 0.79549057\n",
      " 0.79687137 0.79825217 0.79963297 0.80101376 0.80239456 0.80377536\n",
      " 0.80515615 0.80653695 0.80791775 0.80929855 0.81067934 0.81206014\n",
      " 0.81344094 0.81482173 0.81620253 0.81758333 0.81896413 0.82034492\n",
      " 0.82172572 0.82310652 0.82448731 0.82586811 0.82724891 0.8286297\n",
      " 0.8300105  0.8313913  0.8327721  0.83415289 0.83553369 0.83691449\n",
      " 0.83829528 0.83967608 0.84105688 0.84243768 0.84381847 0.84519927\n",
      " 0.84658007 0.84796086 0.84934166 0.85072246 0.85210325 0.85348405\n",
      " 0.85486485 0.85624565 0.85762644 0.85900724 0.86038804 0.86176883\n",
      " 0.86314963 0.86453043 0.86591123 0.86729202 0.86867282 0.87005362\n",
      " 0.87143441 0.87281521 0.87419601 0.87557681 0.8769576  0.8783384\n",
      " 0.8797192  0.88109999]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ACC': [0.9619047619047619,\n",
       "  0.9619047619047619,\n",
       "  0.9619047619047619,\n",
       "  0.9619047619047619,\n",
       "  0.9619047619047619,\n",
       "  0.9619047619047619,\n",
       "  0.9619047619047619,\n",
       "  0.9523809523809523,\n",
       "  0.9523809523809523,\n",
       "  0.9523809523809523,\n",
       "  0.9523809523809523,\n",
       "  0.9523809523809523,\n",
       "  0.9523809523809523,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9428571428571428,\n",
       "  0.9333333333333333,\n",
       "  0.9333333333333333,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9238095238095239,\n",
       "  0.9142857142857143,\n",
       "  0.9142857142857143,\n",
       "  0.9047619047619048,\n",
       "  0.9047619047619048,\n",
       "  0.8952380952380953,\n",
       "  0.8857142857142857,\n",
       "  0.8857142857142857,\n",
       "  0.8857142857142857,\n",
       "  0.8857142857142857,\n",
       "  0.8857142857142857,\n",
       "  0.8857142857142857,\n",
       "  0.8857142857142857,\n",
       "  0.8857142857142857,\n",
       "  0.8857142857142857,\n",
       "  0.8857142857142857,\n",
       "  0.8761904761904762,\n",
       "  0.8761904761904762,\n",
       "  0.8761904761904762,\n",
       "  0.8761904761904762,\n",
       "  0.8761904761904762,\n",
       "  0.8761904761904762,\n",
       "  0.8761904761904762,\n",
       "  0.8761904761904762,\n",
       "  0.8666666666666667,\n",
       "  0.8666666666666667,\n",
       "  0.8666666666666667,\n",
       "  0.8666666666666667,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8571428571428571,\n",
       "  0.8476190476190476,\n",
       "  0.8476190476190476,\n",
       "  0.8476190476190476,\n",
       "  0.8476190476190476,\n",
       "  0.8476190476190476,\n",
       "  0.8476190476190476,\n",
       "  0.8476190476190476,\n",
       "  0.8476190476190476,\n",
       "  0.8476190476190476,\n",
       "  0.8476190476190476,\n",
       "  0.8476190476190476,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8380952380952381,\n",
       "  0.8285714285714286,\n",
       "  0.8285714285714286,\n",
       "  0.8285714285714286,\n",
       "  0.819047619047619,\n",
       "  0.8095238095238095,\n",
       "  0.8095238095238095,\n",
       "  0.8,\n",
       "  0.8,\n",
       "  0.7904761904761904,\n",
       "  0.7904761904761904,\n",
       "  0.7904761904761904,\n",
       "  0.780952380952381,\n",
       "  0.7714285714285715,\n",
       "  0.7714285714285715,\n",
       "  0.7714285714285715,\n",
       "  0.7714285714285715,\n",
       "  0.7714285714285715,\n",
       "  0.7714285714285715,\n",
       "  0.7714285714285715,\n",
       "  0.7523809523809524,\n",
       "  0.7523809523809524,\n",
       "  0.7523809523809524,\n",
       "  0.7523809523809524,\n",
       "  0.7523809523809524,\n",
       "  0.7523809523809524,\n",
       "  0.7428571428571429,\n",
       "  0.7428571428571429,\n",
       "  0.7428571428571429,\n",
       "  0.7428571428571429,\n",
       "  0.7428571428571429,\n",
       "  0.7428571428571429,\n",
       "  0.7428571428571429,\n",
       "  0.7428571428571429,\n",
       "  0.7333333333333333,\n",
       "  0.7333333333333333,\n",
       "  0.7333333333333333,\n",
       "  0.7333333333333333,\n",
       "  0.7333333333333333,\n",
       "  0.7333333333333333,\n",
       "  0.7238095238095238,\n",
       "  0.7238095238095238,\n",
       "  0.7238095238095238,\n",
       "  0.7238095238095238,\n",
       "  0.7238095238095238,\n",
       "  0.7238095238095238,\n",
       "  0.7238095238095238,\n",
       "  0.7238095238095238,\n",
       "  0.7238095238095238,\n",
       "  0.7142857142857143,\n",
       "  0.7142857142857143,\n",
       "  0.7142857142857143,\n",
       "  0.7142857142857143,\n",
       "  0.7142857142857143,\n",
       "  0.7142857142857143,\n",
       "  0.7047619047619048,\n",
       "  0.7047619047619048,\n",
       "  0.7047619047619048,\n",
       "  0.7047619047619048,\n",
       "  0.7047619047619048,\n",
       "  0.7047619047619048,\n",
       "  0.7047619047619048,\n",
       "  0.7047619047619048,\n",
       "  0.7047619047619048,\n",
       "  0.7047619047619048,\n",
       "  0.7047619047619048,\n",
       "  0.7047619047619048,\n",
       "  0.6857142857142857,\n",
       "  0.6857142857142857,\n",
       "  0.6857142857142857,\n",
       "  0.6857142857142857,\n",
       "  0.6857142857142857,\n",
       "  0.6857142857142857,\n",
       "  0.6857142857142857,\n",
       "  0.6857142857142857,\n",
       "  0.6857142857142857,\n",
       "  0.6761904761904762,\n",
       "  0.6761904761904762,\n",
       "  0.6761904761904762,\n",
       "  0.6761904761904762,\n",
       "  0.6761904761904762,\n",
       "  0.6761904761904762,\n",
       "  0.6761904761904762,\n",
       "  0.6761904761904762,\n",
       "  0.6666666666666666,\n",
       "  0.6666666666666666,\n",
       "  0.6666666666666666,\n",
       "  0.6571428571428571,\n",
       "  0.6571428571428571,\n",
       "  0.6571428571428571,\n",
       "  0.6571428571428571,\n",
       "  0.6571428571428571,\n",
       "  0.6571428571428571,\n",
       "  0.6571428571428571,\n",
       "  0.6571428571428571,\n",
       "  0.6476190476190476,\n",
       "  0.6476190476190476,\n",
       "  0.6476190476190476,\n",
       "  0.6476190476190476,\n",
       "  0.6476190476190476,\n",
       "  0.6476190476190476,\n",
       "  0.6476190476190476,\n",
       "  0.6476190476190476,\n",
       "  0.6476190476190476,\n",
       "  0.6476190476190476,\n",
       "  0.6476190476190476,\n",
       "  0.6476190476190476,\n",
       "  0.6476190476190476,\n",
       "  0.638095238095238,\n",
       "  0.638095238095238,\n",
       "  0.638095238095238,\n",
       "  0.638095238095238,\n",
       "  0.638095238095238,\n",
       "  0.638095238095238,\n",
       "  0.638095238095238,\n",
       "  0.638095238095238,\n",
       "  0.638095238095238,\n",
       "  0.638095238095238,\n",
       "  0.638095238095238,\n",
       "  0.638095238095238],\n",
       " 'THR': array([0.4986192 , 0.5       , 0.5013808 , 0.50276159, 0.50414239,\n",
       "        0.50552319, 0.50690399, 0.50828478, 0.50966558, 0.51104638,\n",
       "        0.51242717, 0.51380797, 0.51518877, 0.51656956, 0.51795036,\n",
       "        0.51933116, 0.52071196, 0.52209275, 0.52347355, 0.52485435,\n",
       "        0.52623514, 0.52761594, 0.52899674, 0.53037754, 0.53175833,\n",
       "        0.53313913, 0.53451993, 0.53590072, 0.53728152, 0.53866232,\n",
       "        0.54004312, 0.54142391, 0.54280471, 0.54418551, 0.5455663 ,\n",
       "        0.5469471 , 0.5483279 , 0.54970869, 0.55108949, 0.55247029,\n",
       "        0.55385109, 0.55523188, 0.55661268, 0.55799348, 0.55937427,\n",
       "        0.56075507, 0.56213587, 0.56351667, 0.56489746, 0.56627826,\n",
       "        0.56765906, 0.56903985, 0.57042065, 0.57180145, 0.57318225,\n",
       "        0.57456304, 0.57594384, 0.57732464, 0.57870543, 0.58008623,\n",
       "        0.58146703, 0.58284782, 0.58422862, 0.58560942, 0.58699022,\n",
       "        0.58837101, 0.58975181, 0.59113261, 0.5925134 , 0.5938942 ,\n",
       "        0.595275  , 0.5966558 , 0.59803659, 0.59941739, 0.60079819,\n",
       "        0.60217898, 0.60355978, 0.60494058, 0.60632138, 0.60770217,\n",
       "        0.60908297, 0.61046377, 0.61184456, 0.61322536, 0.61460616,\n",
       "        0.61598695, 0.61736775, 0.61874855, 0.62012935, 0.62151014,\n",
       "        0.62289094, 0.62427174, 0.62565253, 0.62703333, 0.62841413,\n",
       "        0.62979493, 0.63117572, 0.63255652, 0.63393732, 0.63531811,\n",
       "        0.63669891, 0.63807971, 0.6394605 , 0.6408413 , 0.6422221 ,\n",
       "        0.6436029 , 0.64498369, 0.64636449, 0.64774529, 0.64912608,\n",
       "        0.65050688, 0.65188768, 0.65326848, 0.65464927, 0.65603007,\n",
       "        0.65741087, 0.65879166, 0.66017246, 0.66155326, 0.66293406,\n",
       "        0.66431485, 0.66569565, 0.66707645, 0.66845724, 0.66983804,\n",
       "        0.67121884, 0.67259963, 0.67398043, 0.67536123, 0.67674203,\n",
       "        0.67812282, 0.67950362, 0.68088442, 0.68226521, 0.68364601,\n",
       "        0.68502681, 0.68640761, 0.6877884 , 0.6891692 , 0.69055   ,\n",
       "        0.69193079, 0.69331159, 0.69469239, 0.69607319, 0.69745398,\n",
       "        0.69883478, 0.70021558, 0.70159637, 0.70297717, 0.70435797,\n",
       "        0.70573876, 0.70711956, 0.70850036, 0.70988116, 0.71126195,\n",
       "        0.71264275, 0.71402355, 0.71540434, 0.71678514, 0.71816594,\n",
       "        0.71954674, 0.72092753, 0.72230833, 0.72368913, 0.72506992,\n",
       "        0.72645072, 0.72783152, 0.72921231, 0.73059311, 0.73197391,\n",
       "        0.73335471, 0.7347355 , 0.7361163 , 0.7374971 , 0.73887789,\n",
       "        0.74025869, 0.74163949, 0.74302029, 0.74440108, 0.74578188,\n",
       "        0.74716268, 0.74854347, 0.74992427, 0.75130507, 0.75268587,\n",
       "        0.75406666, 0.75544746, 0.75682826, 0.75820905, 0.75958985,\n",
       "        0.76097065, 0.76235144, 0.76373224, 0.76511304, 0.76649384,\n",
       "        0.76787463, 0.76925543, 0.77063623, 0.77201702, 0.77339782,\n",
       "        0.77477862, 0.77615942, 0.77754021, 0.77892101, 0.78030181,\n",
       "        0.7816826 , 0.7830634 , 0.7844442 , 0.785825  , 0.78720579,\n",
       "        0.78858659, 0.78996739, 0.79134818, 0.79272898, 0.79410978,\n",
       "        0.79549057, 0.79687137, 0.79825217, 0.79963297, 0.80101376,\n",
       "        0.80239456, 0.80377536, 0.80515615, 0.80653695, 0.80791775,\n",
       "        0.80929855, 0.81067934, 0.81206014, 0.81344094, 0.81482173,\n",
       "        0.81620253, 0.81758333, 0.81896413, 0.82034492, 0.82172572,\n",
       "        0.82310652, 0.82448731, 0.82586811, 0.82724891, 0.8286297 ,\n",
       "        0.8300105 , 0.8313913 , 0.8327721 , 0.83415289, 0.83553369,\n",
       "        0.83691449, 0.83829528, 0.83967608, 0.84105688, 0.84243768,\n",
       "        0.84381847, 0.84519927, 0.84658007, 0.84796086, 0.84934166,\n",
       "        0.85072246, 0.85210325, 0.85348405, 0.85486485, 0.85624565,\n",
       "        0.85762644, 0.85900724, 0.86038804, 0.86176883, 0.86314963,\n",
       "        0.86453043, 0.86591123, 0.86729202, 0.86867282, 0.87005362,\n",
       "        0.87143441, 0.87281521, 0.87419601, 0.87557681, 0.8769576 ,\n",
       "        0.8783384 , 0.8797192 , 0.88109999])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = apply(actuals,scored, ACC=ACC)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(actuals, scores):\n",
    "    return apply(actuals, scores, FPR=FPR, TPR=TPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds: [0.00966954 0.01054559 0.01142164 0.01229769 0.01317373 0.01404978\n",
      " 0.01492583 0.01580188 0.01667792 0.01755397 0.01843002 0.01930607\n",
      " 0.02018211 0.02105816 0.02193421 0.02281026 0.0236863  0.02456235\n",
      " 0.0254384  0.02631445 0.0271905  0.02806654 0.02894259 0.02981864\n",
      " 0.03069469 0.03157073 0.03244678 0.03332283 0.03419888 0.03507492\n",
      " 0.03595097 0.03682702 0.03770307 0.03857911 0.03945516 0.04033121\n",
      " 0.04120726 0.0420833  0.04295935 0.0438354  0.04471145 0.04558749\n",
      " 0.04646354 0.04733959 0.04821564 0.04909169 0.04996773 0.05084378\n",
      " 0.05171983 0.05259588 0.05347192 0.05434797 0.05522402 0.05610007\n",
      " 0.05697611 0.05785216 0.05872821 0.05960426 0.0604803  0.06135635\n",
      " 0.0622324  0.06310845 0.06398449 0.06486054 0.06573659 0.06661264\n",
      " 0.06748869 0.06836473 0.06924078 0.07011683 0.07099288 0.07186892\n",
      " 0.07274497 0.07362102 0.07449707 0.07537311 0.07624916 0.07712521\n",
      " 0.07800126 0.0788773  0.07975335 0.0806294  0.08150545 0.08238149\n",
      " 0.08325754 0.08413359 0.08500964 0.08588569 0.08676173 0.08763778\n",
      " 0.08851383 0.08938988 0.09026592 0.09114197 0.09201802 0.09289407\n",
      " 0.09377011 0.09464616 0.09552221 0.09639826 0.0972743  0.09815035\n",
      " 0.0990264  0.09990245 0.10077849 0.10165454 0.10253059 0.10340664\n",
      " 0.10428268 0.10515873 0.10603478 0.10691083 0.10778688 0.10866292\n",
      " 0.10953897 0.11041502 0.11129107 0.11216711 0.11304316 0.11391921\n",
      " 0.11479526 0.1156713  0.11654735 0.1174234  0.11829945 0.11917549\n",
      " 0.12005154 0.12092759 0.12180364 0.12267968 0.12355573 0.12443178\n",
      " 0.12530783 0.12618388 0.12705992 0.12793597 0.12881202 0.12968807\n",
      " 0.13056411 0.13144016 0.13231621 0.13319226 0.1340683  0.13494435\n",
      " 0.1358204  0.13669645 0.13757249 0.13844854 0.13932459 0.14020064\n",
      " 0.14107668 0.14195273 0.14282878 0.14370483 0.14458087 0.14545692\n",
      " 0.14633297 0.14720902 0.14808507 0.14896111 0.14983716 0.15071321\n",
      " 0.15158926 0.1524653  0.15334135 0.1542174  0.15509345 0.15596949\n",
      " 0.15684554 0.15772159 0.15859764 0.15947368 0.16034973 0.16122578\n",
      " 0.16210183 0.16297787 0.16385392 0.16472997 0.16560602 0.16648207\n",
      " 0.16735811 0.16823416 0.16911021 0.16998626 0.1708623  0.17173835\n",
      " 0.1726144  0.17349045 0.17436649 0.17524254 0.17611859 0.17699464\n",
      " 0.17787068 0.17874673 0.17962278 0.18049883 0.18137487 0.18225092\n",
      " 0.18312697 0.18400302 0.18487907 0.18575511 0.18663116 0.18750721\n",
      " 0.18838326 0.1892593  0.19013535 0.1910114  0.19188745 0.19276349\n",
      " 0.19363954 0.19451559 0.19539164 0.19626768 0.19714373 0.19801978\n",
      " 0.19889583 0.19977187 0.20064792 0.20152397 0.20240002 0.20327606\n",
      " 0.20415211 0.20502816 0.20590421 0.20678026 0.2076563  0.20853235\n",
      " 0.2094084  0.21028445 0.21116049 0.21203654 0.21291259 0.21378864\n",
      " 0.21466468 0.21554073 0.21641678 0.21729283 0.21816887 0.21904492\n",
      " 0.21992097 0.22079702 0.22167306 0.22254911 0.22342516 0.22430121\n",
      " 0.22517726 0.2260533  0.22692935 0.2278054  0.22868145 0.22955749\n",
      " 0.23043354 0.23130959 0.23218564 0.23306168 0.23393773 0.23481378\n",
      " 0.23568983 0.23656587 0.23744192 0.23831797 0.23919402 0.24007006\n",
      " 0.24094611 0.24182216 0.24269821 0.24357425 0.2444503  0.24532635\n",
      " 0.2462024  0.24707845 0.24795449 0.24883054 0.24970659 0.25058264\n",
      " 0.25145868 0.25233473 0.25321078 0.25408683 0.25496287 0.25583892\n",
      " 0.25671497 0.25759102 0.25846706 0.25934311 0.26021916 0.26109521\n",
      " 0.26197125 0.2628473  0.26372335 0.2645994  0.26547545 0.26635149\n",
      " 0.26722754 0.26810359 0.26897964 0.26985568 0.27073173 0.27160778\n",
      " 0.27248383 0.27335987 0.27423592 0.27511197 0.27598802 0.27686406\n",
      " 0.27774011 0.27861616 0.27949221 0.28036825 0.2812443  0.28212035\n",
      " 0.2829964  0.28387244 0.28474849 0.28562454 0.28650059 0.28737664\n",
      " 0.28825268 0.28912873 0.29000478 0.29088083 0.29175687 0.29263292\n",
      " 0.29350897 0.29438502 0.29526106 0.29613711 0.29701316 0.29788921\n",
      " 0.29876525 0.2996413  0.30051735 0.3013934  0.30226944 0.30314549\n",
      " 0.30402154 0.30489759 0.30577364 0.30664968 0.30752573 0.30840178\n",
      " 0.30927783 0.31015387 0.31102992 0.31190597 0.31278202 0.31365806\n",
      " 0.31453411 0.31541016 0.31628621 0.31716225 0.3180383  0.31891435\n",
      " 0.3197904  0.32066644 0.32154249 0.32241854 0.32329459 0.32417064\n",
      " 0.32504668 0.32592273 0.32679878 0.32767483 0.32855087 0.32942692\n",
      " 0.33030297 0.33117902 0.33205506 0.33293111 0.33380716 0.33468321\n",
      " 0.33555925 0.3364353  0.33731135 0.3381874  0.33906344 0.33993949\n",
      " 0.34081554 0.34169159 0.34256763 0.34344368 0.34431973 0.34519578\n",
      " 0.34607183 0.34694787 0.34782392 0.34869997 0.34957602 0.35045206\n",
      " 0.35132811 0.35220416 0.35308021 0.35395625 0.3548323  0.35570835\n",
      " 0.3565844  0.35746044 0.35833649 0.35921254 0.36008859 0.36096463\n",
      " 0.36184068 0.36271673 0.36359278 0.36446883 0.36534487 0.36622092\n",
      " 0.36709697 0.36797302 0.36884906 0.36972511 0.37060116 0.37147721\n",
      " 0.37235325 0.3732293  0.37410535 0.3749814  0.37585744 0.37673349\n",
      " 0.37760954 0.37848559 0.37936163 0.38023768 0.38111373 0.38198978\n",
      " 0.38286582 0.38374187 0.38461792 0.38549397 0.38637002 0.38724606\n",
      " 0.38812211 0.38899816 0.38987421 0.39075025 0.3916263  0.39250235\n",
      " 0.3933784  0.39425444 0.39513049 0.39600654 0.39688259 0.39775863\n",
      " 0.39863468 0.39951073 0.40038678 0.40126282 0.40213887 0.40301492\n",
      " 0.40389097 0.40476702 0.40564306 0.40651911 0.40739516 0.40827121\n",
      " 0.40914725 0.4100233  0.41089935 0.4117754  0.41265144 0.41352749\n",
      " 0.41440354 0.41527959 0.41615563 0.41703168 0.41790773 0.41878378\n",
      " 0.41965982 0.42053587 0.42141192 0.42228797 0.42316401 0.42404006\n",
      " 0.42491611 0.42579216 0.42666821 0.42754425 0.4284203  0.42929635\n",
      " 0.4301724  0.43104844 0.43192449 0.43280054 0.43367659 0.43455263\n",
      " 0.43542868 0.43630473 0.43718078 0.43805682 0.43893287 0.43980892\n",
      " 0.44068497 0.44156101 0.44243706 0.44331311 0.44418916 0.44506521\n",
      " 0.44594125 0.4468173  0.44769335 0.4485694  0.44944544 0.45032149\n",
      " 0.45119754 0.45207359 0.45294963 0.45382568 0.45470173 0.45557778\n",
      " 0.45645382 0.45732987 0.45820592 0.45908197 0.45995801 0.46083406\n",
      " 0.46171011 0.46258616 0.46346221 0.46433825 0.4652143  0.46609035\n",
      " 0.4669664  0.46784244 0.46871849 0.46959454 0.47047059 0.47134663\n",
      " 0.47222268 0.47309873 0.47397478 0.47485082 0.47572687 0.47660292\n",
      " 0.47747897 0.47835501 0.47923106 0.48010711 0.48098316 0.4818592\n",
      " 0.48273525 0.4836113  0.48448735 0.4853634  0.48623944 0.48711549\n",
      " 0.48799154 0.48886759 0.48974363 0.49061968 0.49149573 0.49237178\n",
      " 0.49324782 0.49412387 0.49499992 0.49587597 0.49675201 0.49762806\n",
      " 0.49850411 0.49938016 0.5002562  0.50113225 0.5020083  0.50288435\n",
      " 0.5037604  0.50463644 0.50551249 0.50638854 0.50726459 0.50814063\n",
      " 0.50901668 0.50989273 0.51076878 0.51164482 0.51252087 0.51339692\n",
      " 0.51427297 0.51514901 0.51602506 0.51690111 0.51777716 0.5186532\n",
      " 0.51952925 0.5204053  0.52128135 0.52215739 0.52303344 0.52390949\n",
      " 0.52478554 0.52566159 0.52653763 0.52741368 0.52828973 0.52916578\n",
      " 0.53004182 0.53091787 0.53179392 0.53266997 0.53354601 0.53442206\n",
      " 0.53529811 0.53617416 0.5370502  0.53792625 0.5388023  0.53967835\n",
      " 0.54055439 0.54143044 0.54230649 0.54318254 0.54405859 0.54493463\n",
      " 0.54581068 0.54668673 0.54756278 0.54843882 0.54931487 0.55019092\n",
      " 0.55106697 0.55194301 0.55281906 0.55369511 0.55457116 0.5554472\n",
      " 0.55632325 0.5571993  0.55807535 0.55895139 0.55982744 0.56070349\n",
      " 0.56157954 0.56245558 0.56333163 0.56420768 0.56508373 0.56595978\n",
      " 0.56683582 0.56771187 0.56858792 0.56946397 0.57034001 0.57121606\n",
      " 0.57209211 0.57296816 0.5738442  0.57472025 0.5755963  0.57647235\n",
      " 0.57734839 0.57822444 0.57910049 0.57997654 0.58085258 0.58172863\n",
      " 0.58260468 0.58348073 0.58435678 0.58523282 0.58610887 0.58698492\n",
      " 0.58786097 0.58873701 0.58961306 0.59048911 0.59136516 0.5922412\n",
      " 0.59311725 0.5939933  0.59486935 0.59574539 0.59662144 0.59749749\n",
      " 0.59837354 0.59924958 0.60012563 0.60100168 0.60187773 0.60275378\n",
      " 0.60362982 0.60450587 0.60538192 0.60625797 0.60713401 0.60801006\n",
      " 0.60888611 0.60976216 0.6106382  0.61151425 0.6123903  0.61326635\n",
      " 0.61414239 0.61501844 0.61589449 0.61677054 0.61764658 0.61852263\n",
      " 0.61939868 0.62027473 0.62115077 0.62202682 0.62290287 0.62377892\n",
      " 0.62465497 0.62553101 0.62640706 0.62728311 0.62815916 0.6290352\n",
      " 0.62991125 0.6307873  0.63166335 0.63253939 0.63341544 0.63429149\n",
      " 0.63516754 0.63604358 0.63691963 0.63779568 0.63867173 0.63954777\n",
      " 0.64042382 0.64129987 0.64217592 0.64305197 0.64392801 0.64480406\n",
      " 0.64568011 0.64655616 0.6474322  0.64830825 0.6491843  0.65006035\n",
      " 0.65093639 0.65181244 0.65268849 0.65356454 0.65444058 0.65531663\n",
      " 0.65619268 0.65706873 0.65794477 0.65882082 0.65969687 0.66057292\n",
      " 0.66144896 0.66232501 0.66320106 0.66407711 0.66495316 0.6658292\n",
      " 0.66670525 0.6675813  0.66845735 0.66933339 0.67020944 0.67108549\n",
      " 0.67196154 0.67283758 0.67371363 0.67458968 0.67546573 0.67634177\n",
      " 0.67721782 0.67809387 0.67896992 0.67984596 0.68072201 0.68159806\n",
      " 0.68247411 0.68335016 0.6842262  0.68510225 0.6859783  0.68685435\n",
      " 0.68773039 0.68860644 0.68948249 0.69035854 0.69123458 0.69211063\n",
      " 0.69298668 0.69386273 0.69473877 0.69561482 0.69649087 0.69736692\n",
      " 0.69824296 0.69911901 0.69999506 0.70087111 0.70174716 0.7026232\n",
      " 0.70349925 0.7043753  0.70525135 0.70612739 0.70700344 0.70787949\n",
      " 0.70875554 0.70963158 0.71050763 0.71138368 0.71225973 0.71313577\n",
      " 0.71401182 0.71488787 0.71576392 0.71663996 0.71751601 0.71839206\n",
      " 0.71926811 0.72014415 0.7210202  0.72189625 0.7227723  0.72364835\n",
      " 0.72452439 0.72540044 0.72627649 0.72715254 0.72802858 0.72890463\n",
      " 0.72978068 0.73065673 0.73153277 0.73240882 0.73328487 0.73416092\n",
      " 0.73503696 0.73591301 0.73678906 0.73766511 0.73854115 0.7394172\n",
      " 0.74029325 0.7411693  0.74204535 0.74292139 0.74379744 0.74467349\n",
      " 0.74554954 0.74642558 0.74730163 0.74817768 0.74905373 0.74992977\n",
      " 0.75080582 0.75168187 0.75255792 0.75343396 0.75431001 0.75518606\n",
      " 0.75606211 0.75693815 0.7578142  0.75869025 0.7595663  0.76044234\n",
      " 0.76131839 0.76219444 0.76307049 0.76394654 0.76482258 0.76569863\n",
      " 0.76657468 0.76745073 0.76832677 0.76920282 0.77007887 0.77095492\n",
      " 0.77183096 0.77270701 0.77358306 0.77445911 0.77533515 0.7762112\n",
      " 0.77708725 0.7779633  0.77883934 0.77971539 0.78059144 0.78146749\n",
      " 0.78234354 0.78321958 0.78409563 0.78497168 0.78584773 0.78672377\n",
      " 0.78759982 0.78847587 0.78935192 0.79022796 0.79110401 0.79198006\n",
      " 0.79285611 0.79373215 0.7946082  0.79548425 0.7963603  0.79723634\n",
      " 0.79811239 0.79898844 0.79986449 0.80074053 0.80161658 0.80249263\n",
      " 0.80336868 0.80424473 0.80512077 0.80599682 0.80687287 0.80774892\n",
      " 0.80862496 0.80950101 0.81037706 0.81125311 0.81212915 0.8130052\n",
      " 0.81388125 0.8147573  0.81563334 0.81650939 0.81738544 0.81826149\n",
      " 0.81913753 0.82001358 0.82088963 0.82176568 0.82264173 0.82351777\n",
      " 0.82439382 0.82526987 0.82614592 0.82702196 0.82789801 0.82877406\n",
      " 0.82965011 0.83052615 0.8314022  0.83227825 0.8331543  0.83403034\n",
      " 0.83490639 0.83578244 0.83665849 0.83753453 0.83841058 0.83928663\n",
      " 0.84016268 0.84103873 0.84191477 0.84279082 0.84366687 0.84454292\n",
      " 0.84541896 0.84629501 0.84717106 0.84804711 0.84892315 0.8497992\n",
      " 0.85067525 0.8515513  0.85242734 0.85330339 0.85417944 0.85505549\n",
      " 0.85593153 0.85680758 0.85768363 0.85855968 0.85943572 0.86031177\n",
      " 0.86118782 0.86206387 0.86293992 0.86381596 0.86469201 0.86556806]\n"
     ]
    }
   ],
   "source": [
    "ab = ROC(actuals,scores)\n",
    "# ab['FPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x268f24332b0>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ab['FPR'],ab['TPR'])\n",
    "# plt.plot(np.multiply(ab['FPR'],len(test)/6),ab['TPR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Declaring namedtuple()   \n",
    "# Student = namedtuple('Student',['name','age','DOB'])   \n",
    "      \n",
    "# # Adding values   \n",
    "# S = Student('Nandini','19','2541997')   \n",
    "      \n",
    "# # Access using index   \n",
    "# print (\"The Student age using index is : \",end =\"\")   \n",
    "# print (S.DOB)   \n",
    "      \n",
    "# # Access using name    \n",
    "# print (\"The Student name using keyname is : \",end =\"\")   \n",
    "# print (S[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make Predictions with k-nearest neighbors on the Iris Flowers Dataset\n",
    "# from csv import reader\n",
    "# from math import sqrt\n",
    " \n",
    "# # Load a CSV file\n",
    "# def load_csv(filename):\n",
    "# \tdataset = list()\n",
    "# \twith open(filename, 'r') as file:\n",
    "# \t\tcsv_reader = reader(file)\n",
    "# \t\tfor row in csv_reader:\n",
    "# \t\t\tif not row:\n",
    "# \t\t\t\tcontinue\n",
    "# \t\t\tdataset.append(row)\n",
    "# \treturn dataset\n",
    " \n",
    "# # Convert string column to float\n",
    "# def str_column_to_float(dataset, column):\n",
    "# \tfor row in dataset:\n",
    "# \t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# # Convert string column to integer\n",
    "# def str_column_to_int(dataset, column):\n",
    "# \tclass_values = [row[column] for row in dataset]\n",
    "# \tunique = set(class_values)\n",
    "# \tlookup = dict()\n",
    "# \tfor i, value in enumerate(unique):\n",
    "# \t\tlookup[value] = i\n",
    "# \t\tprint('[%s] => %d' % (value, i))\n",
    "# \tfor row in dataset:\n",
    "# \t\trow[column] = lookup[row[column]]\n",
    "# \treturn lookup\n",
    " \n",
    "# # Find the min and max values for each column\n",
    "# # def dataset_minmax(dataset):\n",
    "# # \tminmax = list()\n",
    "# # \tfor i in range(len(dataset[0])):\n",
    "# # \t\tcol_values = [row[i] for row in dataset]\n",
    "# # \t\tvalue_min = min(col_values)\n",
    "# # \t\tvalue_max = max(col_values)\n",
    "# # \t\tminmax.append([value_min, value_max])\n",
    "# # \treturn minmax\n",
    "\n",
    "# # Find the min and max values for each column\n",
    "# def dataset_mean_std(dataset):\n",
    "# \tmean_std = list()\n",
    "# \tfor i in range(len(dataset[0])):\n",
    "# \t\tcol_values = [row[i] for row in dataset]\n",
    "# \t\tvalue_mean = np.mean(col_values)\n",
    "# \t\tvalue_std = np.std(col_values)\n",
    "# \t\tmean_std.append([value_mean, value_std])\n",
    "# \treturn mean_std\n",
    "\n",
    "# # Rescale dataset columns to the range 0-1\n",
    "# # def normalize_dataset(dataset, minmax):\n",
    "# # \tfor row in dataset:\n",
    "# # \t\tfor i in range(len(row)):\n",
    "# # \t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "            \n",
    "# # Rescale dataset columns to the range 0-1\n",
    "# def normalize_dataset(dataset, mean_std):\n",
    "# \tfor row in dataset:\n",
    "# \t\tfor i in range(len(row)):\n",
    "# \t\t\trow[i] = (row[i] - mean_std[i][0]) / (mean_std[i][1])\n",
    " \n",
    " \n",
    "# # Calculate the Euclidean distance between two vectors\n",
    "# def euclidean_distance(row1, row2):\n",
    "# \tdistance = 0.0\n",
    "# \tfor i in range(len(row1)-1):\n",
    "# \t\tdistance += (row1[i] - row2[i])**2\n",
    "# \treturn sqrt(distance)\n",
    " \n",
    "# # Locate the most similar neighbors\n",
    "# def get_neighbors(train, test_row, num_neighbors):\n",
    "# \tdistances = list()\n",
    "# \tfor train_row in train:\n",
    "# \t\tdist = euclidean_distance(test_row, train_row)\n",
    "# \t\tdistances.append((train_row, dist))\n",
    "# \tdistances.sort(key=lambda tup: tup[1])\n",
    "# \tneighbors = list()\n",
    "# \tfor i in range(num_neighbors):\n",
    "# \t\tneighbors.append(distances[i][0])\n",
    "# \treturn neighbors\n",
    " \n",
    "# # Make a prediction with neighbors\n",
    "# def predict_classification(train, test_row, num_neighbors):\n",
    "# \tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "# \toutput_values = [row[-1] for row in neighbors]\n",
    "# \tprediction = max(set(output_values), key=output_values.count)\n",
    "# \treturn prediction\n",
    " \n",
    "# # Make a prediction with KNN on Iris Dataset\n",
    "# filename = 'iris.csv'\n",
    "# dataset = load_csv(filename)\n",
    "# for i in range(len(dataset[0])-1):\n",
    "# \tstr_column_to_float(dataset, i)\n",
    "# # convert class column to integers\n",
    "# str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# # define model parameter\n",
    "# num_neighbors = 5\n",
    "# # define a new record\n",
    "# row = [4.7,7.9,3.2,0.3]\n",
    "# row1 = [0.7,1.5,8.4,2.0]\n",
    "# # predict the label\n",
    "# label = predict_classification(dataset, row1, num_neighbors)\n",
    "# print('Data=%s, Predicted: %s' % (row, label))\n",
    "# # print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 0. 0. 1. 0. 2. 0. 1. 1. 2. 2. 1. 1. 2. 1. 0. 0. 2. 0. 0. 0. 1. 2.\n",
      " 1. 2. 1. 2. 1. 1. 0. 0. 1. 1. 1. 2. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2.\n",
      " 2. 1. 0. 2. 2. 1. 1. 0. 0. 2. 0. 2. 0. 2. 1. 1. 2. 1. 0. 2. 2. 0. 2. 1.\n",
      " 0. 2. 2. 1. 1. 0. 2. 1. 1. 2. 0. 2. 2. 0. 0. 0. 2. 1. 1. 2. 0. 1. 1. 1.\n",
      " 0. 2. 1. 2. 2. 1. 0. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.8743984 , 0.02389182, 0.0968861 ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    \"\"\" applies softmax to an input x\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def softmax2(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "ass = softmax2(test[4])\n",
    "# ass[:,1]\n",
    "# test_labels\n",
    "test_labels[0]\n",
    "print(test_labels)\n",
    "ass[:3]\n",
    "# test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import softmax\n",
    "# ass = softmax(test_labels,axis=0)\n",
    "# ass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ass = list(softmax2(test))\n",
    "# print(ass)\n",
    "# ass_1 = ass[:1]\n",
    "# ass_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "0.471006103650701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename, listify \n",
    "# train, test, train_labels, test_labels \n",
    "actuals = list(test_labels)\n",
    "\n",
    "# Predict probablities of test data [0,1]\n",
    "scores = list(logregr.predict_proba(test)[:,1])\n",
    "# scores_2 = scores\n",
    "scores_full = list(logregr.predict_proba(test))\n",
    "\n",
    "index = 2\n",
    "ind2 = len(test[1])\n",
    "ind22 = ind2-2\n",
    "\n",
    "scores_2 = list(logregr.predict_proba(test)[:,ind22])\n",
    "\n",
    "# Equivalently\n",
    "import math\n",
    "def sigmoid(x): \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "# scores = [sigmoid(logregr.coef_@test_i + logregr.intercept_) for test_i in test]\n",
    "print(len(scores))\n",
    "print(scores_2[104])\n",
    "# print(scores_full[ind2-1])\n",
    "# print(scores[index])\n",
    "# print(scores[ind2-1])\n",
    "# actuals\n",
    "# test\n",
    "ind22\n",
    "# scores_full\n",
    "# actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _net_input(self, X):\n",
    "    \"\"\"Compute the linear net input.\"\"\"\n",
    "    return (X.dot(self.w_) + self.b_).flatten()\n",
    "\n",
    "def _sigmoid_activation(self, z):\n",
    "    \"\"\"Compute the output of the logistic sigmoid function.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def _forward(self, X):\n",
    "    z = self._net_input(X)\n",
    "    a = self._sigmoid_activation(z)\n",
    "    return a\n",
    "\n",
    "def predict_proba(self, X):\n",
    "    \"\"\"Predict class probabilities of X from the net input.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "        Training vectors, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "    Returns\n",
    "    ----------\n",
    "    Class 1 probability : float\n",
    "    \"\"\"\n",
    "    return self._forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_3948/1291536472.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  abd = np.array(logi.w)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([ 1.00023881,  0.29627953,  1.92564133, -2.90270875, -1.01109027]),\n",
       "  0.0),\n",
       " (array([ 1.31881842,  0.8512732 , -2.20636545,  0.14255992, -1.06879364]),\n",
       "  1.0),\n",
       " (array([-0.42498545, -2.19485993, -2.03082358,  3.2961568 ,  2.68164414]),\n",
       "  2.0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z = test @ logi.w\n",
    "abd = np.array(logi.w)\n",
    "test[0]\n",
    "logi.w\n",
    "# z = np.matmul(test,abd)\n",
    "# type(abd)\n",
    "# print(test)\n",
    "test.shape\n",
    "logi.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00023881,  0.29627953,  1.92564133, -2.90270875, -1.01109027])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi.w\n",
    "log_point = logi.w\n",
    "log_point = log_point[0][0]\n",
    "log_point_2 = log_point\n",
    "log_point_2\n",
    "# log_point_2\n",
    "# len(X_test)\n",
    "# y_test.shape\n",
    "# _test.reshape(5,6)\n",
    "# log_point_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.33838762e+00],\n",
       "       [-1.04753100e+01],\n",
       "       [ 2.19594033e-01],\n",
       "       [-6.35505212e+00],\n",
       "       [ 6.42935706e-01],\n",
       "       [-5.91262074e+00],\n",
       "       [-3.28712608e-01],\n",
       "       [-5.05476566e+00],\n",
       "       [ 4.89291327e+00],\n",
       "       [-1.15442429e+01],\n",
       "       [ 2.85720872e+00],\n",
       "       [-8.43244556e+00],\n",
       "       [ 5.37909714e+00],\n",
       "       [-1.38789390e+01],\n",
       "       [-6.70521405e-01],\n",
       "       [ 5.26663494e+00],\n",
       "       [ 3.66127972e+00],\n",
       "       [-1.64083257e+01],\n",
       "       [ 2.79257180e+00],\n",
       "       [-5.54231375e+00],\n",
       "       [ 4.79700432e+00],\n",
       "       [-1.43750011e+01],\n",
       "       [ 7.08584325e+00],\n",
       "       [-4.68268073e+00],\n",
       "       [ 3.40563244e+00],\n",
       "       [-1.57986882e+01],\n",
       "       [ 4.14079284e+00],\n",
       "       [-4.91518180e+00],\n",
       "       [ 5.11793071e+00],\n",
       "       [-1.22361632e+01],\n",
       "       [-3.69804169e-02],\n",
       "       [ 5.18065862e+00],\n",
       "       [ 3.38958003e+00],\n",
       "       [-1.83400511e+01],\n",
       "       [ 4.96096808e+00],\n",
       "       [ 3.77633159e+00],\n",
       "       [ 3.25992820e+00],\n",
       "       [-2.02124082e+01],\n",
       "       [ 5.63467018e+00],\n",
       "       [-4.19411343e+00],\n",
       "       [ 8.02197488e-01],\n",
       "       [-1.45513386e+01],\n",
       "       [ 6.31249614e+00],\n",
       "       [-3.22033318e+00],\n",
       "       [ 2.72460227e+00],\n",
       "       [-2.03104137e+01],\n",
       "       [ 5.67700587e+00],\n",
       "       [-1.01432231e+01],\n",
       "       [ 9.22507099e-01],\n",
       "       [-1.06319961e+01],\n",
       "       [ 5.49265555e+00],\n",
       "       [-9.02872977e+00],\n",
       "       [ 5.45522361e+00],\n",
       "       [-1.46500951e+01],\n",
       "       [ 2.26437412e+00],\n",
       "       [ 6.61975036e+00],\n",
       "       [ 5.87209204e+00],\n",
       "       [-1.54244856e+01],\n",
       "       [ 3.23704291e-01],\n",
       "       [-6.99830241e+00],\n",
       "       [ 2.87531870e+00],\n",
       "       [-1.03186924e+01],\n",
       "       [ 1.74041107e+00],\n",
       "       [-3.92572164e+00],\n",
       "       [ 4.44670459e+00],\n",
       "       [-9.37841705e+00],\n",
       "       [-6.38697413e-01],\n",
       "       [-9.97327702e+00],\n",
       "       [ 1.96180537e+00],\n",
       "       [-1.43494304e+01],\n",
       "       [ 1.04631858e-02],\n",
       "       [-5.03882052e+00],\n",
       "       [ 4.27030012e+00],\n",
       "       [-1.36410923e+01],\n",
       "       [ 1.57313344e+00],\n",
       "       [-5.94868763e+00],\n",
       "       [ 7.34174279e+00],\n",
       "       [-1.70516329e+01],\n",
       "       [ 5.61193247e+00],\n",
       "       [-1.08734712e+01],\n",
       "       [ 3.41869592e+00],\n",
       "       [-1.02482965e+01],\n",
       "       [-3.18911839e-01],\n",
       "       [-6.00170477e+00]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.dot(test.reshape(84,5),log_point_2.reshape(5,1))\n",
    "# # # type(abd)\n",
    "# log_point_2.reshape(5,1)\n",
    "# X_test\n",
    "# logi.w_\n",
    "# z\n",
    "# test.shape\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.87110737e-01],\n",
       "       [2.82239901e-05],\n",
       "       [5.54678959e-01],\n",
       "       [1.73492940e-03],\n",
       "       [6.55416779e-01],\n",
       "       [2.69779049e-03],\n",
       "       [4.18553898e-01],\n",
       "       [6.33842942e-03],\n",
       "       [9.92556282e-01],\n",
       "       [9.69158514e-06],\n",
       "       [9.45690116e-01],\n",
       "       [2.17641090e-04],\n",
       "       [9.95409189e-01],\n",
       "       [9.38540425e-07],\n",
       "       [3.38380099e-01],\n",
       "       [9.94865550e-01],\n",
       "       [9.74944318e-01],\n",
       "       [7.48091419e-08],\n",
       "       [9.42273096e-01],\n",
       "       [3.90216578e-03],\n",
       "       [9.91813140e-01],\n",
       "       [5.71499842e-07],\n",
       "       [9.99163831e-01],\n",
       "       [9.16931819e-03],\n",
       "       [9.67880100e-01],\n",
       "       [1.37631183e-07],\n",
       "       [9.84338939e-01],\n",
       "       [7.28098285e-03],\n",
       "       [9.94047246e-01],\n",
       "       [4.85176463e-06],\n",
       "       [4.90755949e-01],\n",
       "       [9.94407155e-01],\n",
       "       [9.67377294e-01],\n",
       "       [1.08396939e-08],\n",
       "       [9.93042602e-01],\n",
       "       [9.77606393e-01],\n",
       "       [9.63028234e-01],\n",
       "       [1.66672007e-09],\n",
       "       [9.96440858e-01],\n",
       "       [1.48599604e-02],\n",
       "       [6.90444349e-01],\n",
       "       [4.79108365e-07],\n",
       "       [9.98189782e-01],\n",
       "       [3.84076784e-02],\n",
       "       [9.38462853e-01],\n",
       "       [1.51112151e-09],\n",
       "       [9.96587888e-01],\n",
       "       [3.93402493e-05],\n",
       "       [7.15552669e-01],\n",
       "       [2.41308317e-05],\n",
       "       [9.95899982e-01],\n",
       "       [1.19900338e-04],\n",
       "       [9.95744268e-01],\n",
       "       [4.34054597e-07],\n",
       "       [9.05883226e-01],\n",
       "       [9.98668013e-01],\n",
       "       [9.97190939e-01],\n",
       "       [2.00092551e-07],\n",
       "       [5.80226750e-01],\n",
       "       [9.12597691e-04],\n",
       "       [9.46612779e-01],\n",
       "       [3.30091605e-05],\n",
       "       [8.50739272e-01],\n",
       "       [1.93462348e-02],\n",
       "       [9.88418585e-01],\n",
       "       [8.45217575e-05],\n",
       "       [3.45541050e-01],\n",
       "       [4.66273328e-05],\n",
       "       [8.76728202e-01],\n",
       "       [5.86301890e-07],\n",
       "       [5.02615773e-01],\n",
       "       [6.43965061e-03],\n",
       "       [9.86215092e-01],\n",
       "       [1.19055206e-06],\n",
       "       [8.28229847e-01],\n",
       "       [2.60247207e-03],\n",
       "       [9.99352499e-01],\n",
       "       [3.93160538e-08],\n",
       "       [9.96359302e-01],\n",
       "       [1.89541020e-05],\n",
       "       [9.68283747e-01],\n",
       "       [3.54165291e-05],\n",
       "       [4.20940964e-01],\n",
       "       [2.46842189e-03]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "    \n",
    "h = sigmoid(z)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.51112151e-09]),\n",
       " array([1.66672007e-09]),\n",
       " array([1.08396939e-08]),\n",
       " array([3.93160538e-08]),\n",
       " array([7.48091419e-08]),\n",
       " array([1.37631183e-07]),\n",
       " array([2.00092551e-07]),\n",
       " array([4.34054597e-07]),\n",
       " array([4.79108365e-07]),\n",
       " array([5.71499842e-07]),\n",
       " array([5.8630189e-07]),\n",
       " array([9.38540425e-07]),\n",
       " array([1.19055206e-06]),\n",
       " array([4.85176463e-06]),\n",
       " array([9.69158514e-06]),\n",
       " array([1.8954102e-05]),\n",
       " array([2.41308317e-05]),\n",
       " array([2.82239901e-05]),\n",
       " array([3.30091605e-05]),\n",
       " array([3.54165291e-05]),\n",
       " array([3.93402493e-05]),\n",
       " array([4.66273328e-05]),\n",
       " array([8.45217575e-05]),\n",
       " array([0.0001199]),\n",
       " array([0.00021764]),\n",
       " array([0.0009126]),\n",
       " array([0.00173493]),\n",
       " array([0.00246842]),\n",
       " array([0.00260247]),\n",
       " array([0.00269779]),\n",
       " array([0.00390217]),\n",
       " array([0.00633843]),\n",
       " array([0.00643965]),\n",
       " array([0.00728098]),\n",
       " array([0.00916932]),\n",
       " array([0.01485996]),\n",
       " array([0.01934623]),\n",
       " array([0.03840768]),\n",
       " array([0.3383801]),\n",
       " array([0.34554105]),\n",
       " array([0.4185539]),\n",
       " array([0.42094096]),\n",
       " array([0.49075595]),\n",
       " array([0.50261577]),\n",
       " array([0.55467896]),\n",
       " array([0.58022675]),\n",
       " array([0.65541678]),\n",
       " array([0.69044435]),\n",
       " array([0.71555267]),\n",
       " array([0.82822985]),\n",
       " array([0.85073927]),\n",
       " array([0.8767282]),\n",
       " array([0.90588323]),\n",
       " array([0.93846285]),\n",
       " array([0.9422731]),\n",
       " array([0.94569012]),\n",
       " array([0.94661278]),\n",
       " array([0.96302823]),\n",
       " array([0.96737729]),\n",
       " array([0.9678801]),\n",
       " array([0.96828375]),\n",
       " array([0.97494432]),\n",
       " array([0.97760639]),\n",
       " array([0.98433894]),\n",
       " array([0.98621509]),\n",
       " array([0.98711074]),\n",
       " array([0.98841858]),\n",
       " array([0.99181314]),\n",
       " array([0.99255628]),\n",
       " array([0.9930426]),\n",
       " array([0.99404725]),\n",
       " array([0.99440715]),\n",
       " array([0.99486555]),\n",
       " array([0.99540919]),\n",
       " array([0.99574427]),\n",
       " array([0.99589998]),\n",
       " array([0.9963593]),\n",
       " array([0.99644086]),\n",
       " array([0.99658789]),\n",
       " array([0.99719094]),\n",
       " array([0.99818978]),\n",
       " array([0.99866801]),\n",
       " array([0.99916383]),\n",
       " array([0.9993525])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x268f34707f0>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sorted(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi.w\n",
    "log_point = logi.w\n",
    "log_point = log_point[0][0]\n",
    "log_point_2 = log_point\n",
    "log_point_2\n",
    "\n",
    "\n",
    "z = np.dot(test.reshape(84,5),log_point_2.reshape(5,1))\n",
    "# print(z)\n",
    "\n",
    "h1 = sigmoid(z)\n",
    "# h1\n",
    "# print(test_labels)\n",
    "# print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds: [-9.99350990e-04  1.51112151e-09  9.99354012e-04 ...  9.97353797e-01\n",
      "  9.98353150e-01  9.99352502e-01]\n"
     ]
    }
   ],
   "source": [
    "ab = ROC(actuals,sorted(h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x268f34cad60>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ab['FPR'],ab['TPR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x268f35316a0>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# acc\n",
    "plt.plot(acc['THR'],acc['ACC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds: [-9.99350990e-04  1.51112151e-09  9.99354012e-04 ...  9.97353797e-01\n",
      "  9.98353150e-01  9.99352502e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x268f35ad1c0>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = apply(actuals,sorted(h), ACC=ACC)\n",
    "plt.plot(acc['THR'],acc['ACC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
