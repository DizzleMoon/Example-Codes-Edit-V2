{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Iterable, Tuple, Callable\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from functools import partial, reduce\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    m = np.sum(vectors,axis=0)\n",
    "    vec_mean = np.multiply(1/n,m)\n",
    "    return vec_mean\n",
    "\n",
    "# Standard deviation                        \n",
    "def standard_deviation(xs):\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    std_dev = np.sqrt(variance(xs)) \n",
    "    return std_dev\n",
    "\n",
    "def variance(xs):\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    vari = sum_of_squares(deviations)/(n-1)\n",
    "    return vari\n",
    "\n",
    "def de_mean(xs):\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = np.mean(xs)\n",
    "    d_mean = [x - x_bar for x in xs]\n",
    "    return d_mean\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return np.dot(v, v)\n",
    "\n",
    "def\tvector_add(v,\tw):\n",
    "\t\t\t\t\"\"\"adds\tcorresponding\telements\"\"\"\n",
    "\t\t\t\treturn\t[v_i\t+\tw_i\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tfor\tv_i,\tw_i\tin\tzip(v,\tw)]\n",
    "        \n",
    "def\tstep(v,\tdirection,\tstep_size):\n",
    "        \"\"\"move\tstep_size\tin\tthe\tdirection\tfrom\tv\"\"\"\n",
    "        return\t[v_i\t+\tstep_size\t*\tdirection_i\n",
    "                                        for\tv_i,\tdirection_i\tin\tzip(v,\tdirection)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def\tminimize_batch(target_fn,\tgradient_fn,\ttheta_0,\ttolerance=0.000001):\n",
    "    \"\"\"use\tgradient\tdescent\tto\tfind\ttheta\tthat\tminimizes\ttarget\tfunction\"\"\"\n",
    "    step_sizes\t=\t[10,1,0,\t0.1,\t0.01,\t0.001,\t0.0001,\t0.00001, 0.000001, 0.0000001]\n",
    "    #                 step_sizes\t=\t[100,1,\t0.01,\t0.0001,\t0.00001,\t0.0000001,\t0.000000001, 0.00000000001, 0.0000000000001]\n",
    "    theta\t=\ttheta_0\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tset\ttheta\tto\tinitial\tvalue\n",
    "    target_fn\t=\tsafe(target_fn)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tsafe\tversion\tof\ttarget_fn\n",
    "    value\t=\ttarget_fn(theta)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tvalue\twe're\tminimizing\n",
    "    while\tTrue:\n",
    "        gradient\t=\tgradient_fn(theta)\n",
    "        next_thetas\t=\t[step(theta,\tgradient,\t-step_size) for\tstep_size\tin\tstep_sizes]\n",
    "        #\tchoose\tthe\tone\tthat\tminimizes\tthe\terror\tfunction\n",
    "        next_theta\t=\tmax(next_thetas,\tkey=target_fn)\n",
    "        next_value\t=\ttarget_fn(next_theta)\n",
    "        #\tstop\tif\twe're\t\"converging\"\n",
    "        if\tabs(value\t-\tnext_value)\t<\ttolerance:\n",
    "            return\ttheta\n",
    "        else:\n",
    "            theta,\tvalue\t=\tnext_theta,\tnext_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def\tnegate(f):\n",
    "\t\t\t\t\"\"\"return\ta\tfunction\tthat\tfor\tany\tinput\tx\treturns\t-f(x)\"\"\"\n",
    "\t\t\t\treturn\tlambda\t*args,\t**kwargs:\t-f(*args,\t**kwargs)\n",
    "def\tnegate_all(f):\n",
    "\t\t\t\t\"\"\"the\tsame\twhen\tf\treturns\ta\tlist\tof\tnumbers\"\"\"\n",
    "\t\t\t\treturn\tlambda\t*args,\t**kwargs:\t[-y\tfor\ty\tin\tf(*args,\t**kwargs)]\n",
    "def\tmaximize_batch(target_fn,\tgradient_fn,\ttheta_0,\ttolerance=0.000001):\n",
    "\t\t\t\treturn\tminimize_batch(target_fn, gradient_fn, theta_0, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(A):\n",
    "    \"\"\"Returns (# of rows of A, # of columns of A)\"\"\"\n",
    "    num_rows = len(A)\n",
    "    num_cols = len(A[0]) if A else 0   # number of elements in first row\n",
    "    return num_rows, num_cols\n",
    "\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\"returns the mean and standard deviation for each position\"\"\"\n",
    "    dim = len(data[0])\n",
    "\n",
    "    means = vector_mean(data)\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "\n",
    "    return means, stdevs\n",
    "\n",
    "def rescale(data):\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = len(data[0])\n",
    "    means, stdevs = scale(data)\n",
    "    \n",
    "    means = list(means)\n",
    "    stdevs = list(stdevs)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "    v0 = []\n",
    "    for v in rescaled:\n",
    "        v = list(v)\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "        v0.append(v)\n",
    "\n",
    "    return v0\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "     # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train \n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-5    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0 / (1 + np.exp(-(x + epsilon)))\n",
    "\n",
    "def logistic_prime(x):\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_log_likelihood_i(x_i, y_i, beta):\n",
    "    if y_i==1:\n",
    "        return np.log(expit(np.dot(x_i, beta)) + epsilon)\n",
    "    else:\n",
    "        return np.log(1 - expit(np.dot(x_i, beta)) + epsilon)\n",
    "    \n",
    "def logistic_log_likelihood(x, y, beta):\n",
    "    return sum(logistic_log_likelihood_i(x_i, y_i, beta)\n",
    "        for x_i, y_i in zip(x, y))\n",
    "\n",
    "def logistic_log_partial_ij(x_i, y_i, beta, j):\n",
    "    \"\"\"here i is the index of the data point, j the index of the derivative\"\"\"\n",
    "    return (y_i - expit(np.dot(x_i, beta))) * x_i[j]\n",
    "\n",
    "def logistic_log_gradient_i(x_i, y_i, beta):\n",
    "    \"\"\"the gradient of the log likelihood corresponding to the ith data point\"\"\"\n",
    "    return [logistic_log_partial_ij(x_i, y_i, beta, j) for j, _ in enumerate(beta)]\n",
    "\n",
    "def logistic_log_gradient(x, y, beta):\n",
    "    return reduce(vector_add,\n",
    "        [logistic_log_gradient_i(x_i, y_i, beta) for x_i, y_i in zip(x,y)])\n",
    "\n",
    "def safe(f):\n",
    "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')         # this means \"infinity\" in Python\n",
    "    return safe_f\n",
    "                                                                \n",
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # use expit as alternative\n",
    "    return 1.0 / (1 + np.exp(-(x+epsilon)))\n",
    "\n",
    "def predictOne(x,w):\n",
    "    return max((x.dot(w), c) for w, c in w)[1]\n",
    "\n",
    "def predict(X,w):\n",
    "    return np.array([predictOne(i,w) for i in X])\n",
    "\n",
    "def predict2(X,w):\n",
    "    output = np.insert(X, 0, 1, axis=1).dot(w)\n",
    "    return (np.floor(sigmoid(output) + .5)).astype(int)\n",
    "\n",
    "def score(X, y, w):\n",
    "    return sum(predict(X,w) == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=3)\n",
    "breasts = datasets.load_breast_cancer()\n",
    "X = breasts.data\n",
    "y = breasts.target\n",
    "# x = [(1) + X]\n",
    "# x\n",
    "X1 = np.insert(list(X), 0, 1, axis=1)\n",
    "X1\n",
    "\n",
    "x = []\n",
    "for i in X1:\n",
    "    x.append(list(i))\n",
    "    \n",
    "# x\n",
    "    \n",
    "# x = [[1] + list(row) for row in X]\n",
    "# x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [(0.7,48000,1),(1.9,48000,0),(2.5,60000,1),(4.2,63000,0),(6,76000,0),(6.5,69000,0),(7.5,76000,0),(8.1,88000,0),(8.7,83000,1),(10,83000,1),(0.8,43000,0),(1.8,60000,0),(10,79000,1),(6.1,76000,0),(1.4,50000,0),(9.1,92000,0),(5.8,75000,0),(5.2,69000,0),(1,56000,0),(6,67000,0),(4.9,74000,0),(6.4,63000,1),(6.2,82000,0),(3.3,58000,0),(9.3,90000,1),(5.5,57000,1),(9.1,102000,0),(2.4,54000,0),(8.2,65000,1),(5.3,82000,0),(9.8,107000,0),(1.8,64000,0),(0.6,46000,1),(0.8,48000,0),(8.6,84000,1),(0.6,45000,0),(0.5,30000,1),(7.3,89000,0),(2.5,48000,1),(5.6,76000,0),(7.4,77000,0),(2.7,56000,0),(0.7,48000,0),(1.2,42000,0),(0.2,32000,1),(4.7,56000,1),(2.8,44000,1),(7.6,78000,0),(1.1,63000,0),(8,79000,1),(2.7,56000,0),(6,52000,1),(4.6,56000,0),(2.5,51000,0),(5.7,71000,0),(2.9,65000,0),(1.1,33000,1),(3,62000,0),(4,71000,0),(2.4,61000,0),(7.5,75000,0),(9.7,81000,1),(3.2,62000,0),(7.9,88000,0),(4.7,44000,1),(2.5,55000,0),(1.6,41000,0),(6.7,64000,1),(6.9,66000,1),(7.9,78000,1),(8.1,102000,0),(5.3,48000,1),(8.5,66000,1),(0.2,56000,0),(6,69000,0),(7.5,77000,0),(8,86000,0),(4.4,68000,0),(4.9,75000,0),(1.5,60000,0),(2.2,50000,0),(3.4,49000,1),(4.2,70000,0),(7.7,98000,0),(8.2,85000,0),(5.4,88000,0),(0.1,46000,0),(1.5,37000,0),(6.3,86000,0),(3.7,57000,0),(8.4,85000,0),(2,42000,0),(5.8,69000,1),(2.7,64000,0),(3.1,63000,0),(1.9,48000,0),(10,72000,1),(0.2,45000,0),(8.6,95000,0),(1.5,64000,0),(9.8,95000,0),(5.3,65000,0),(7.5,80000,0),(9.9,91000,0),(9.7,50000,1),(2.8,68000,0),(3.6,58000,0),(3.9,74000,0),(4.4,76000,0),(2.5,49000,0),(7.2,81000,0),(5.2,60000,1),(2.4,62000,0),(8.9,94000,0),(2.4,63000,0),(6.8,69000,1),(6.5,77000,0),(7,86000,0),(9.4,94000,0),(7.8,72000,1),(0.2,53000,0),(10,97000,0),(5.5,65000,0),(7.7,71000,1),(8.1,66000,1),(9.8,91000,0),(8,84000,0),(2.7,55000,0),(2.8,62000,0),(9.4,79000,0),(2.5,57000,0),(7.4,70000,1),(2.1,47000,0),(5.3,62000,1),(6.3,79000,0),(6.8,58000,1),(5.7,80000,0),(2.2,61000,0),(4.8,62000,0),(3.7,64000,0),(4.1,85000,0),(2.3,51000,0),(3.5,58000,0),(0.9,43000,0),(0.9,54000,0),(4.5,74000,0),(6.5,55000,1),(4.1,41000,1),(7.1,73000,0),(1.1,66000,0),(9.1,81000,1),(8,69000,1),(7.3,72000,1),(3.3,50000,0),(3.9,58000,0),(2.6,49000,0),(1.6,78000,0),(0.7,56000,0),(2.1,36000,1),(7.5,90000,0),(4.8,59000,1),(8.9,95000,0),(6.2,72000,0),(6.3,63000,0),(9.1,100000,0),(7.3,61000,1),(5.6,74000,0),(0.5,66000,0),(1.1,59000,0),(5.1,61000,0),(6.2,70000,0),(6.6,56000,1),(6.3,76000,0),(6.5,78000,0),(5.1,59000,0),(9.5,74000,1),(4.5,64000,0),(2,54000,0),(1,52000,0),(4,69000,0),(6.5,76000,0),(3,60000,0),(4.5,63000,0),(7.8,70000,0),(3.9,60000,1),(0.8,51000,0),(4.2,78000,0),(1.1,54000,0),(6.2,60000,0),(2.9,59000,0),(2.1,52000,0),(8.2,87000,0),(4.8,73000,0),(2.2,42000,1),(9.1,98000,0),(6.5,84000,0),(6.9,73000,0),(5.1,72000,0),(9.1,69000,1),(9.8,79000,1),]\n",
    "# x = [(1,) + row[:2] for row in data]\n",
    "# y = [row[2] for row in data]\n",
    "# # xx = [row[:2] for row in data]\n",
    "# x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "#     \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
    "    \n",
    "#     step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "#     theta = theta_0\n",
    "#     target_fn = safe(target_fn)\n",
    "#     value = target_fn(theta)\n",
    "#     values = []\n",
    "    \n",
    "#     # set theta to initial value\n",
    "#     # safe version of target_fn\n",
    "#     # value we're minimizing\n",
    "#     while True:\n",
    "#         values.append(value)\n",
    "#         gradient = gradient_fn(theta)\n",
    "#         next_thetas = [np.array(step(theta, gradient, -step_size))\n",
    "#                        for step_size in step_sizes]\n",
    "\n",
    "#         # choose the one that minimizes the error function\n",
    "#         next_theta = min(next_thetas, key=target_fn)\n",
    "#         next_value = target_fn(next_theta)\n",
    "\n",
    "#         # stop if we're \"converging\"\n",
    "#         if abs(value - next_value) < tolerance:\n",
    "#             values.append(next_value)\n",
    "#             break\n",
    "#         else:\n",
    "#             theta, value = next_theta, next_value\n",
    "\n",
    "#     return theta, values\n",
    "    \n",
    "    \n",
    "# def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "#     return minimize_batch(negate(target_fn),\n",
    "#                           negate_all(gradient_fn),\n",
    "#                           theta_0,\n",
    "#                           tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize2(x, y,learning_rate,iterations,tolerance): \n",
    "    size = x.shape[0]\n",
    "    size1 = len(y)\n",
    "    print('size1:\\n', size1)\n",
    "#     weight = parameters[\"weight\"] \n",
    "#     bias = parameters[\"bias\"]\n",
    "    w1 = []\n",
    "    losses = []\n",
    "#     wg = np.ones(x.shape[1])\n",
    "    \n",
    "    for i in np.unique(y):\n",
    "        y_copy = [1 if c == i else 0 for c in y]\n",
    "        wg = np.ones(x.shape[1])\n",
    "        wg_0 = 0\n",
    "        loss_0 = np.ones(len(y))\n",
    "        iteration = 0\n",
    "        diff = 0.01\n",
    "        diff_1 = 0.01\n",
    "        diff_3 = 0.1\n",
    "        decay = learning_rate/iterations\n",
    "        decay = decay*learning_rate * 0.1\n",
    "#         print('wg:\\n', wg)\n",
    "#         print('wg len:\\n', len(wg))\n",
    "\n",
    "        for j in range(iterations):\n",
    "            iteration += 1\n",
    "            sigma = expit(np.dot(x, wg))\n",
    "            loss = -1/size * np.sum(y * np.log(sigma)) + (1 - y) * np.log(1-sigma)\n",
    "            loss[np.isnan(loss)] = 0\n",
    "            loss[np.isinf(loss)] = 0\n",
    "#             print('Loss1:\\n', loss)\n",
    "#             if np.all(np.isnan(abs(loss))) or np.all(np.isinf(abs(loss))) or np.any(loss == float('-inf')):\n",
    "#                 loss = 0\n",
    "#             print('loss: \\n', loss)\n",
    "#             print('loss len: \\n', len(loss))\n",
    "            dW = 1/size * np.dot(x.T, (y_copy - sigma))\n",
    "#             db = 1/size * np.sum(sigma - y)\n",
    "            wg += learning_rate * dW\n",
    "#             bias -= learning_rate * db \n",
    "            diff_0 = np.sum(wg) - wg_0 \n",
    "            diff_2 = np.abs(diff_0 - diff_1)\n",
    "            diff = np.abs(diff_2 - diff_3)\n",
    "            diff_3 = diff_2\n",
    "            print('Diff: ', diff)\n",
    "            diff_1 = diff_0\n",
    "#                 if np.isnan(abs(diff)) or np.isinf(abs(diff)):\n",
    "#                     diff = 0\n",
    "            wg_0 = np.sum(wg)\n",
    "    \n",
    "#             if diff == 0.013568115566890526:\n",
    "# #             if diff  0.013568115566890526:\n",
    "#                 print('Exit')\n",
    "#                 break\n",
    "\n",
    "            if diff <= tolerance:\n",
    "                print('Exit')\n",
    "                break\n",
    "            else:\n",
    "                learning_rate = learning_rate * (1/(1 + decay * iteration))\n",
    "            \n",
    "\n",
    "#             for i in range(0,len(loss)):\n",
    "#                 diff = np.abs(abs(loss_0[i]) - abs(loss[i]))\n",
    "# #                 if np.isnan(abs(diff)) or np.isinf(abs(diff)):\n",
    "# #                     diff = 0\n",
    "#                 loss_0[i] = loss[i]\n",
    "#                 if np.all(np.abs(diff) <= 1e-4):\n",
    "#                     break\n",
    "#                 print('Difference: \\n', diff)\n",
    "#         print(wg)\n",
    "        w1.append((wg,i))\n",
    "        losses.append((loss,i))\n",
    "        print(\"Iterations: \", iteration)\n",
    "#     parameters[\"weight\"] = w1\n",
    "#     parameters[\"bias\"] = bias\n",
    "    return w1,losses\n",
    "\n",
    "# Define the train function\n",
    "# def train(x, y, learning_rate,iterations):\n",
    "#     parameters_out = optimize2(x, y, learning_rate, iterations ,init_parameters)\n",
    "#     return parameters_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "rescaled_x = rescale(X1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x , y, 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weight and bais\n",
    "init_parameters = {} \n",
    "# init_parameters[\"weight\"] = np.zeros((X1.shape[1],1))\n",
    "init_parameters[\"weight\"] = np.zeros((X.shape[1],2))\n",
    "init_parameters[\"weight\"]\n",
    "init_parameters[\"bias\"] = 0\n",
    "# init_parameters['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size1:\n",
      " 569\n",
      "Diff:  30.88999117352336\n",
      "Diff:  0.01000882647666046\n",
      "Diff:  31.000000000000004\n",
      "Diff:  0.0\n",
      "Exit\n",
      "Iterations:  4\n",
      "Diff:  30.88985922740205\n",
      "Diff:  0.010140772593363323\n",
      "Diff:  30.99999999999084\n",
      "Diff:  1.7763568394002505e-14\n",
      "Diff:  1.4210854715202004e-14\n",
      "Diff:  1.0658141036401503e-14\n",
      "Diff:  7.105427357601002e-15\n",
      "Diff:  1.7763568394002505e-14\n",
      "Diff:  2.842170943040401e-14\n",
      "Diff:  1.0658141036401503e-14\n",
      "Diff:  1.4210854715202004e-14\n",
      "Diff:  1.0658141036401503e-14\n",
      "Diff:  3.552713678800501e-15\n",
      "Diff:  1.7763568394002505e-14\n",
      "Diff:  2.1316282072803006e-14\n",
      "Diff:  7.105427357601002e-15\n",
      "Diff:  3.552713678800501e-15\n",
      "Diff:  0.0\n",
      "Exit\n",
      "Iterations:  18\n",
      "Weights: \n",
      " [(array([0.99999835, 1.00000245, 1.00000133, 1.00000218, 1.0000022 ,\n",
      "       0.99999773, 0.99999714, 0.99999841, 0.99999988, 0.99999751,\n",
      "       0.99999434, 1.00000031, 0.99999812, 0.99999993, 1.00000084,\n",
      "       0.99999613, 0.99999455, 0.99999519, 0.99999604, 0.99999719,\n",
      "       0.99999353, 1.00000249, 1.00000179, 1.00000218, 1.00000217,\n",
      "       0.99999895, 0.9999983 , 0.99999867, 1.00000017, 1.00000007,\n",
      "       0.99999653]), 0), (array([1.00003846, 0.99988407, 0.99993379, 0.99988068, 0.99988661,\n",
      "       0.99992744, 0.99988338, 0.99987176, 0.99986439, 0.99993132,\n",
      "       0.99997678, 0.99990275, 0.999993  , 0.99990298, 0.99990843,\n",
      "       0.99999424, 0.99992452, 0.99993423, 0.99991123, 0.99998848,\n",
      "       0.99995734, 0.9998762 , 0.99992862, 0.99987364, 0.99988215,\n",
      "       0.99992198, 0.99988958, 0.99987931, 0.99986277, 0.99992792,\n",
      "       0.99992805]), 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17748/2795118072.py:28: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -1/size * np.sum(y * np.log(sigma)) + (1 - y) * np.log(1-sigma)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17748/2795118072.py:28: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -1/size * np.sum(y * np.log(sigma)) + (1 - y) * np.log(1-sigma)\n"
     ]
    }
   ],
   "source": [
    "rescaled_x = rescale(X1)\n",
    "\n",
    "# Train the model\n",
    "y = np.array(y)\n",
    "x = np.array(rescaled_x)\n",
    "# print(x)\n",
    "parameters_out,losses = optimize2(x, y, learning_rate = 0.00001, iterations = 10000, tolerance = 1e-15)\n",
    "# parameters_out,losses = optimize2(x, y, learning_rate = 0.00001, iterations = 10000, tolerance = 1e-15)\n",
    "print('Weights: \\n',parameters_out)\n",
    "# print('Losses: \\n', losses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31,)\n",
      "(31,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17748/2694799327.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-(x+epsilon)))\n"
     ]
    }
   ],
   "source": [
    "X1 = np.insert(X, 0, 1, axis=1)\n",
    "# X1 = X\n",
    "wg = []\n",
    "\n",
    "for i in np.unique(y):\n",
    "    y_copy = [1 if c == i else 0 for c in y]\n",
    "    w = np.ones(X1.shape[1])\n",
    "    print(w.shape)\n",
    "    eta = 0.001\n",
    "    size = X1.shape[0]\n",
    "\n",
    "    # print('training ', i)\n",
    "    # counter = 0\n",
    "\n",
    "    for _ in range(1000):\n",
    "        output = X1.dot(w)\n",
    "        errors = y_copy - sigmoid(output)\n",
    "        w += eta * 1/size * errors.T.dot(X1)\n",
    "#         diff = -eta * 1/size * errors.T.dot(X1)\n",
    "        \n",
    "#         if np.all(np.abs(diff) <= 1e-06):\n",
    "#             break\n",
    "#         w += diff\n",
    "\n",
    "        # counter += 1\n",
    "        # if counter // 10 == 0:\n",
    "        #     print(sum(errors**2) / 2.0)\n",
    "    wg.append((w, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 0.9361783 ,  0.51567161,  0.10742403, -1.90289952, -0.91709114,\n",
       "          0.99485819,  0.99933824,  1.00489222,  1.00226116,  0.99014219,\n",
       "          0.99607887,  0.99130718,  0.92937902,  0.95979098,  1.5349912 ,\n",
       "          0.99961113,  0.99968841,  0.99979371,  0.99983619,  0.99889394,\n",
       "          0.99982796,  0.47944857, -0.16614955, -2.05339775,  1.11925872,\n",
       "          0.99322903,  1.00109009,  1.00853844,  1.00172408,  0.98584062,\n",
       "          0.99576648]),\n",
       "  0),\n",
       " (array([ 1.01613229,  1.11999649,  0.97605182,  1.60518168,  0.47003344,\n",
       "          1.0005089 ,  0.99683924,  0.99407594,  0.99764168,  1.00098844,\n",
       "          1.00057859,  0.99488348,  0.992323  ,  0.94764371,  0.04490092,\n",
       "          0.99993833,  0.99896574,  0.9985841 ,  0.99968224,  0.99983116,\n",
       "          0.99990964,  1.12053659,  0.93605465,  1.49576883, -0.86082199,\n",
       "          1.00029509,  0.98971269,  0.98547037,  0.99632577,  1.00024291,\n",
       "          0.99990332]),\n",
       "  1)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape\n",
    "wg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9226713532513181"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rescaled_x = rescale(X1)\n",
    "rescaled_x = rescale(x)\n",
    "rescaled_x = np.array(rescaled_x)\n",
    "score(X1,y,wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=3)\n",
    "breasts = datasets.load_breast_cancer()\n",
    "X = breasts.data\n",
    "y = breasts.target\n",
    "# x = [(1) + X]\n",
    "# x\n",
    "X1 = np.insert(list(X), 0, 1, axis=1)\n",
    "X1\n",
    "\n",
    "x = []\n",
    "for i in X1:\n",
    "    x.append(list(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = (x - np.min(x))/(np.max(x) - np.min(x))\n",
    "x_norm = x_norm.tolist()\n",
    "y_norm = (y - np.min(y))/(np.max(y) - np.min(y))\n",
    "y_norm = y_norm.tolist()\n",
    "\n",
    "# x_norm = (x - np.mean(x))/np.std(x)\n",
    "# y_norm = (y - np.mean(y))/np.std(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "# rescaled_x = rescale(x_norm)\n",
    "rescaled_x = rescale(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(rescaled_x, y, 0.33)\n",
    "# type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimize3(x, y,learning_rate,iterations): \n",
    "#     size = len(x)\n",
    "#     size1 = len(y)\n",
    "#     print('size1:\\n', size1)\n",
    "# #     weight = parameters[\"weight\"] \n",
    "# #     bias = parameters[\"bias\"]\n",
    "#     w1 = []\n",
    "#     losses = []\n",
    "# #     wg = np.ones(x.shape[1])\n",
    "#     x = np.array(x)\n",
    "#     y = np.array(y)\n",
    "    \n",
    "#     for i in np.unique(y):\n",
    "#         y_copy = [1 if c == i else 0 for c in y]\n",
    "#         wg = np.ones(len(x))\n",
    "#         loss_0 = np.ones(len(y))\n",
    "# #         print('wg:\\n', wg)\n",
    "# #         print('wg len:\\n', len(wg))\n",
    "\n",
    "#         for _ in range(iterations): \n",
    "#             sigma = sigmoid(np.dot(x.T, wg))\n",
    "#             loss = -1/size * np.sum(y * np.log(sigma)) + (1 - y) * np.log(1-sigma)\n",
    "#             loss[np.isnan(loss)] = 0\n",
    "#             loss[np.isinf(loss)] = 0\n",
    "# #             print('Loss1:\\n', loss)\n",
    "# #             if np.all(np.isnan(abs(loss))) or np.all(np.isinf(abs(loss))) or np.any(loss == float('-inf')):\n",
    "# #                 loss = 0\n",
    "# #             print('loss: \\n', loss)\n",
    "# #             print('loss len: \\n', len(loss))\n",
    "#             dW = 1/size * np.dot(x.T, (y_copy - sigma))\n",
    "# #             db = 1/size * np.sum(sigma - y)\n",
    "#             wg += learning_rate * dW\n",
    "# #             bias -= learning_rate * db \n",
    "#             for i in range(0,len(loss)):\n",
    "#                 diff = np.abs(abs(loss_0[i]) - abs(loss[i]))\n",
    "# #                 if np.isnan(abs(diff)) or np.isinf(abs(diff)):\n",
    "# #                     diff = 0\n",
    "#                 loss_0[i] = loss[i]\n",
    "#                 if np.all(np.abs(diff) <= 1e-4):\n",
    "#                     break\n",
    "# #                 print('Difference: \\n', diff)\n",
    "# #         print(wg)\n",
    "#         w1.append((wg,i))\n",
    "#         losses.append((loss,i))\n",
    "# #     parameters[\"weight\"] = w1\n",
    "# #     parameters[\"bias\"] = bias\n",
    "#     return w1,losses\n",
    "\n",
    "# # Define the train function\n",
    "# # def train(x, y, learning_rate,iterations):\n",
    "# #     parameters_out = optimize2(x, y, learning_rate, iterations ,init_parameters)\n",
    "# #     return parameters_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rescaled_x = rescale(X1)\n",
    "\n",
    "# # Train the model\n",
    "# y = np.array(y)\n",
    "# x = np.array(rescaled_x)\n",
    "# # print(x)\n",
    "# x_train_1 = np.array(x_train)\n",
    "# y_train_1 = np.array(y_train)\n",
    "# parameters_out_1,losses_1 = optimize2(x_train_1, y_train_1, learning_rate = 0.0001, iterations = 10000, tolerance = 1e-12)\n",
    "# # print('Weights: \\n',parameters_out)\n",
    "# # print('Losses: \\n', losses[0])\n",
    "# # print('Weights: \\n',parameters_out)\n",
    "# print('Weights2: \\n',parameters_out_1)\n",
    "# print('Weights2_0: \\n',parameters_out_1[0][0])\n",
    "# print('Weights2_1: \\n',parameters_out_1[1][0])\n",
    "# print('Weights2_0_len: \\n',len(parameters_out_1[0][0]))\n",
    "# print('Weights2_1_len: \\n',len(parameters_out_1[1][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rescaled_x = rescale(X1)\n",
    "\n",
    "# # Train the model\n",
    "# y = np.array(y)\n",
    "# x = np.array(rescaled_x)\n",
    "# # print(x)\n",
    "# x_train_1 = np.array(x_train)\n",
    "# y_train_1 = np.array(y_train)\n",
    "# parameters_out_1,losses_1 = optimize2(x_train_1, y_train_1, learning_rate = 0.0001, iterations = 10000, tolerance = 1e-11)\n",
    "# # print('Weights: \\n',parameters_out)\n",
    "# # print('Losses: \\n', losses[0])\n",
    "# # print('Weights: \\n',parameters_out)\n",
    "# print('Weights2: \\n',parameters_out_1)\n",
    "# print('Weights2_0: \\n',parameters_out_1[0][0])\n",
    "# print('Weights2_1: \\n',parameters_out_1[1][0])\n",
    "# print('Weights2_0_len: \\n',len(parameters_out_1[0][0]))\n",
    "# print('Weights2_1_len: \\n',len(parameters_out_1[1][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size1:\n",
      " 381\n",
      "Diff:  30.889989850958358\n",
      "Diff:  0.010010149041555394\n",
      "Diff:  30.99999999999983\n",
      "Diff:  7.105427357601002e-15\n",
      "Diff:  7.105427357601002e-15\n",
      "Diff:  3.552713678800501e-15\n",
      "Diff:  1.7763568394002505e-14\n",
      "Diff:  2.1316282072803006e-14\n",
      "Diff:  7.105427357601002e-15\n",
      "Diff:  3.552713678800501e-15\n",
      "Diff:  3.552713678800501e-15\n",
      "Diff:  3.552713678800501e-15\n",
      "Diff:  7.105427357601002e-15\n",
      "Diff:  1.7763568394002505e-14\n",
      "Diff:  2.1316282072803006e-14\n",
      "Diff:  1.4210854715202004e-14\n",
      "Diff:  2.1316282072803006e-14\n",
      "Diff:  3.552713678800501e-14\n",
      "Diff:  3.197442310920451e-14\n",
      "Diff:  1.7763568394002505e-14\n",
      "Diff:  3.552713678800501e-15\n",
      "Diff:  7.105427357601002e-15\n",
      "Diff:  3.552713678800501e-15\n",
      "Diff:  1.0658141036401503e-14\n",
      "Diff:  1.0658141036401503e-14\n",
      "Diff:  0.0\n",
      "Exit\n",
      "Iterations:  26\n",
      "Diff:  30.889856453814122\n",
      "Diff:  0.010143546180866991\n",
      "Diff:  30.999999999989985\n",
      "Diff:  3.552713678800501e-15\n",
      "Diff:  1.7763568394002505e-14\n",
      "Diff:  1.4210854715202004e-14\n",
      "Diff:  0.0\n",
      "Exit\n",
      "Iterations:  7\n",
      "Weights2: \n",
      " [(array([0.99999253, 1.00001828, 1.00001138, 1.00001637, 1.00001596,\n",
      "       0.99998377, 0.99998025, 0.99998655, 0.99999918, 0.9999825 ,\n",
      "       0.99996021, 1.00000139, 0.99998907, 0.99999893, 1.00000519,\n",
      "       0.99997709, 0.99995917, 0.9999607 , 0.99997133, 0.99997756,\n",
      "       0.99995   , 1.00001764, 1.00001363, 1.00001529, 1.00001485,\n",
      "       0.99999321, 0.99998637, 0.99998666, 0.99999972, 0.99999825,\n",
      "       0.99997311]), 0), (array([1.00001214, 0.99995461, 0.9999767 , 0.99995315, 0.99995551,\n",
      "       0.99996823, 0.99995288, 0.99994852, 0.99994572, 0.99997002,\n",
      "       0.99998922, 0.99996207, 0.99999765, 0.99996173, 0.99996341,\n",
      "       0.99999789, 0.99997118, 0.99997447, 0.99996547, 0.99999562,\n",
      "       0.99998464, 0.99995191, 0.9999738 , 0.99995072, 0.99995421,\n",
      "       0.99996711, 0.99995611, 0.99995224, 0.99994562, 0.99997049,\n",
      "       0.99997215]), 1)]\n",
      "Weights2_0: \n",
      " [0.99999253 1.00001828 1.00001138 1.00001637 1.00001596 0.99998377\n",
      " 0.99998025 0.99998655 0.99999918 0.9999825  0.99996021 1.00000139\n",
      " 0.99998907 0.99999893 1.00000519 0.99997709 0.99995917 0.9999607\n",
      " 0.99997133 0.99997756 0.99995    1.00001764 1.00001363 1.00001529\n",
      " 1.00001485 0.99999321 0.99998637 0.99998666 0.99999972 0.99999825\n",
      " 0.99997311]\n",
      "Weights2_1: \n",
      " [1.00001214 0.99995461 0.9999767  0.99995315 0.99995551 0.99996823\n",
      " 0.99995288 0.99994852 0.99994572 0.99997002 0.99998922 0.99996207\n",
      " 0.99999765 0.99996173 0.99996341 0.99999789 0.99997118 0.99997447\n",
      " 0.99996547 0.99999562 0.99998464 0.99995191 0.9999738  0.99995072\n",
      " 0.99995421 0.99996711 0.99995611 0.99995224 0.99994562 0.99997049\n",
      " 0.99997215]\n",
      "Weights2_0_len: \n",
      " 31\n",
      "Weights2_1_len: \n",
      " 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17748/2795118072.py:28: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -1/size * np.sum(y * np.log(sigma)) + (1 - y) * np.log(1-sigma)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17748/2795118072.py:28: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -1/size * np.sum(y * np.log(sigma)) + (1 - y) * np.log(1-sigma)\n"
     ]
    }
   ],
   "source": [
    "# rescaled_x = rescale(X1)\n",
    "\n",
    "# Train the model\n",
    "y = np.array(y)\n",
    "x = np.array(rescaled_x)\n",
    "# print(x)\n",
    "x_train_1 = np.array(x_train)\n",
    "y_train_1 = np.array(y_train)\n",
    "parameters_out_1,losses_1 = optimize2(x_train_1, y_train_1, learning_rate = 0.00001, iterations = 10000, tolerance = 1e-15)\n",
    "# print('Weights: \\n',parameters_out)\n",
    "# print('Losses: \\n', losses[0])\n",
    "# print('Weights: \\n',parameters_out)\n",
    "print('Weights2: \\n',parameters_out_1)\n",
    "print('Weights2_0: \\n',parameters_out_1[0][0])\n",
    "print('Weights2_1: \\n',parameters_out_1[1][0])\n",
    "print('Weights2_0_len: \\n',len(parameters_out_1[0][0]))\n",
    "print('Weights2_1_len: \\n',len(parameters_out_1[1][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rescaled_x = rescale(x)\n",
    "\n",
    "# # Train the model\n",
    "# y = np.array(y)\n",
    "# x = np.array(rescaled_x)\n",
    "# # print(x)\n",
    "# parameters_out,losses = optimize2(x, y, learning_rate = 0.01, iterations = 1000)\n",
    "# # print('Weights: \\n',parameters_out)\n",
    "# print('Losses: \\n', losses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(0)\n",
    "# rescaled_x = rescale(x)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x , y, 0.33)\n",
    "# want to maximize log likelihood on the training data\n",
    "fn = partial(logistic_log_likelihood, x_train_1, y_train_1)\n",
    "gradient_fn = partial(logistic_log_gradient, x_train_1, y_train_1)\n",
    "# pick a random starting point\n",
    "# beta_0 = [random.random() for _ in range(len(x_train[0]))]\n",
    "# # and maximize using gradient descent\n",
    "# beta_hat, values = maximize_batch(fn, gradient_fn, beta_0)\n",
    "# beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00001214 0.99995461 0.9999767  0.99995315 0.99995551 0.99996823\n",
      " 0.99995288 0.99994852 0.99994572 0.99997002 0.99998922 0.99996207\n",
      " 0.99999765 0.99996173 0.99996341 0.99999789 0.99997118 0.99997447\n",
      " 0.99996547 0.99999562 0.99998464 0.99995191 0.9999738  0.99995072\n",
      " 0.99995421 0.99996711 0.99995611 0.99995224 0.99994562 0.99997049\n",
      " 0.99997215]\n",
      "[1.00001214 0.99995461 0.9999767  0.99995315 0.99995551 0.99996823\n",
      " 0.99995288 0.99994852 0.99994572 0.99997002 0.99998922 0.99996207\n",
      " 0.99999765 0.99996173 0.99996341 0.99999789 0.99997118 0.99997447\n",
      " 0.99996547 0.99999562 0.99998464 0.99995191 0.9999738  0.99995072\n",
      " 0.99995421 0.99996711 0.99995611 0.99995224 0.99994562 0.99997049\n",
      " 0.99997215]\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# beta_0 = parameters_out['weight']\n",
    "beta_0 = parameters_out_1[1][0]\n",
    "print(beta_0)\n",
    "# X_train = np.array(X_train)\n",
    "# y_train = np.array(y_train)\n",
    "# print(X_train.shape)\n",
    "# print(y_train)\n",
    "# beta_0 = [random.random() for _ in range(3)] # and maximize using gradient descent\n",
    "# list(beta_0)\n",
    "beta_hat_0 = maximize_batch(fn, gradient_fn, beta_0)\n",
    "# beta_hat\n",
    "print(beta_hat_0)\n",
    "print(len(beta_hat_0))\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = partial(logistic_log_likelihood, x_train_1, y_train_1)\n",
    "gradient_fn = partial(logistic_log_gradient, x_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999925325933372, 1.0000182820809804, 1.0000113777632291, 1.0000163692042119, 1.0000159581962735, 0.9999837732841453, 0.9999802490579656, 0.9999865478440229, 0.9999991779708218, 0.9999824992961381, 0.9999602123719105, 1.0000013917605466, 0.9999890675300223, 0.9999989320007276, 1.0000051855418945, 0.9999770929585529, 0.9999591655651925, 0.9999606976187927, 0.9999713273963612, 0.9999775587247264, 0.9999499951181824, 1.0000176430649126, 1.0000136310547414, 1.0000152873340735, 1.0000148508001134, 0.9999932134165423, 0.9999863684287679, 0.9999866621132036, 0.9999997177956469, 0.9999982459779798, 0.9999731110816034]\n",
      "[0.99999253 1.00001828 1.00001138 1.00001637 1.00001596 0.99998377\n",
      " 0.99998025 0.99998655 0.99999918 0.9999825  0.99996021 1.00000139\n",
      " 0.99998907 0.99999893 1.00000519 0.99997709 0.99995917 0.9999607\n",
      " 0.99997133 0.99997756 0.99995    1.00001764 1.00001363 1.00001529\n",
      " 1.00001485 0.99999321 0.99998637 0.99998666 0.99999972 0.99999825\n",
      " 0.99997311]\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# beta_0 = parameters_out['weight']\n",
    "beta_1 = parameters_out_1[0][0]\n",
    "print(list(beta_1))\n",
    "# X_train = np.array(X_train)\n",
    "# y_train = np.array(y_train)\n",
    "# print(X_train.shape)\n",
    "# print(y_train)\n",
    "# beta_0 = [random.random() for _ in range(3)] # and maximize using gradient descent\n",
    "# list(beta_0)\n",
    "beta_hat_1 = maximize_batch(fn, gradient_fn, beta_1)\n",
    "print(beta_hat_1)\n",
    "print(len(beta_hat_1))\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1.0000121363945316, 0.9999546055402168, 0.9999766974113472, 0.9999531477725989, 0.9999555111688334, 0.9999682275593521, 0.999952884088896, 0.9999485164063994, 0.9999457175201515, 0.9999700191238838, 0.999989223074592, 0.9999620701618496, 0.9999976512501663, 0.9999617327230843, 0.9999634051293608, 0.9999978903317477, 0.9999711810649519, 0.9999744672733811, 0.9999654709874736, 0.9999956168635131, 0.9999846395400805, 0.9999519107509459, 0.999973800569025, 0.9999507181323256, 0.9999542117648872, 0.9999671122236391, 0.9999561068017278, 0.9999522428806331, 0.9999456172327952, 0.9999704949284126, 0.9999721501332426])\n",
      " 1]\n",
      "[list([0.9999925325933372, 1.0000182820809804, 1.0000113777632291, 1.0000163692042119, 1.0000159581962735, 0.9999837732841453, 0.9999802490579656, 0.9999865478440229, 0.9999991779708218, 0.9999824992961381, 0.9999602123719105, 1.0000013917605466, 0.9999890675300223, 0.9999989320007276, 1.0000051855418945, 0.9999770929585529, 0.9999591655651925, 0.9999606976187927, 0.9999713273963612, 0.9999775587247264, 0.9999499951181824, 1.0000176430649126, 1.0000136310547414, 1.0000152873340735, 1.0000148508001134, 0.9999932134165423, 0.9999863684287679, 0.9999866621132036, 0.9999997177956469, 0.9999982459779798, 0.9999731110816034])\n",
      " 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17748/2798940438.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  wg_0 = np.array([wg_0_lst,wg[1][1]])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17748/2798940438.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  wg_1 = np.array([wg_1_lst,wg[0][1]])\n"
     ]
    }
   ],
   "source": [
    "wg_0_lst = []\n",
    "for i in beta_hat_0:\n",
    "    wg_0_lst.append(i)\n",
    "wg_0_lst\n",
    "\n",
    "wg_0 = np.array([wg_0_lst,wg[1][1]])\n",
    "print(wg_0)\n",
    "\n",
    "wg_1_lst = []\n",
    "for i in beta_hat_1:\n",
    "    wg_1_lst.append(i)\n",
    "wg_1_lst\n",
    "\n",
    "wg_1 = np.array([wg_1_lst,wg[0][1]])\n",
    "print(wg_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0.99999253, 1.00001828, 1.00001138, 1.00001637, 1.00001596,\n",
       "         0.99998377, 0.99998025, 0.99998655, 0.99999918, 0.9999825 ,\n",
       "         0.99996021, 1.00000139, 0.99998907, 0.99999893, 1.00000519,\n",
       "         0.99997709, 0.99995917, 0.9999607 , 0.99997133, 0.99997756,\n",
       "         0.99995   , 1.00001764, 1.00001363, 1.00001529, 1.00001485,\n",
       "         0.99999321, 0.99998637, 0.99998666, 0.99999972, 0.99999825,\n",
       "         0.99997311]),\n",
       "  0],\n",
       " [array([1.00001214, 0.99995461, 0.9999767 , 0.99995315, 0.99995551,\n",
       "         0.99996823, 0.99995288, 0.99994852, 0.99994572, 0.99997002,\n",
       "         0.99998922, 0.99996207, 0.99999765, 0.99996173, 0.99996341,\n",
       "         0.99999789, 0.99997118, 0.99997447, 0.99996547, 0.99999562,\n",
       "         0.99998464, 0.99995191, 0.9999738 , 0.99995072, 0.99995421,\n",
       "         0.99996711, 0.99995611, 0.99995224, 0.99994562, 0.99997049,\n",
       "         0.99997215]),\n",
       "  1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg_lst = np.array([wg_1,wg_0])\n",
    "\n",
    "wg_lst_v0 = np.array(wg_lst[0][0])\n",
    "wg_lst_0 = [wg_lst_v0,wg_lst[0][1]]\n",
    "# wg_lst_0 = np.array(wg_lst_0)\n",
    "# wg_lst_0\n",
    "\n",
    "wg_lst_v1 = np.array(wg_lst[1][0])\n",
    "# wg_lst_v1\n",
    "wg_lst_1 = [wg_lst_v1,wg_lst[1][1]]\n",
    "wg_lst_1\n",
    "\n",
    "wg_lst_a = (wg_lst_0,wg_lst_1)\n",
    "wg_lst_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([array([0.99999253, 1.00001828, 1.00001138, 1.00001637, 1.00001596,\n",
      "       0.99998377, 0.99998025, 0.99998655, 0.99999918, 0.9999825 ,\n",
      "       0.99996021, 1.00000139, 0.99998907, 0.99999893, 1.00000519,\n",
      "       0.99997709, 0.99995917, 0.9999607 , 0.99997133, 0.99997756,\n",
      "       0.99995   , 1.00001764, 1.00001363, 1.00001529, 1.00001485,\n",
      "       0.99999321, 0.99998637, 0.99998666, 0.99999972, 0.99999825,\n",
      "       0.99997311]), 0], [array([1.00001214, 0.99995461, 0.9999767 , 0.99995315, 0.99995551,\n",
      "       0.99996823, 0.99995288, 0.99994852, 0.99994572, 0.99997002,\n",
      "       0.99998922, 0.99996207, 0.99999765, 0.99996173, 0.99996341,\n",
      "       0.99999789, 0.99997118, 0.99997447, 0.99996547, 0.99999562,\n",
      "       0.99998464, 0.99995191, 0.9999738 , 0.99995072, 0.99995421,\n",
      "       0.99996711, 0.99995611, 0.99995224, 0.99994562, 0.99997049,\n",
      "       0.99997215]), 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_17748/483162239.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  wg_lst_a = np.array(wg_lst_a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9648506151142355"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.set_printoptions(precision=3)\n",
    "breasts = datasets.load_breast_cancer()\n",
    "X = breasts.data\n",
    "y = breasts.target\n",
    "# x = [(1) + X]\n",
    "# x\n",
    "X1 = np.insert(list(X), 0, 1, axis=1)\n",
    "# X1 = rescale(X1)\n",
    "\n",
    "x = []\n",
    "for i in X1:\n",
    "    x.append(list(i))\n",
    "x = np.array(x)\n",
    "    \n",
    "\n",
    "\n",
    "# x = []\n",
    "# for i in X1:\n",
    "#     x.append(list(i))\n",
    "    \n",
    "# x\n",
    "    \n",
    "# x = [[1] + list(row) for row in X]\n",
    "# x\n",
    "\n",
    "\n",
    "\n",
    "print(wg_lst_a)\n",
    "X1 = np.array(rescale(X1))\n",
    "y = np.array(y)\n",
    "wg_lst_a = np.array(wg_lst_a)\n",
    "score(X1,y,wg_lst_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
